\chapter{A formal functional language based on first-class destinations: \destcalculus{}}\label{chap:dest-calculus}

In the previous chapter we laid out important building blocks for the functional language that we develop here, namely \destcalculus{}. Let's see why destination passing is valuable in a pure functional programming context, and how \destcalculus{} constitutes a good foundational layer for this.

The results presented in this chapter have also been published at the OOPSLA 2025 conference~\cite{bagrel_destination_2025}.

% TODO: Context
% TODO: Contributions
% TODO: Plan du chapitre

\section{Destination-passing style programming, in a functional setting}\label{sec:destcalc-intro}

In destination-passing style, a function doesn't return a value: it takes as an argument a location---named \emph{destination}---where the output of the function ought to be written. In this chapter, we will denote destinations by $[[⌊ T ⌋ ¹ν]]$ where $[[T]]$ is the type of what can be stored in the destination. A function of type $[[T → U]]$ would have signature $[[T → ⌊ U ⌋ ¹ν → ①]]$ when transformed into destination-passing style. Instead of returning an $[[U]]$, it consumes a destination of type $[[⌊U⌋ ¹ν]]$ and returns just \emph{unit} (type $[[①]]$). This style is common in systems programming, where destinations $[[⌊ U ⌋ ¹ν]]$ are more commonly known as \emph{out parameters} (in C, $[[⌊ U ⌋ ¹ν]]$ would typically be a pointer of type $\ottstype{U*}$).

The reason why systems programs rely on destinations so much is that using destinations can save calls to the memory allocator, which are costly, in a context where every gain of performance is highly valuable. If a function returns a $[[U]]$, it has to allocate the space for a $[[U]]$. But with destinations, the caller is responsible for finding space for a $[[U]]$. The caller may simply ask for the space to the memory allocator, in which case we've saved nothing; but it can also reuse the space of an existing $[[U]]$ which it doesn't need anymore, or space in an array, or even space in a region of memory that the allocator doesn't have access to, like a memory-mapped file. In fact, as we'll see in \cref{sec:syntax-type-system}, destination-passing style is always more general than its direct style counterpart (though it might be slightly more verbose); there are no drawbacks to use it as we can always recover regular direct style functions in a systematic way from the destination-passing ones.

So far we mostly considered destination passing in imperative contexts, such as systems programming, but we argue that destination passing also presents many benefits for functional programming languages, even pure ones. Where destinations truly shine in functional programming is that they let us borrow imperative-like programming techniques to get a more expressive language, with more control over processing, and with careful attention, we can still preserve the nice properties of functional languages such as purity and (apparent) immutability.

Thus, the goal here will be to modify a standard functional programming language just enough to be able to build immutable structures by destination passing without endangering purity and memory safety. Destinations in this particular setting become \emph{write-once-only} references into a structure that contains holes and that cannot be read yet. Quite more restrictive than C pointers (that can be used in read and write fashion without restrictions), but still powerful enough to bring new programming techniques into the functional world.

More precisely, there are two key elements that contribute to the extra expressiveness of destination passing in functional contexts:
\begin{itemize}
\item structures can be built in any order. Not only from the leaves to the root, like in ordinary functional programming, but also from the root to the leaves, or any combination thereof. This can be done in ordinary functional programming using function composition in a form of continuation-passing; however destinations act as a very direct optimization of this scheme. This line of work was pioneered by~\citet{minamide_functional_1998};
\item when destinations are first-class values, they can be passed and stored like ordinary values. The consequence is not only that the order in which a structure is built is arbitrary, but also that this order can be determined dynamically during the runtime of the program. Here is the main novelty of our approach compared to earlier work.
\end{itemize}

In this chapter, we introduce \destcalculus{}, a pure functional calculus that is based on the very concept of destinations at its core. We intend \destcalculus{} to serve as a foundational, theoretical calculus to reason about safe destinations in a functional setting; thus, we will only cover very briefly the implementation concerns in this chapter, as \destcalculus{} is not really meant to be implemented as a real programming language. Actual implementation of destination passing in an existing functional language will be the focus of the next chapters. 

\destcalculus{} subsumes all the functional systems with destinations from the literature, from~\cite{minamide_functional_1998} to~\cite{lorenzen_searchtree_2024}. As such we expect that these systems or their extensions can be justified simply by giving them a translation into \destcalculus{}, in order to get all the safety results and metatheory of \destcalculus{} for free. Indeed, we proved type safety theorems for \destcalculus{} with the Coq proof assistant, as described in \cref{sec:formal-proof}.

\section{Working with destinations in \destcalculus{}}\label{sec:working-with-dests}

Let's introduce and get familiar with \destcalculus{}, a modal, linear, simply typed $\lambda$-calculus with destinations. We borrow most of the syntax of the previous chapter, especially from \CLdm{}. We still use linear logic's $[[T⨁U]]$ and $[[T⨂U]]$ for sums and products, and linear function arrow $\ottstype{\multimap}$, since \destcalculus{} is linearly typed. The main difference with previous calculi being that \destcalculus{} doesn't have first-class data constructors like $[[ˢInl t]]$ or $[[ˢ(t1, t2)]]$, as they are replaced with the more general destination-filling operators that we'll discover in the next paragraphs.

\subsection{Building up a vocabulary}\label{ssec:build-up-vocab}

\activespaces

In its simplest form, destination passing, much like continuation passing, is using a location, received as an argument, to write a the output value instead of returning it proper. For instance, the following is a destination-passing version of the identity function:

\codehere{\newoperator
{\ottkw{dId}}{[[T ¹ν → ⌊ T ⌋ ¹ν ¹ν → ①]]}
{\ottkw{dId}~[[x]]~[[d]]}{[[d ◀ x]]}}

We can think of a destination as a reference to an uninitialized memory location, and $[[d ◀ x]]$ (read “fill $[[d]]$ with $[[x]]$”) as writing $[[x]]$ to the memory location pointed to by $[[d]]$.

Performing $[[d ◀ x]]$ is the simplest way to use a destination: with that we fill it with a value directly. But we can also fill a destination piecewise, by specifying just the outermost constructor that we want to fill it with:

\codehere{\newoperator
{\ottkw{fillWithInl}}{[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]}
{\ottkw{fillWithInl}~[[d]]}{[[d ⨞ Inl]]}}

In this example, we're filling a destination for type $[[T ⨁ U]]$ by setting the outermost constructor to left variant $[[Inl]]$. We think of $[[d ⨞ Inl]]$ (read “fill $[[d]]$ with $[[Inl]]$”) as allocating memory to store a block of the form $[[Inl]]~\holesq$, and write the address of that block to the location that $[[d]]$ points to. Because we've just created an $[[Inl]]$ constructor with no argument yet, we return a new destination of type $[[⌊ T ⌋ ¹ν]]$ pointing to the uninitialized argument of $[[Inl]]$. Uninitialized memory, when part of a structure or value, like $\holesq$ in $[[Inl]]~\holesq$, is called a \emph{hole}.

Notice that with $\ottkw{fillWithInl}$ we are constructing the structure from the outermost constructor inward: we've written a value of the form $[[Inl]]~\holesq$ into a hole, but we have yet to describe what goes in the new hole $\holesq$. Such data constructors with uninitialized arguments are called \emph{hollow constructors}\footnote{The full triangle $[[◀]]$ is used to fill a destination with a fully-formed value, while the \emph{hollow} triangle $[[⨞]]$ is used to fill a destination with a \emph{hollow constructor}.}. This is opposite to how functional programming usually works, where values are built from the innermost constructors outward: first we build a value $[[v]]$ and only then can we use $[[Inl]]$ to make $[[Inl v]]$. Being able to build structures from the outermost constructor inward is a key ingredient to the extra expressiveness that we promised earlier.

Yet, everything we've shown so far could, in principle, have been expressed using continuations. So it's worth asking: how exactly are destinations different from continuations? Part of the answer lies in our intention to effectively implement destinations as pointers to uninitialized memory (see \cref{sec:implem-destcalculus} but also \cref{chap:dps-haskell,chap:ext-linear-nonlinear}). But where destinations really differ from continuations is that we can easily split a destination for a data structure into several destinations for each of its parts. Then, the user is required to fill \emph{all} the resulting destinations. One example of such splitting is the $\ottkw{fillWithPair}$ function, which writes a hollow pair constructor into a destination:

\codehere{\newoperator
{\ottkw{fillWithPair}}{[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]}
{\ottkw{fillWithPair}~[[d]]}{[[d ⨞ (,)]]}}

After using $\ottkw{fillWithPair}$, both the first field \emph{and} the second field of the pair must be filled, using the returned destinations of type $[[⌊ T ⌋ ¹ν]]$ and $[[⌊ U ⌋ ¹ν]]$ respectively.

In contrast, a continuation-passing style equivalent of $\ottkw{fillWithPair}$ would need signature $[[ (T ⨂ U ¹ν → S) ¹ν → ( T ¹ν → S) ⨂ (U ¹ν → S)]]$ (this is different from the usual CPS encoding of the pair constructor with signature $[[ (T ⨂ U ¹ν → S) ¹ν → ( T ¹ν → S) ⨂ (U ¹ν → S)]]$). In fact $\ottkw{fillWithPairCPS} \pmb{:} [[ (T ⨂ U ¹ν → S) ¹ν → ( T ¹ν → S) ⨂ (U ¹ν → S)]]$ cannot be defined, as we would need to collect the results of the two created continuations before we can consume the input continuation of type $[[ (T ⨂ U ¹ν → S)]]$ (thus requiring a form of synchronization and state-passing).

Similarly to continuation-passing transformation though, we can already note that there is a sort of duality between destination-filling operators and the corresponding data constructors. Usual $[[Inl]]$ constructor has signature $[[T ¹ν → T ⨁ U]]$, while destination-filling $[[⨞]][[Inl]]$ has signature $[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]$. Similarly, pair constructor $[[(,)]]$ has signature $[[T ¹ν → U ¹ν → T⨂U]]$, while destination-filling $[[⨞]][[(,)]]$ has signature $[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]$. Assuming some flexibility with currying, we see that the types of arguments and results switch sides around the arrow, and get wrapped/unwrapped from $\ottstype{\lfloor\smallbullet\rfloor}$ when we go from the constructor to the destination-filling operator and vice-versa. This observation will generalize to all destination-filling operators and the corresponding constructors.

\paragraph{Structures with holes}
It is crucial to note that while a destination is used to build a structure, the destination refers only to a specific part of the structure that hasn't been defined yet; not the structure as a whole. Consequently, the (root) type of the structure being built will often be different from the type of the destination at hand. A destination of type $[[⌊ T ⌋ ¹ν]]$ only indicates that some bigger structure has at least a hole of type $[[T]]$ somewhere in it. The type of the structure itself never appears in the signature of destination-filling functions (for instance, using $\ottkw{fillWithPair}$ only indicates that the structure being operated on has a hole of type $[[T ⨂ U]]$ that is being written to).

Thus, we still need a type to tie the structure under construction---left implicit by destination-filling primitives---with the destinations representing its holes. To represent this, \destcalculus{} introduces a type $[[S ⧔ ⌊ T ⌋ ¹ν]]$ for a structure of type $[[S]]$ missing a value of type $[[T]]$ to be complete. There can be several holes in $[[S]]$---resulting in several destinations on the right hand side---and as long as there remain holes in $[[S]]$, it cannot be read. For instance, $[[S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]$ represents an $[[S]]$ that misses both a $[[T]]$ and a $[[U]]$ to be complete (thus to be readable). The right hand side of $\ottstype{\ltimes}$ is not restricted to pair and destination types only; it can be of arbitrarily complex type.

The general form $[[S ⧔ T ]]$ is read “$[[S]]$ ampar $[[T]]$”. The name “ampar” stands for “asymmetric memory par”. The “par” $\ottstype{\parr}$ operator originally comes from (classical) linear logic, and is an associative and commutative operator that can be used in place of linear implication and is slightly more flexible: $[[T ⊸ S]]$ is equivalent to $\ottstype{T^\perp \parr S}$ or $\ottstype{S \parr T^\perp}$. Similarly, $[[T1 ⊸ T2 ⊸ S]]$ is equivalent to $\ottstype{T1^\perp \parr T2^\perp \parr S}$. Function input's types---which are in negative position---are wrapped in the dualizing operator $\ottstype{\smallbullet^\perp}$ when a function is put into ``par'' form.
Here we take a similar approach:~\citet{minamide_functional_1998} first observed that structures with holes are akin to linear functions $[[T ⊸ S]]$, where $[[T]]$ represents the missing part; so we decide to represent them in a more flexible fashion under the form $[[S ⧔ ⌊ T ⌋ ¹ν]]$. In this presentation, the \emph{missing part of type $[[T]]$} appears as a first-class type $[[⌊ T ⌋ ¹ν]]$, a.k.a the \emph{destination} type. The asymmetric nature of our memory par is an unfortunate byproduct of wanting simple sequential semantics. Indeed, it seems that having a symmetric memory par would require concurrent semantics, as the structure being constructed and the destinations pointing to it could be consumed alike, meaning that we could have to pause and wait for a hole to be filled\footnote{\cite{deyoung_sax_2020} features such a system with destinations and a concurrent execution model}.

Destinations, albeit being first-class, always exist within the context of a structure with holes. A destination is both a witness of a hole present in the structure, and a handle to write to it. Crucially, destinations are otherwise ordinary values that lives in the right side of the corresponding ampar. To access the destinations of an ampar, \destcalculus{} provides a $\ottkw{upd}_{\ottkw{\ltimes} }$ construction, which lets us apply a function to the right-hand side of the ampar. It is in the body of $\ottkw{upd}_{\ottkw{\ltimes} } \pmb{:} [[S ⧔ T ¹ν → (T ¹ν → U) ¹ν → S ⧔ U]]$ that functions operating on destinations can be called to update the structure:

\codehere{
  \newoperator
  {\ottkw{fillWithInl'}}{[[S ⧔ ⌊ T ⨁ U ⌋ ¹ν ¹ν → S ⧔ ⌊ T ⌋ ¹ν]]}
  {\ottkw{fillWithInl'}~[[x]]}{[[x ►map d ⟼ d ⨞ Inl]]}
  \newoperator
  {\ottkw{fillWithPair'}}{[[S ⧔ ⌊ T ⨂ U ⌋ ¹ν ¹ν → S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]}
  {\ottkw{fillWithPair'}~[[x]]}{[[x ►map d ⟼ fillWithPair d]]}
}

To tie this up, we need a way to introduce and to eliminate structures with holes. Structures with holes are introduced with $[[alloc]]$ which creates a value of type $[[T ⧔ ⌊ T ⌋ ¹ν]]$. $[[alloc]]$ is somewhat similar to the linear identity function: it is a hole (of type $[[T]]$) that needs a value of type $[[T]]$ to be a complete value of type $[[T]]$. Memory-wise, it is an uninitialized block large enough to host a value of type $[[T]]$, and a destination pointing to it. Conversely, structures with holes are eliminated with\footnote{As the name suggest, there is a more general elimination $\ottkw{from}_{\ottkw{\ltimes} }$. It will be discussed in \cref{sec:syntax-type-system}.} $\ottkw{from}_{\ottkw{\ltimes} }' : [[S⧔① ¹ν → S]]$: if all the destinations have been consumed and only unit remains on the right side, then $[[S]]$ no longer has holes and thus is just a normal data structure.

Equipped with these new operators, we can finally show how to derive traditional constructors from piecemeal filling. Indeed, as we said earlier, \destcalculus{} doesn't have primitive constructor forms; constructors in \destcalculus{} are syntactic sugar. We show here the definition of $[[Inl]]$ and $[[(,)]]$, but the other constructors are derived similarly. Operator $\patu \pmb{:} [[① ¹ν → U ¹ν → U]]$, present in second example, is used to chain operations returning unit type $[[①]]$. For easier reading, we also provide type annotation when $[[alloc]]$ is used:

\codehere{\newoperator
{[[Inl]]}{[[T ¹ν → T ⨁ U]]}
{[[ˢInl x]]}{[[from⧔' ((alloc @ (T ⨁ U)⧔ ⌊ T ⨁ U ⌋ ¹ν) ►map d ⟼ d ⨞ Inl ◀ x)]]}
\newoperator
{[[(,)]]}{[[T ¹ν → U ¹ν → T ⨂ U]]}
{[[ˢ(x, y)]]}{[[from⧔' ((alloc @ (T ⨂ U)⧔ ⌊ T ⨂ U ⌋ ¹ν) ►map d ⟼ (d ⨞ (,)) ►case ¹ν (d1, d2) ⟼ d1 ◀ x; d2 ◀ y)]]}}


\paragraph{Memory safety and purity}

We must reassure the reader here. Of course, using destinations in an unrestricted fashion is not memory safe. We need a linear discipline on destinations for them to be safe. Otherwise, we can encounter two sorts of issues:

\begin{itemize}
\item if destinations are not written at least once, as in:

  \codehere{\newoperator
  {\ottkw{forget}}{[[T]]}
  {\ottkw{forget}}{[[from⧔' ((alloc @ T ⧔ ⌊ T ⌋ ¹ν) ►map d ⟼ ())]]}}

  then reading the result of $\ottkw{forget}$ would lead to reading a location pointed to by a destination that we never used, in other words, reading uninitialized memory. This must be prevented at all cost;
\item if destinations are written several times, as in:

  \codehere{\newoperator
  {\ottkw{ambiguous1}}{[[Bool]]}
  {\ottkw{ambiguous1}}{[[from⧔' ((alloc @ Bool ⧔ ⌊ Bool ⌋ ¹ν) ►map d ⟼ d ◀ true; d ◀ false)]]}
  \newoperator
  {\ottkw{ambiguous2}}{[[Bool]]}
  {\ottkw{ambiguous2}}{[[from⧔' ((alloc @ Bool ⧔ ⌊ Bool ⌋ ¹ν) ►map d ⟼ let x ≔ (d ◀ false) in d ◀ true; x)]]}}

  then,
   we have $\ottkw{ambiguous1}$ that returns $[[false]]$ and $\ottkw{ambiguous2}$ that returns $[[true]]$ due to evaluation order, so we break \emph{let expansion} that is supposed to be valid in a pure language (including \destcalculus{}).
\end{itemize}

Actually, we'll see in \cref{sec:scope-escape-dests} that a linear discipline is not even enough to ensure fully safe use of destinations in \destcalculus{}. But before dealing with this, let's get more familiar with \destcalculus{} through more complex examples.

\subsection{Tail-recursive map}\label{ssec:map-tr}

Let's see how destinations can be used to build usual data structures. For these examples, we suppose that \destcalculus{} has equirecursive types and a fixed-point operator (though this isn't included in the formal system of \cref{sec:syntax-type-system}).

\paragraph{Linked lists}

We define lists as a fixpoint, as usual: $[[List T]] \btriangleq [[① ⨁ (T ⨂ (List T))]]$. For convenience, we also define filling operators $\triangleleft\ottsctor{[]}$ and $\triangleleft\ottsctor{(::)}$, as macros that use the primitive destination-filling operators for sum, product and unit types:

\sidebysidecodehere{b}{0.50}{
\newoperator
  {\triangleleft\ottsctor{[]}}{[[⌊ List T ⌋ ¹ν ¹ν → ①]]}
  {[[d ⨞ [] ]]}{[[d ⨞Inl ⨞()]]}
}{
\newoperator
  {\triangleleft\ottsctor{(::)}}{[[⌊ List T ⌋ ¹ν ¹ν → ⌊ T⌋ ¹ν ⨂ ⌊ List T ⌋ ¹ν]]}
  {[[d ⨞ (::)]]}{[[d ⨞Inr ⨞(,)]]}
}

Just like we did in \cref{ssec:build-up-vocab} we can recover traditional constructors systematically from destination-filling operators, using $[[alloc]]$, $\ottkw{upd}_{\ottkw{\ltimes} }$ and $\ottkw{from}_{\ottkw{\ltimes} }'$:

\codehere{\newoperator
{\ottsctor{(::)}}{[[T ⨂ (List T) ¹ν → List T]]}
{[[x ˢ:: xs]]}{\!\!\!\begin{array}[t]{l}[[from⧔' ((alloc @ (List T) ⧔ ⌊ List T ⌋ ¹ν) ►map d ⟼⮒
‥‥‥‥‥‥༼(d ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ x ; dxs ◀ xs༽)]]\end{array}}}

\paragraph{A tail-recursive map function}

List being ubiquitous in functional programming, the fact that the most natural way to write a $\ottkw{map}$ function on lists isn't tail recursive (hence consumes unbounded stack space), is unpleasant. Map can be made tail-recursive in two passes: first build the result list in reverse order, then reverse it. But thanks to destinations, we are able to avoid this two-pass process altogether, as they let us extend the tail of the result list directly.
We give the complete implementation in \cref{fig:impl-map-tr}.

The tail-recursive function is $\ottkw{map'}$, it has type $
[[(T ¹ν → U) ɷ → List T ¹ν → ⌊ List U ⌋ ¹ν ¹ν → ①]]
$\footnote{In fact, we'll develop a more complex system of modes starting from \cref{sec:syntax-type-system} to ensure scope control of destinations, and thus the proper signature will be $[[(T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①]]$.}. That is, instead of returning a resulting list, it takes a destination as an input and fills it with the result. At each recursive call, $\ottkw{map'}$ creates a new hollow cons cell to fill the destination. A destination pointing to the tail of the new cons cell is also created, on which $\ottkw{map'}$ is called (tail) recursively. This is really the same algorithm that you could write to implement map on a mutable list in an imperative language. Nevertheless \destcalculus{} is a pure language with only immutable types\footnote{We consider data structures in \destcalculus{} to be immutable thanks to the fact that structure with holes cannot be scrutinized while they haven't been fully completed, so destination-related mutations are completely opaque and unobservable.}.

To obtain the regular $\ottkw{map}$ function, all is left to do is to call $[[alloc]]$ to create an initial destination, and $\ottkw{from}_{\ottkw{\ltimes} }'$ when all destinations have been filled to extract the completed list; much like when we make constructors out of filling operators, like $\ottsctor{(::)}$ above.

\begin{codefig}{\caption{Tail-recursive $\ottkw{map}$ function on lists}\label{fig:impl-map-tr}}
\newtype{[[List T]]}{[[① ⨁ (T ⨂ (List T))]]}
\newoperatorb
  {\ottkw{map'}}{[[(T ¹ν → U) ɷ → List T ¹ν → ⌊ List U ⌋ ¹ν ¹ν → ①]]}
  {[[map' f l dl]]}{\!\!\!\begin{array}[t]{l}[[
l ►case ¹ν {⮒
‥‥ˢ[] ⟼ dl⨞[],⮒
‥‥x ˢ:: xs ⟼ (dl⨞(::)) ►case ¹ν⮒
‥‥‥‥(dx, dxs) ⟼ dx ◀ f x ; map' f xs dxs}]]\end{array}}
\newoperator
  {\ottkw{map}}{[[(T ¹ν → U) ɷ → List T ¹ν → List U]]}
  {[[map f l]]}{[[from⧔' ((alloc @ (List U) ⧔ ⌊ List U ⌋ ¹ν) ►map dl ⟼ map' f l dl)]]}
\end{codefig}

\subsection{Functional queues, with destinations}\label{ssec:efficient-queue}

Implementations for a tail-recursive map are present in most of the previous works, from~\cite{minamide_functional_1998}, to recent work~\cite{bour_tmc_2021,leijen_tail_2025}. Tail-recursive map doesn't need the full power of \destcalculus{}'s first-class destinations: it just needs a notion of structure with a (single) hole. We'll slowly works towards an example which uses first-class destinations at their full potential.

% STOPPED THERE

\paragraph{Difference lists}
\newcommand{\lstcat}{\mathop{+\!+}}
Just like in any programming language, the iterated concatenation of linked lists
$(([[xs1]] \lstcat [[xs2]])\lstcat \ldots)\lstcat [[xs]]_n$
is quadratic in \destcalculus{}. The usual solution to improve this complexity is to use \emph{difference lists}. The name \emph{difference lists} covers many related implementations for the concept of a ``linked list missing a queue''. The idea is that a difference list carries the same elements as a list would, but can be easily extended by the back in constant time as we retain a way to set a value for its queue later. In pure functional languages, a difference list is usually represented as a function instead~\cite{hughes_dlist_1986}, as we usually don't have write pointers. A singleton difference list is then $\lamnt{[[ys]]}{[[¹ν]]}{[[x ˢ:: ys]]}$, and concatenation of difference lists is function composition. A difference list is turned into a list by setting its queue to be the empty list, or in the functional case, by applying it to the empty list. The consequence is that no matter how many compositions we have, each cons cell will be allocated a single time, making the iterated concatenation linear indeed.

The problem is that in the functional implementation, each concatenation still allocates a closure. If we're building a difference list from singletons and composition, there's roughly one composition per cons cell, so iterated composition effectively performs two traversals of the list. In \destcalculus{}, we actually have write pointers, in the form of \emph{destinations}, so we can do better by representing a difference list as a list with a hole, much like in an imperative setting. A singleton difference list becomes $[[x]] \ottsctor{::} \holesq$, and concatenation is filling the hole with another difference list, using composition operator $\mathop{\triangleleft\mycirc}$. The detailed implementation is given on the left of \cref{fig:impl-dlist-queue}. This encoding for difference lists makes no superfluous traversal: concatenation is just an $O(1)$ in-place update.

% Piggyback

\sidebysidecodefig{\caption{Difference list and queue implementation in equirecursive \destcalculus{}}\label{fig:impl-dlist-queue}}{t}{0.47}{
\newtype{[[DList T]]}{[[(List T) ⧔ ⌊ List T ⌋ ¹ν]]}
\newoperatorb
  {\ottkw{append}}{[[DList T ¹ν → T ¹ν → DList T]]}
  {[[ys append y]]}{\!\!\!\begin{array}[t]{l}[[
ys ►map dys ⟼ (dys ⨞ (::)) ►case ¹ν⮒
‥‥(dy, dys') ⟼ dy ◀ y ; dys'
]]\end{array}}
\newoperator
  {\ottkw{concat}}{[[DList T ¹ν → DList T ¹ν → DList T]]}
  {[[ys concat ys']]}{[[ys ►map d ⟼ d ⨞· ys']]}
\newoperator
  {\ottkw{toList}}{[[DList T ¹ν → List T]]}
  {[[toList ys]]}{[[from⧔' (ys ►map d ⟼ d ⨞ [])]]}
}{
\newtype{[[Queue T]]}{[[(List T) ⨂ (DList T)]]}
\newoperator
  {\ottkw{singleton}}{[[T ¹ν → Queue T]]}
  {[[singleton x]]}{[[ˢ(ˢInr (x ˢ:: ˢ[]), (alloc @ DList T))]]}
\newoperatorb
  {\ottkw{enqueue}}{[[Queue T ¹ν → T ¹ν → Queue T]]}
  {[[q enqueue y]]}{[[q ►case ¹ν (xs, ys) ⟼ ˢ(xs, ys append y)]]}
\newoperatorb
  {\ottkw{dequeue}}{[[Queue T ¹ν → ① ⨁ (T ⨂ (Queue T))]]}
  {[[dequeue q]]}{\!\!\!\begin{array}[t]{l}[[
q ►case ¹ν {⮒
‥‥ˢ((x ˢ:: xs), ys) ⟼ ˢInr ˢ(x, ˢ(xs, ys)),⮒
‥‥ˢ(ˢ[], ys) ⟼ (toList ys) ►case ¹ν {⮒
‥‥‥‥ˢ[] ⟼ Inl (),⮒
‥‥‥‥x ˢ:: xs ⟼ ˢInr ˢ(x, ˢ(xs, (alloc @ DList T)))}}]]\end{array}}
}

\paragraph{Efficient queue using difference lists}
In an immutable functional language, a queue can be implemented as a pair of lists $[[ˢ(front, back)]]$~\cite{hood_queue_1981}. The list $[[back]]$ stores new elements in reverse order ($O(1)$ prepend). We pop elements from $[[front]]$, except when it is empty, in which case we set the queue to $[[ˢ(reverse back, ˢ[])]]$, and pop from the new front.

For their simple implementation, Hood-Melville queues are surprisingly efficient: the cost of the reverse operation is $O(1)$ amortized for a single-threaded use of the queue. Still, it would be better to get rid of this full traversal of the back list. Taking a step back, this $[[back]]$ list that has to be reversed before it is accessed is really merely a representation of a list that can be extended from the back. And we already know an efficient implementation for this: difference lists.

So we can give an improved version of the simple functional queue using destinations. This implementation is presented on the right-hand side of \cref{fig:impl-dlist-queue}. Note that contrary to an imperative programming language, we can't implement the queue as a single difference list: as mentioned earlier, our type system prevents us from reading the front elements of difference lists. Just like for the simple functional queue, we need a pair of one list that we can read from, and one that we can extend. Nevertheless this implementation of queues is both pure, as guaranteed by the \destcalculus{} type system, and nearly as efficient as what an imperative programming language would afford.

\section{Scope escape of destinations}\label{sec:scope-escape-dests}

In \cref{sec:working-with-dests}, we made an implicit assumption: establishing a linear discipline on destinations ensures that all destinations will eventually find their way to the left of a fill operator $\blacktriangleleft$ or $\triangleleft$, so that the associated holes get written to. This turns out to be slightly incomplete.

To see why, let's consider the type $[[⌊ ⌊ T⌋ ¹ν⌋ ¹ν]]$: the type of a destination pointing to a hole where a destination is expected. Think of it as an equivalent of the pointer type $\ottstype{T*\!*}$ in the C language. Destinations are indeed ordinary values, so they can be stored in data structures, and before they get stored, holes stand in their place in the structure. For instance, if we have $[[d]]\pmb{:}[[⌊ T⌋ ¹ν]]$ and $[[dd]]\pmb{:}[[⌊ ⌊ T⌋ ¹ν⌋ ¹ν]]$, we can form $[[dd ◀ d]]$: $[[d]]$ will be stored in the structure pointed to by $[[dd]]$.

Should we count $[[d]]$ as linearly used here? The alternatives don't seem promising:
\vspace{-0.04cm}\begin{itemize}
\item If we count this as a non-linear use of $[[d]]$, then $[[dd ◀ d]]$ is rejected since destinations (like $[[d]]$ here) can only be used linearly. This choice is fairly limiting, as it would prevent us from storing destinations in structures with holes, as we will do, crucially, in \cref{sec:bft}.
\item If we do not count this use of $[[d]]$ at all, we can write $[[dd ◀ d ; d ◀ v]]$ so that $[[d]]$ is both stored for later use \emph{and} filled immediately (resulting in the corresponding hole being potentially written to twice), which is unsound, as discussed at the end of \cref{ssec:build-up-vocab}.
\end{itemize}\vspace{-0.04cm}
So linear use it is. But this creates a problem: there's no way, within our linear type system, to distinguish between ``a destination that has been used on the left of a triangle so its corresponding hole has been filled'' and ``a destination that has been stored and its hole still exists at the moment''. This oversight may allow us to read uninitialized memory!

Let's compare two examples. We assume a simple store semantics for now where structures with holes stay in the store until they are completed. A reduction step goes from a pair $[[Z]] | [[t]]$ of a store and a term to a new pair $[[Z']] | [[t']]$, and store are composed of named bindings of the form $[[ { lb ≔ v } ]]$ with $[[v]]$ a value that may contain holes. We'll also need the $\ottkw{alloc} \pmb{:} [[(⌊ T ⌋ ¹ν ¹ν → ①) ¹ν → T]]$ operator. The semantics of $\ottkw{alloc}$ is: allocate a structure with a single root hole in the store, call the supplied function with the destination to the root hole as an argument; when the function has consumed all destinations (so only unit remains), pop the structure from the store to obtain a complete $[[T]]$. $[[alloc' f]]$ corresponds, in type and behavior, to
$[[from⧔' (alloc ►map d ⟼ f d)]]$.

In the following snippets, structures with holes are given names $[[lb]]$ and $[[lbd]]$ in the store; holes are given names too and denoted by $[[+h]]$ and $[[+hd]]$, and concrete destinations are denoted by $[[-h]]$ and $[[-hd]]$.

When the building scope of $[[lb]] \pmb{:} [[Bool]]$ is parent to the one of $[[lbd]] \pmb{:} [[⌊ Bool ⌋ ¹ν]]$, everything works well because $[[lbd]]$, that contains destination pointing to $[[+h]]$, has to be consumed before $[[lb]]$ can be read. In other terms, storing an older destination into a fresher one is not a problem:
\bgroup\setlength{\arraycolsep}{0.5ex}
\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|& [[ alloc' (ˢλ d ¹ν ⟼ (alloc' (ˢλ dd ¹ν ⟼ dd ◀ d) @ ⌊ Bool ⌋ ¹ν) ◀ true)]] \\
[[⟶]] & [[ { lb ≔ +h } ]] &|& [[ (alloc' (ˢλ dd ¹ν ⟼ dd ◀ -h) @ ⌊ Bool ⌋ ¹ν) ◀ true ; deref lb ]] \\
[[⟶]] & [[ { lb ≔ +h, lbd ≔ +hd } ]] &|& [[ (-hd ◀ -h ; deref lbd) ◀ true ; deref lb ]] \\
[[⟶]] & [[ { lb ≔ +h, lbd ≔ -h } ]] &|& [[ deref lbd ◀ true ; deref lb ]] \\
[[⟶]] & [[ { lb ≔ +h } ]] &|& [[ -h ◀ true ; deref lb ]] \\
[[⟶]] & [[ { lb ≔ true } ]] &|& [[ deref lb ]] \\
[[⟶]] & $\{ \}$ &|& [[ true ]]
\end{array}}

However, when $[[lbd]]$'s scope is parent to $[[lb]]$'s, i.e. when a newer destination is stored into an older one, then we can write a linearly typed yet unsound program:
\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|& [[alloc' (ˢλ dd ¹ν ⟼ (alloc' (ˢλ d ¹ν ⟼ dd ◀ d) @ Bool) ►case ¹ν { true ⟼ (), false ⟼ () })]] \\
[[⟶]] & [[ { lbd ≔ +hd } ]] &|& [[(alloc' (ˢλ d ¹ν ⟼ -hd ◀ d)  @ Bool) ►case ¹ν { true ⟼ (), false ⟼ () } ; deref lbd]] \\
[[⟶]] & [[ { lbd ≔ +hd , lb ≔ +h } ]] &|& [[(-hd ◀ -h ; deref lb) ►case ¹ν { true ⟼ (), false ⟼ () } ; deref lbd]] \\
[[⟶]] & [[ { lbd ≔ -h , lb ≔ +h } ]] &|& [[ (deref lb) ►case ¹ν { true ⟼ (), false ⟼ () } ; deref lbd]] \\
[[⟶]] & [[ { lbd ≔ -h } ]] &|& [[ +h ►case ¹ν { true ⟼ (), false ⟼ () } ; deref lbd]] \qquad\qquad \raisebox{-0.8ex}{\scalebox{0.35}{\bcbombe\bcbombe\bcbombe}}
\end{array}}
\noindent{}Here the expression $[[dd ◀ d]]$ results in $[[d]]$ escaping its scope for the parent one, so $[[lb]]$ is just uninitialized memory (the hole $[[+h]]$) when we dereference it. This example must be rejected by our type system.

A neighboring issue is when sibling destinations interact with one another:
\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|& [[alloc' (ˢλ d ¹ν ⟼ (d ⨞ (::)) ►case ¹ν (d' @ ⌊ Bool ⌋ ¹ν, dd' @ ⌊⌊ Bool ⌋ ¹ν ⌋ ¹ν) ⟼ dd' ◀ d') @ Bool ⨂ ⌊ Bool ⌋ ¹ν]] \\
[[⟶]] & [[ { lb ≔ +h } ]] &|& [[(-h ⨞ (::)) ►case ¹ν (d' @ ⌊ Bool ⌋ ¹ν, dd' @ ⌊⌊ Bool ⌋ ¹ν ⌋ ¹ν) ⟼ dd' ◀ d' ; deref lb]] \\
[[⟶]] & [[ { lb ≔ ˢ(+h', +hd') } ]] &|& [[ˢ(-h', -hd') ►case ¹ν (d' @ ⌊ Bool ⌋ ¹ν, dd' @ ⌊⌊ Bool ⌋ ¹ν ⌋ ¹ν) ⟼ dd' ◀ d' ; deref lb]] \\
[[⟶]] & [[ { lb ≔ ˢ(+h', +hd') } ]] &|& [[-hd' ◀ -h' ; deref lb]] \\
[[⟶]] & [[ { lb ≔ ˢ(+h', -h') } ]] &|& [[deref lb]] \\
[[⟶]] & $\{ \}$ &|& [[ˢ(+h', -h')]] \qquad\qquad \raisebox{-0.8ex}{\scalebox{0.35}{\bcbombe\bcbombe\bcbombe}}
\end{array}}
\egroup

Here also, we have a hole $[[+h']]$ escaping from the store, which means that if we pattern-match on the result of the code block above, we will also end up reading uninitialized memory!

Again, the problem is that, using purely a linear type system, we can only reject the two last examples if we also reject the first, sound one. In this case, the type $[[⌊ ⌊ T⌋ ¹ν⌋ ¹ν]]$ would become practically useless: such destinations could never be filled. This isn't the direction we want to take: we really want to be able to store destinations in data structures with holes. So we want $[[t]]$ in $[[d ◀ t]]$ to be allowed to be linear. Without further restrictions, it wouldn't be sound, so to address this, we need an extra control system to prevent scope escape; it can't be just linearity. More specifically, we must ensure that if a destination $[[d]]$ is stored inside another destination $[[dd]]$, then $[[d]]$ originates from a scope that is strictly older—that is, one that was opened strictly before the scope of $[[dd]]$. This condition rules out the two problematic examples we have just discussed.

For \destcalculus{}, we decided to use a system of \emph{ages} to represent which scope a resource originates from. Ages are described in detail in \cref{sec:syntax-type-system}. In the rest of this document, we often refer to \emph{scope escape} to indistinctly designate the two issues presented above: either the case $[[dd ◀ d]]$ where $[[d]]$ comes from a strictly younger scope, or the case where $[[d]]$ comes from the same scope as $[[dd]]$, as both are often avoided using the same techniques or tricks.

% TODO: raconter ce qu'on a compris sur l'intéraction entre ref, linéarité, escape, dests

\paragraph{General considerations on linearity, scopes, and references}

Scope escape, as just described, is not an issue specific to destinations. In fact, it seems that any form of storage, be it destinations, or \mintinline{haskellc}/ref/s \emph{à la} OCaml, or anything that can be written to, may act as a channel through which scope escape can occur, if it is given a linear interface.

Conversely, scope escape poses a risk not only to destinations but to most linear resources.
Linear interfaces typically rely on the fact that there is a specific set of destructor-like functions for a given resource type, and that we want one of them to be called for each resource at some point. Linearity forces us to consume the resource in the scope it is made available, and if the resource type is opaque, then the only way to consume it is by eventually calling one of these destructors, as expected. However, linear storage undermines this assumption by faking resource consumption without immediately invoking a destructor.

\section{Breadth-first tree traversal}\label{sec:bft}

Before turning to the formal details, let us look at a full-fledged example, which uses the whole expressive power of \destcalculus{}. We focus here on breadth-first tree relabeling:
% \begin{quote}
\emph{``
Given a tree, create a new one of the same shape, but with the values at the nodes replaced by the numbers $1\ldots|T|$ in breadth-first order.
''}
% \end{quote}

This isn't a very natural problem for functional programming, as breadth-first traversal implies that the order in which the structure must be built (left-to-right, top-to-bottom) is not the same as the structural order of a functional tree---building the leaves first and going up to the root. So it usually requires fancy functional workarounds~\cite{okasaki_bfs_2000,jones_gibbons_linearbfs_93,gibbons_phases_2023}.

We may feel drawn to solve this exercise in an efficient imperative-like fashion, where a queue drives the processing order. That's the standard algorithm taught at university, where the next node to process is dequeued from the front of the queue, and its children nodes are enqueued at the back of the queue for later processing, achieving breadth-first traversal. For that, Minamide's system~\cite{minamide_functional_1998} where structures with holes are represented as linear functions cannot help. We really need destinations as first-class values to borrow from this imperative power.

\Cref{fig:impl-bfs} presents how we can implement this breadth-first tree traversal in \destcalculus{}, thanks to first-class destinations. We assume the data type $[[Tree T]]$ has been defined, unsurprisingly, as $[[Tree T]]\btriangleq [[① ⨁ (T ⨂ ((Tree T) ⨂ (Tree T)))]]$; and we refer to the constructors of $[[Tree T]]$ as $\ottsctor{Nil}$ and $\ottsctor{Node}$, defined as syntactic sugar as we did for the other constructors before. We also assume some encoding of the type $[[Nat]]$ of natural numbers. Remember that $[[Queue T]]$ is the efficient queue type from \cref{ssec:efficient-queue}.

The core idea of our algorithm is that we hold a queue of pairs, storing each an input subtree and (a destination to) its corresponding output subtree. When the element $[[ˢ(tree, dtree)]]$ at the front of the queue has been processed, the children nodes of $[[tree]]$ and children destinations of $[[dtree]]$ are enqueued to be processed later, much as the original imperative algorithm.

We implement the actual breadth-first relabeling $\ottkw{relabelDps}$ as an instance of a more general breadth-first traversal function $\ottkw{mapAccumBfs}$, which applies any state-passing style transformation of labels in breadth-first order. In $\ottkw{mapAccumBfs}$, we create a new destination $[[dtree]]$ into which we will write the result of the traversal, then call $\ottkw{go}$. The $\ottkw{go}$ function is in destination-passing style, but what's remarkable is that $\ottkw{go}$ takes an unbounded number of destinations as arguments, since there are as many destinations as items in the queue. This is where we use the fact that destinations are ordinary values.

% The implementation of \cref{fig:impl-bfs} is very close to the one found in~\cite{bagrel_destination-passing_2024}. The difference is that, because they can't store destinations in structures with holes (see the discussion in \cref{sec:scope-escape-dests}), their implementation can't use the efficient queue implementation from \cref{ssec:efficient-queue}. So they have to revert to using a Hood-Melville queue for breadth-first traversal.

This freshly gained expressivity has a cost though: we need a type system that combines linearity control \emph{and} age control to make the system sound, otherwise we run into the issues of scope escape described in the previous section. We'll combine both linearity and the aforementioned ages system in the same \emph{mode ringoid}\footnote{We said in \cref{sec:modal-lin} that the modal approach with ringoids would make the type system easier to extend, here is the proof.}. You already know the linearity annotations $[[¹]]$ and $[[ω]]$ from \cref{chap:preli}; here we also introduce the new age annotation $[[∞]]$, that indicates that the associated argument cannot carry destinations. Arguments with no modes displayed on them, or function arrows with no modes, default to the unit of the ringoid; in particular they are linear, and can capture destinations.

\begin{codefig}{\caption{Breadth-first tree traversal in destination-passing style}\label{fig:impl-bfs}}
\newoperator[~\mathbf{rec}]
{\ottkw{go}}{[[(S ω∞ → T1 ¹ν → (! ω∞ S) ⨂ T2) ω∞→ S ω∞ → Queue (Tree T1 ⨂ ⌊ Tree T2 ⌋ ¹ν) ¹ν → ①]]}
{[[go f st q]]}{\!\!\!\begin{array}[t]{l}[[
(dequeue q) ►case ¹ν {⮒
‥‥Inl () ⟼ (),⮒
‥‥ˢInr ˢ(ˢ(tree, dtree), q') ⟼ tree ►case ¹ν {⮒
‥‥‥‥ˢNil ⟼ dtree ⨞ Nil ; go f st q',⮒
‥‥‥‥ˢNode x tl tr ⟼ ༼(dtree ⨞ Node) ►case ¹ν⮒
‥‥‥‥‥‥ˢ(dy, ˢ(dtl, dtr)) ⟼ ༼(༼f st༽ x) ►case ¹ν⮒
‥‥‥‥‥‥‥‥ˢ(ˢᴇ ω∞ st', y) ⟼⮒
‥‥‥‥‥‥‥‥‥‥dy ◀ y ;⮒
‥‥‥‥‥‥‥‥‥‥go f st' (q' enqueue ˢ(tl, dtl) enqueue ˢ(tr, dtr))༽༽}}
]]\end{array}}
\newoperator
{\ottkw{mapAccumBfs}}{[[(S ω∞ → T1 ¹ν → (! ω∞ S) ⨂ T2) ω∞→ S ω∞ → Tree T1 ¹∞ → Tree T2]]}
{[[mapAccumBfs f st tree]]}{\!\!\!\begin{array}[t]{l}[[
from⧔' ((alloc @ (Tree T2) ⧔ ⌊ Tree T2 ⌋ ¹ν) ►map dtree ⟼⮒
‥‥‥‥‥‥༼go f st (singleton ˢ(tree, dtree))༽)
]]\end{array}}
\newoperator
{\ottkw{relabelDps}}{[[Tree ① ¹∞ → Tree Nat]]}
{[[relabelDps tree]]}{\!\!\!\begin{array}[t]{l}[[
mapAccumBfs (ˢλ st ω∞ ⟼ ˢλ un ¹ν ⟼ un ; ˢ(ˢᴇ ω∞ (succ st), st)) ˢ1 tree
]]\end{array}}
\end{codefig}

%------------------------------------------------------------------------------


\section{Type system}\label{sec:syntax-type-system}

\begin{ottfig}{\caption{Terms, types and modes of \destcalculus{}}\label{fig:grammar}\label{fig:mul-age-tables}}\bgroup

\begin{minipage}{\linewidth}\figtextsize\textit{Core grammar of terms:}\end{minipage}

\smallskip

\begin{minipage}{\linewidth}\[{\setlength{\arraycolsep}{0.6ex}\begin{array}{rrl}
[[t]], [[u]] &\grammdef& [[x]] \grammsep [[t' t]] \grammsep [[t ; t']] \\
             &|\,& [[ t ►case m { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case m ( x1 , x2 ) ⟼ u]] \grammsep [[t ►case m ᴇ n x ⟼ u]] \\
             &|\,& [[t ►map x ⟼ t']] \grammsep [[ to⧔ t ]] \grammsep [[ from⧔ t ]] \grammsep [[ alloc ]] \\
             &|\,& [[ t ⨞ () ]] \grammsep [[ t ⨞ Inl ]] \grammsep [[t ⨞ Inr]] \grammsep [[t ⨞ (,)]] \grammsep [[t ⨞ ᴇ m]] \grammsep [[t ⨞ ( λ x m ⟼ u )]] \grammsep [[t ⨞· t']] \grammsep [[t ◀ t']]
\end{array}}\]\end{minipage}

\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Grammar of types, modes and contexts:}\end{minipage}

\smallskip

\begin{minipage}{\linewidth}\sidebysidecodehere{t}{0.51}{\setlength{\arraycolsep}{0.6ex}\!\begin{array}{rrl}
[[T]], [[U]], [[S]] &\grammdef& [[⌊ T ⌋ n]] \quad\quad\textit{(destination)} \\
                    &|\,& [[S ⧔ T]] \hspace*{\widthof{$[[⌊ T ⌋ n]]$}-\widthof{$[[U ⧔ T]]$}}\quad\quad\textit{(ampar)} \\
                    &|\,& [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! m T]] \grammsep [[T m → U]] \\
&&\\
[[P]] &\grammdef& [[{ }]] \grammsep [[{ x : m T }]] \grammsep [[P1 , P2]]% \grammsep [[P1 + P2]] \grammsep [[m · P]]
\end{array}}{\setlength{\arraycolsep}{0.6ex}\!\begin{array}[b]{rrl}
[[m]], [[n]] &\grammdef& [[p a]] \hspace*{\widthof{$[[⌊ T ⌋ m]]$}-\widthof{$[[p a]]$}}\quad\textit{(pair of multiplicity and age)} \\
  [[p]] &\grammdef& [[¹]] \grammsep [[ω]] \\
  [[a]] &\grammdef& [[↑^ka]] \grammsep [[∞]]
\end{array}\\[\interdefskip]
[[ν]] \btriangleq [[↑^0]] \quad [[↑]] \btriangleq [[↑^1]]}\end{minipage}

\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Ordering on modes:}\end{minipage}

{\figtextsize
\vspace*{-0.3cm}

\hfill $[[p a ⥶ p' a']] \Longleftrightarrow [[p]] \pleq [[p']] \land~[[a]] \aleq [[a']]$ \hfill \begin{tikzpicture}[baseline=(current bounding box.center), clip]
% Define nodes
\node (omega) at (0,1) {$[[ω]]$};
\node (oneA) at (0,0) {$[[¹]]$};

% Draw edge with label
\draw (omega) -- node[midway, below, rotate=90] {$\pleq$} (oneA);

\node (inf) at (4,1) {$[[∞]]$};
\node (zero) at (2,0) {$[[↑^0]]$};
\node (oneB) at (3,0) {$[[↑^1]]$};
\node (dots1) at (4,0) {\ldots};
\node (k) at (5,0) {$[[↑^ka]]$};
\node (dots2) at (6,0) {\ldots};

% Draw edges with labels
\draw (inf) -- node[fill=none, midway, above, rotate=30, inner sep=1pt] {$\aleq$} (zero);
\draw (inf) -- node[midway, below, rotate=45, inner sep=1pt] {$\aleq$} (oneB);
\draw (inf) -- node[midway, above, rotate=135, yscale=-1, inner sep=1pt] {$[[⥶]]_{\reflectbox{$\scriptscriptstyle{\pmb{\mathtt{a}}}$}}$} (k);
\end{tikzpicture}\hfill\phantom{.}
}

\vspace*{-0.2cm}

\begin{minipage}{\linewidth}\figtextsize\textit{Operations on modes:}\end{minipage}

\smallskip

{\figtextsize
\hfill\begin{tabular}{|c||c|c|}\hline
$\ottsmode{+}$ & $[[¹]]$ & $[[ω]]$ \\\hhline{|=#=|=|}
$[[¹]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hfill
\begin{tabular}{|c||c|c|}\hline
$[[·]]$        & $[[¹]]$ & $[[ω]]$ \\\hhline{|=#=|=|}
$[[¹]]$        & $[[¹]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hfill
\vrule width 0.5pt % Vertical rule of 1pt width
\hfill
\begin{tabular}{|c||c|c|}\hline
$\ottsmode{+}$ & $[[↑^ka]]$               & $[[∞]]$ \\\hhline{|=#=|=|}
$[[↑^ja]]$     & $\text{if }\ottsmodee{k} = \ottsmodee{j}\text{ then }[[↑^ka]]\text{ else }[[∞]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\hfill
\begin{tabular}{|c||c|c|}\hline
$[[·]]$        & $[[↑^ka]]$    & $[[∞]]$ \\\hhline{|=#=|=|}
$[[↑^ja]]$     & $[[↑^ka+ja]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$       & $[[∞]]$ \\\hline
\end{tabular}\hfill\phantom{.}
\smallskip

\hfill\hspace*{-1.5cm}
$[[(p a) · (p' a')]] \btriangleq [[(p · p') (a · a')]]$
\hfill\hspace*{0.5cm}
$[[(p a) + (p' a')]] \btriangleq [[(p + p') (a + a')]]$
\hfill\phantom{.}}

\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Operations on typing contexts:}\end{minipage}

\smallskip

{\figtextsize
\bgroup
\renewcommand\tabcolsep{2pt}
\hfill\begin{tabular}[c]{rclcc@{\qquad}l}
  $[[n]]$ &$\cdot$& $[[{}]]$ & $\btriangleq$ & $[[{}]]$\\
  $[[n]]$ &$\cdot$& $[[({ x : m T },P)]]$ & $\btriangleq$ & $[[({ x : n · m T }),n·P]]$\\
\end{tabular}
\hfill\hspace*{-1cm}
\vrule width 0.5pt % Vertical rule of 1pt width
\hfill
\begin{tabular}[c]{rclcc@{\qquad}l}
  $[[{}]]$ &$+$& $[[P]]$ & $\btriangleq$ & $[[P]]$\\
  $[[({ x : m T }, P1)]]$ &$+$& $[[P2]]$ & $\btriangleq$ & $[[{ x : m T },(P1+P2)]]$ & \textrm{if $[[x]]\notin[[P2]]$}\\
  $[[({ x : m T }, P1)]]$ &$+$& $[[({ x : m' T }, P2)]]$ & $\btriangleq$ & $[[{x : m+m' T}, (P1+P2)]]$
\end{tabular}\hfill\phantom{.}
\egroup
}

\egroup
\end{ottfig}

\begin{codefig}{\caption{Syntactic sugar definitions for terms}\label{fig:sterm}}
\begin{minipage}{\linewidth}
\sidebysidecodehere{t}{0.42}{
[[ˢInl t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ Inl ◀ t)
]]\end{array}\\[\interdefskip]
[[ˢInr t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ Inr ◀ t)
]]\end{array}\\[\interdefskip]
[[ˢᴇ m t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ ᴇ m ◀ t)
]]\end{array}\\[\interdefskip]
[[ˢλ x m ⟼ u]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ ( λ x m ⟼ u ))
]]\end{array}
}{
[[from⧔' t]] \btriangleq \\
\myspace{1}\!\!\!\begin{array}[t]{l}[[
(from⧔ (t ►map un ⟼ un ; ᴇ ¹∞ () )) ►case ¹ν⮒
‥‥( st , ex ) ⟼ ༼ex ►case ¹ν⮒
‥‥‥‥ᴇ ¹∞ un ⟼ un ; st༽
]]\end{array}\\[\interdefskip]
[[ˢ()]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼ d ⨞ () )
]]\end{array}\\[\interdefskip]
[[ˢ( t1 , t2 )]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼ (d ⨞ (,)) ►case ¹ν⮒
‥‥‥‥‥‥( d1 , d2 ) ⟼ d1 ◀ t1 ; d2 ◀ t2)
]]\end{array}
}
\end{minipage}
\end{codefig}

\destcalculus{} is a simply typed $\lambda$-calculus with unit ($[[①]]$), product ($\ottstype{\otimes}$) and sum ($\ottstype{\oplus}$) types, inspired from \CLdm{} described in \cref{chap:preli}. It also features a mode-polymorphic function arrow $_{[[m]]}\!\ottstype{\multimap}$. Its most distinctive features however are the destination $[[⌊ T ⌋ m]]$ and ampar $[[S ⧔ T]]$ types which we've introduced in \cref{sec:working-with-dests,sec:scope-escape-dests,sec:bft}. Don't pay too much attention to the mode annotation on the destination type; so far it was hidden in the examples because we only considered linear destinations.

To ensure that destinations are used soundly, we need both to enforce the linearity of destinations but also to prevent destinations from escaping their scope, as discussed in \cref{sec:scope-escape-dests}. To that effect, \destcalculus{} tracks the \emph{age} of destinations, that is how many nested scopes have been open between the current expression and the scope from which a destination originates. We'll see in \cref{ssec:ty-term} that scopes are introduced by the $[[t ►map x ⟼ t']]$ construct. For instance, if we have a term $[[t1 ►map x1 ⟼ t2 ►map x2 ⟼ t3 ►map x3 ⟼ x1]]$, then we say that the innermost occurrence of $[[x1]]$ has age $[[↑]]^{\ottsmodee{2}}$ because two nested $\ottkw{upd}_{\ottkw{\ltimes} }$ separate the definition and use site of $[[x1]]$.

For \destcalculus{}, we take the same modal approach as for \CLdm{}, but we enrich our mode ringoid to have an \emph{age} axis. Thanks to the algebraic nature of the modal approach, for most typing rules, we'll be able to reuse those of \CLdm{} without any modification as just the elements of the ringoid change, not the properties nor the structure of the ringoid.

The syntax of \destcalculus{} terms is presented in \cref{fig:grammar}, and the syntactic sugar that we used in \cref{sec:working-with-dests,sec:scope-escape-dests,sec:bft} is presented in \cref{fig:sterm}.

\subsection{Modes and the age ringoid}\label{ssec:age-control}

% The precise algebraic structure that we'll be needing on modes is both a commutative additive semigroup $\ottsmode{+}$ and a multiplicative monoid $(\ottsmode{[[·]]}\,, \ottsmode{1})$, with the usual distributivity law $[[n·(m1+m2)]] = [[n·m1 + n·m2]]$. In addition we require a partial order $⥶$, such that $\ottsmode{+}$ and $\ottsmode{[[·]]}$ are order-preserving. In other words, we'll be dealing with ordered semirings\footnote{There terminology dispute where some prefer to use the term ``semiring'' when the additive semigroup has a zero. This terminology is arguably more popular, but leaves no term for the version without a zero. We'll follow the convention, in this article, that semirings with a zero are called ``rigs''.}. In the rest of the article, we'll just say ``semiring''. In practice, all our semirings will be commutative, and we won't be paying attention to the order of factors in mode multiplication.

Our mode ringoid (more precisely, a commutative additive semigroup $\ottsmode{+}$ and a multiplicative monoid $(\ottsmode{[[·]]}\,, \ottsmode{1})$, with the usual distributivity law $[[n·(m1+m2)]] = [[n·m1 + n·m2]]$, and equipped with a partial order $⥶$, such that $\ottsmode{+}$ and $\ottsmode{[[·]]}$ are order-preserving) is, as promised, the product of a multiplicity ringoid, to track linearity, and an age ringoid, to prevent scope escape. The resulting compound structure is also a ringoid.

The multiplicity ringoid has elements $[[¹]]$ (linear) and $[[ω]]$ (unrestricted), it's the same ringoid as in~\cite{qtt_2018} or~\cite{bernardy_linear_2018}, and the same as described in detail in \cref{chap:preli}. The full description of the multiplicity ringoid is given again in \cref{fig:mul-age-tables}.

Ages are more interesting. We write ages as $[[↑^ka]]$ (with $[[ka]]$ a natural number), for ``defined $[[ka]]$ scopes ago''.
We also have an age $[[∞]]$ for variables that don't originate from a $[[t ►map x ⟼ t']]$ construct i.e. that aren't destinations, and can be freely used in and returned by any scope. The main role of age $[[∞]]$ is thus to act as a guarantee that a value doesn't contain destinations. Finally, we will write $[[ν]] \btriangleq [[↑^0]]$  (``now'') for the age of destinations that originate from the current scope; and $[[↑]] \btriangleq [[↑^1]]$.

The operations or order on ages aren't the usual ones on natural numbers though. Indeed, it's crucial that \destcalculus{} tracks the precise age of variables, and usually, ordering on modes usually implies a form of subtyping (where a variable having mode $[[n]]$ can be used in place where a variable with mode $[[m]]$ is expected if $[[m]] ⥶ [[n]]$). Here we don't want variables from two scopes ago to be used as if they were from one scope ago. The ordering reflects this with finite ages being arranged in a flat order (so $[[↑^1]]$ and $[[↑^2]]$ aren't comparable), with $[[∞]]$ being bigger than all of them. Multiplication of ages reflects nesting of scope, as such, (finite) ages are multiplied by adding their numerical exponents $[[↑^ka · ↑^ja]] = [[↑^ka + ja]]$. In the typing rules, the most common form of scope nesting is opening a scope, which is represented by multiplying the age of existing bindings by $[[↑]]$ (that is, adding $1$ to the ages seen as a natural numbers). When a same variable is shared between two subterms, $\ottsmode{+}$ takes the least upper bound for the age order above, in other terms, the variable must be at the same age in both subterms, or have age $[[∞]]$ otherwise, which let it assume whichever age it needs in each subterm.

The unit of the new mode ringoid is the pair of the units from the multiplicity and age ringoids, ie. $[[¹ν]]$. We will usually omit the mode annotations when the mode is that unit.

Finally, as in \CLm{} and \CLdm{} from \cref{chap:preli}, operations on modes are lifted to typing contexts, still following the insights from~\cite{ghica_bounded_2014} (see \cref{fig:mul-age-tables}).

Anticipating on next section, let's see how \emph{plus} works on context, modes, and especially ages. Let's type the expression $[[d ◀ x ; x]]$. Most of the typing here behaves the same as \rref*{\CLdm{}\CTy{}}, the only particularity is that the operator $[[◀]]$ scales the typing context of its right operand by $[[↑]]$ (which corresponds to asking it to be from the previous scope, to prevent scope escape of destinations). We have the following typing tree:

\[
\inferrule*[right=\CPatU]{
  \inferrule*[right=\CFillLeaf]{
    \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ d : ¹ν ⌊T⌋ ¹ν } ⊢ d : ⌊T⌋ ¹ν]]} \quad
    \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ x : ¹ν T } ⊢ x : T]]}
  }{[[{ d : ¹ν ⌊T⌋ ¹ν }, { x : ¹↑ T } ⊢ d ◀ x : ①]]}  \quad
  \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ x : ¹ν T } ⊢ x : T]]}
}{[[{ d : ¹ν ⌊T⌋ ¹ν },{ x : ω∞ T } ⊢ d ◀ x ; x : T]]}
\]

At the root of the tree, with operator $\patu$, the typing context of the two initial branches are summed, so we have ${[[({ d : ¹ν ⌊T⌋ ¹ν }, { x : ¹↑ T })+({ x : ¹ν T })]]=[[{ d : ¹ν ⌊T⌋ ¹ν },{ x : (¹+¹)(ν+↑) T }]]=[[{ d : ¹ν ⌊T⌋ ¹ν },{ x : ω∞ T }]]}$. In other terms, because we need $[[x]]$ to be of age $[[↑]]$ in the first branch, but of age $[[ν]]$ in the second branch---as, we recall, $[[ν]]=[[↑^0]]$ and $[[↑]]=[[↑^1]]$ are \emph{not} comparable, so we cannot use the rule \rref*{\CTyTerm\CSep\CVar} on a variable with mode $[[¹↑]]$--- then $[[x]]$ must be of age $[[∞]]$\footnote{Age requirements are only there for scope-sensitive resources, that is, just destinations and the structures they are stored in. Age $[[∞]]$ is meant to designate non scope-sensitive resources, and consequently, is also the result of incompatible age requirements (as only non scope-sensitive resources can fit incompatible age requirements).}.

Alternatively, we can have:

\[
\inferrule*[right=\CPatU]{
  \inferrule*[right=\CFillLeaf]{
    \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ d : ¹ν ⌊T⌋ ¹ν } ⊢ d : ⌊T⌋ ¹ν]]}  \quad
    \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[ω∞]]}{[[{ x : ω∞ T } ⊢ x : T]]}
  }{[[{ d : ¹ν ⌊T⌋ ¹ν }, { x : ω∞ T } ⊢ d ◀ x : ①]]}  \quad
  \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[ω∞]]}{[[{ x : ω∞ T } ⊢ x : T]]}
}{[[{ d : ¹ν ⌊T⌋ ¹ν },{ x : ω∞ T } ⊢ d ◀ x ; x : T]]}
\]

\noindent where $[[x]]$ is given mode $[[ω∞]]$ from the leaves to the root. We can make use of a variable with mode $[[ω∞]]$ because we have $[[¹ν]] ⥶ [[ω∞]]$, which matches the premise of rule \rref*{\CTyTerm\CSep\CVar}.

\subsection{Typing rules}\label{ssec:ty-term}

Core typing rules of \destcalculus{} are given in \cref{fig:ty-term}. Rules for elimination of $\ottstype{\multimap}$, $[[①]]$, $\ottstype{\oplus}$, $\ottstype{\otimes}$ and $\ottstype{!_{[[m]]}}$ are the same as in \CLdm{} of \cref{chap:preli}, so we won't cover them again here.

\begin{ottfig}{\caption{Typing rules of \destcalculus{} (\rref*{\CTyTerm})}\label{fig:ty-term}}\bgroup\renewcommand{\ottdruleTyXXtermXXVal}{}

\bgroup\SetPrefix{}
\renewcommand\ottaltinferrule[4]{
      \inferrule*[narrower=0.3,right=#1,#2]
        {#3}
        {#4}
    }
\ottdefnTyXXterm{}
\egroup

\egroup
\end{ottfig}
\begin{ottfig}{\caption{Derived typing rules for syntactic sugar (\rref*{\CTySTerm})}\label{fig:ty-sterm}}\bgroup

\bgroup\SetPrefix{}
\renewcommand\ottaltinferrule[4]{
  \inferrule*[fraction={===},narrower=0.3,right=#1,#2]
    {#3}
    {#4}
}
\ottdefnTyXXsterm{}\egroup
\egroup
\end{ottfig}

One notable difference with \CLdm{} though, is, as announced, that introduction rules for data structures, aka. data constructors for types $[[①]]$, $\ottstype{\oplus}$, $\ottstype{\otimes}$ and $\ottstype{!_{[[m]]}}$, are derived from elimination rules of destinations, and thus are not part of the core language. They are presented on the distinct \cref{fig:ty-sterm}. The introduction form for functions \rref*{\CTySTerm\CSep\CFun} is also derived from elimination form \rref*{\CTyTerm\CSep\CFillF} for the corresponding destinations of function, although functions are not purely ``data''.

Rules \rref*{\CTyTerm\CSep\CVar}, \rref*{\CTyTerm\CSep\CNewA} and \rref*{\CTySTerm\CSep\CUnit}, that are the potential leaves of the typing tree, is where weakening for unrestricted variables is allowed to happen. That's why they allow to discard any typing context of the form $[[ων·P]]$. It's very similar to what happened in \rref*{\CLdm\CTy\CSep\CId} or \rref*{\CLdm\CTy\CSep\CUnit}, except that $[[P]]$ is scaled by $[[ων]]$ instead of $[[ɷ]]$ now.

Rule \rref*{\CTyTerm\CSep\CVar}, in addition to weakening, allows for coercion of the mode $[[m]]$ of the variable used, with ordering constraint $[[¹ν ⥶ m]]$ as defined in \cref{fig:mul-age-tables}. Unlike in \CLdm{}, here not all modes $[[m]]$ are compatible with $[[¹ν]]$. Notably, mode coercion still doesn't allow for a finite age to be changed to another, as $[[↑^ja]]$ and $[[↑^ka]]$ are not comparable w.r.t. $\aleq$ when $[[ja]]\neq[[ka]]$. So for example we cannot use a variable with mode $[[¹↑]]$ in most contexts.

For pattern-matching, with rules \rref*{\CTyTerm\CSep\CPatS}, \rref*{\CTyTerm\CSep\CPatP} and \rref*{\CTyTerm\CSep\CPatE}, we keep the same \emph{deep mode} approach as in \CLdm{}: $\ottkw{case}$ expressions are parametrized by a mode $[[m]]$ by which the typing context $[[P1]]$ of the scrutinee is multiplied. The variables which bind the subcomponents of the scrutinee then inherit this mode.

Let's now move on to the real novelties of the typing rules for \destcalculus{}, that is, rules for operators acting on ampars and destinations.

\paragraph{Rules for scoping}

As destinations always exist in the context of a structure with holes, and must stay in that context, we need a formal notion of \emph{scope}. Scopes are created by \rref*{\CTyTerm\CSep\CUpdA}, as destinations are only ever accessed through $\ottkw{upd}_{\ottkw{\ltimes} }$. More precisely, $[[t ►map x ⟼ t']]$ creates a new scope which spans over $[[t']]$. In that scope, $[[x]]$ has age $[[ν]]$ (now), and the ages of the existing bindings in $[[P2]]$ are multiplied by $[[↑]]$ (i.e. we add $1$ to ages seen as a numbers). That is represented by $[[t']]$ typing in $[[¹↑·P2,{ x : ¹ν T }]]$ while the parent term $[[t ►map x ⟼ t']]$ types in unscaled contexts $[[P1+P2]]$. This difference of age between $[[x]]$---introduced by $\ottkw{upd}_{\ottkw{\ltimes} }$, containing destinations---and $[[P2]]$ lets us see what originates from older scopes. Specifically, distinguishing the age of destinations is crucial when typing filling primitives to avoid the pitfalls of \cref{sec:scope-escape-dests}.
%

\begin{ottfig}{\caption{Evolution of ages through nested scopes}\label{fig:scope-rules}}
  \scalebox{0.85}{\tikzfig{schemas/mapscopes}}
\end{ottfig}

\Cref{fig:scope-rules} illustrates scopes introduced by $\ottkw{upd}_{\ottkw{\ltimes} }$, and how the typing rules for $\ottkw{upd}_{\ottkw{\ltimes} }$ and $\blacktriangleleft$ interact. For the first time here we see ampar values $\ottsctor{/}[[v2]]\,\ottsctor{\bbcomma}~[[v1]]\ottsctor{/}$, represented as pair-like objects with a structure with holes $[[v2]]$ on the left, and an arbitrary structure $[[v1]]$ containing destinations on the right (ampar values will be formally introduced in \cref{ssec:runtime-values}). With $\ottkw{upd}_{\ottkw{\ltimes} }$ we enter a new scope where the destinations are accessible, but the structure with holes remains in the outer scope. As a result, when filling a destination with \rref*{\CTyTerm\CSep\CFillLeaf}, for instance $[[d11 ◀ x0]]$ in \cref{fig:scope-rules}, we type $[[d11]]$ in the new scope, while we type $[[x0]]$ in the outer scope, as it’s being moved to the structure with holes on the left of the ampar, which lives in the outer scope too. This is the opposite of the scaling that $\ottkw{upd}_{\ottkw{\ltimes} }$ does: while $\ottkw{upd}_{\ottkw{\ltimes} }$ creates a new scope for its body, operator $\blacktriangleleft$, and similarly, $\triangleleft\mycirc$ and $[[⨞]](\lamnt{[[x]]}{[[m]]}{[[u]]})$, transfer their right operand to the outer scope. In other words, the right-hand side of $[[◀]]$ or $[[⨞]]$ is an enclave for the parent scope.

When using $\ottkw{from}_{\ottkw{\ltimes} }'$ (rule \rref*{\CTySTerm\CSep\CFromA'}), the left of an ampar is extracted to the current scope. This is the fundamental reason why the left of an ampar has to ``take place'' in the current scope. We know the structure is complete and can be extracted because the right-hand side is of type unit ($[[①]]$) and thus, no destination on the right-hand side means no hole can remain on the left. $\ottkw{from}_{\ottkw{\ltimes} }'$ is implemented in terms of $\ottkw{from}_{\ottkw{\ltimes} }$ in \cref{fig:sterm} to keep the core calculus tidier (and limit the number of typing rules, evaluation contexts, etc), but it can be implemented much more efficiently in a real-world implementation.

When an ampar is eliminated with the more general $\ottkw{from}_{\ottkw{\ltimes} }$ in rule \rref*{\CTyTerm\CSep\CFromA} however, we extract both sides of the ampar to the current scope, even though the right-hand side is normally in a different scope. This is only safe to do because the right-hand side is required to have type $[[! ¹∞ T]]$, which means it is scope-insensitive: it can't contain any scope-controlled resource. Furthermore, it ensures that the right-hand side cannot contain destinations, so the structure has to be complete and thus is ready to be read.

In \rref*{\CTyTerm\CSep\CToA}, on the other hand, there is no need to bother with scopes: the operator $\ottkw{to}_{\ottkw{\ltimes} }$ wraps an already completed structure in a trivial ampar whose left side is the structure (that continues to type in the current scope), and right-hand side is unit.

The remaining operators $[[⨞]][[()]], [[⨞]][[Inl]], [[⨞]][[Inr]], [[⨞]]\,\expcons{[[m]]}, [[⨞]][[(,)]]$ from rules \IfFancyRuleNames{of the form \textsc{\CTyTerm\CSep$\ottstype{\lfloor}~\ottstype{\rfloor}$E}}{\textsc{Ty-term-Fill$*$}} are the other destination-filling primitives. They write a hollow constructor to the hole pointed by the destination operand, and return the potential new destinations that are created for new holes in the hollow constructor (or unit if there is none).

One specificity of $\triangleleft\mycirc$, compared to other destination-filling primitives, is that it only accepts a destination of type $[[⌊ U ⌋ \¹ν/]]$---that is, a destination whose corresponding hole has unit mode. This restriction is deliberate. Suppose we have $[[d ⨞· (ampar @ U ⧔ ⌊ T ⌋ \¹ν/)]]$ where $[[d]] \pmb{:} [[⌊ U ⌋ ων]]$, meaning $[[d]]$ refers to a hole behind a modality $\ottstype{!_{[[ων]]}}$. The expression $[[d ⨞· ampar]]$ would return the destination of type $[[⌊ T ⌋ \¹ν/]]$ (such a type signature claims that the associated hole is of mode $[[¹ν]]$) that constituted the right-hand side of $[[ampar]]$. However, this destination actually points to a hole in a structure that has now been moved behind a modality $\ottstype{!_{[[ων]]}}$, and must therefore only accept values of mode $[[ων]]$.

There is no simple way to update the mode annotations of such destinations on the fly when using $\triangleleft\mycirc$ on a destination with a modality $[[m]]$ different than $[[¹ν]]$. The problem becomes even more delicate when foreign destinations---those referencing holes from others ampars---are stored on the right-hand side: their modes, on the other hand, must not be modified at all. To avoid these complexities and ensure soundness, we restrict $\triangleleft\mycirc$ to operated only on destinations whose corresponding holes have mode $[[¹ν]]$.

\paragraph{Putting into practice}

As an exercise, now that we have all the typing rules in place, we can work out the typing tree for the synthetic destination-filling operator $\ottsctor{[[⨞]] (::)}$ and the corresponding synthetic constructor $\ottsctor{(::)}$ that we used since \cref{ssec:map-tr}. We recall that $[[List T]] \btriangleq [[① ⨁ (T ⨂ (List T))]]$. Let's start with the definition and then the typing tree of $\ottsctor{[[⨞]] (::)}$:

\codehere{
\newoperator
  {\ottsctor{[[⨞]] (::)}}{[[⌊ List T ⌋ m ¹ν → (⌊T⌋ m ⨂ ⌊ List T ⌋ m)]]}
  {[[d ⨞(::)]]}{[[d ⨞ Inr ⨞ (,)]]}
}

\noindent\rref*{$\ottnt{A}$-ty}:
\[
\inferrule*[right=\CTySTerm\CSep\CFun]{
  \inferrule*[right=\CFillP]{
    \inferrule*[right=\CFillR]{
      \inferrule*[right=\text{\textnormal{unfold $[[List T]]$ definition on the right}}]{
        \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ d : ¹ν ⌊ List T ⌋ m } ⊢ d : ⌊ List T ⌋ m]]}
      }{[[{ d : ¹ν ⌊ List T ⌋ m } ⊢ d : ⌊ ① ⨁ (T ⨂ (List T)) ⌋ m]]}
    }{[[{ d : ¹ν ⌊ List T ⌋ m } ⊢ d ⨞ Inr : ⌊T ⨂ (List T)⌋ m]]}
  }{[[{ d : ¹ν ⌊ List T ⌋ m } ⊢ d ⨞ Inr ⨞ (,) : ⌊T⌋ m ⨂ ⌊List T⌋ m]]}
}{[[{} ⊢ ˢλ d ¹ν ⟼ d ⨞ Inr ⨞ (,) : ⌊ List T ⌋ m ¹ν → ⌊T⌋ m ⨂ ⌊List T⌋ m]]}
\]

\noindent{}Now, let's move on to $\ottsctor{(::)}$ synthetic constructor:

\codehere{
\newoperator
  {\ottsctor{(::)}}{[[T ¹ν → List T ¹ν → List T]]}
  {[[x ˢ:: xs ]]}{[[from⧔' (alloc ►map d ⟼ (d ⨞(::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ x; dxs ◀ xs)]]}
}

\noindent{}Given that the typing tree will grow very large, we will first type the subexpression $[[B]] \btriangleq [[(d ⨞(::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ x; dxs ◀ xs]]$, then type the whole expression corresponding to operator $\ottsctor{(::)}$.

\noindent\rref*{$\ottnt{B}$-ty}:

\noindent\begin{augmentwidth}{8cm}
\[
  \inferrule*[Right=\CPatP]{
    \inferrule*[right=$\ottnt{A}$-ty]{
      \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ d : ¹ν ⌊ List T ⌋ ¹ν } ⊢ d : ⌊ List T ⌋ ¹ν]]}
    }{[[{ d : ¹ν ⌊ List T ⌋ ¹ν } ⊢ d ⨞(::) : ⌊T⌋ ¹ν ⨂ ⌊List T⌋ ¹ν]]}\quad
    \inferrule*[Right=\CPatU]{
      \inferrule*[right=\CFillLeaf]{
        \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ dx : ¹ν ⌊ T ⌋ ¹ν } ⊢ dx : ⌊ T ⌋ ¹ν]]} \quad
        \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ x : ¹ν T } ⊢ x : T]]}
      }{[[{ dx : ¹ν ⌊ T ⌋ ¹ν }, ¹↑·{ x : ¹ν T } ⊢ dx ◀ x : ①]]} \quad
      \inferrule*[Right=\CFillLeaf]{
        \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ dxs : ¹ν ⌊ List T ⌋ ¹ν } ⊢ dxs : ⌊ List T ⌋ ¹ν]]}\quad
        \inferrule*[Right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ xs : ¹ν List T } ⊢ xs : List T]]}
      }{[[{ dxs : ¹ν ⌊ List T ⌋ ¹ν }, ¹↑·{ xs : ¹ν List T } ⊢ dxs ◀ xs : ①]]}
    }{[[{ dx : ¹ν ⌊ T ⌋ ¹ν }, { dxs : ¹ν ⌊ List T ⌋ ¹ν }, ¹↑·({ x : ¹ν T },{ xs : ¹ν List T }) ⊢ dx ◀ x ; dxs ◀ xs : ①]]}
  }{[[{ d : ¹ν ⌊ List T ⌋ ¹ν },¹↑·({ x : ¹ν T },{ xs : ¹ν List T }) ⊢ (d ⨞(::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ x; dxs ◀ xs : ①]]}
\]
\end{augmentwidth}

\noindent\rref*{$\ottnt{C}$-ty}:

\noindent\begin{augmentwidth}{8cm}
\[
\inferrule*[Right=\CTySTerm\CSep\CFun]{
  \inferrule*[Right=\CTySTerm\CSep\CFun]{
    \inferrule*[Right=\CFromA']{
      \inferrule*[Right=\CUpdA]{
        \inferrule*[right=\CNewA]{\phantom{a}}{[[{} ⊢ alloc : List T ⧔ ⌊List T⌋ ¹ν]]}\quad
        % \inferrule*[Right=(B)]{}{[[{ d : ¹ν ⌊ List T ⌋ ¹ν },¹↑·({ x : ¹ν T },{ xs : ¹ν List T }) ⊢ B : ①]]}
        \inferrule*[Right=$\ottnt{B}$-ty]{\phantom{a}}{[[{ d : ¹ν ⌊ List T ⌋ ¹ν },¹↑·({ x : ¹ν T },{ xs : ¹ν List T }) ⊢ B : ①]]}
      }{[[{ x : ¹ν T },{ xs : ¹ν List T }⊢alloc ►map d ⟼ B : List T ⧔ ①]]}
    }{[[{ x : ¹ν T },{ xs : ¹ν List T }⊢from⧔' (alloc ►map d ⟼ B): List T]]}
  }{[[{ x : ¹ν T }⊢ˢλ xs ¹ν ⟼ from⧔' (alloc ►map d ⟼ B): List T ¹ν → List T]]}
}{[[{}⊢ˢλ x ¹ν ⟼ ˢλ xs ¹ν ⟼ from⧔' (alloc ►map d ⟼ B): T ¹ν → List T ¹ν → List T]]}
\]
\end{augmentwidth}

\bigskip

If look at the typing tree $\rref*{$\ottnt{C}$-ty}$ from the root to the leaves, we see that when going under $\ottkw{upd}_{\ottkw{\ltimes} }$, variable bindings for $[[x]]$ and $[[xs]]$ are multiplied by $[[¹↑]]$ (their age is increased by one). But this is not an issue; in fact, it's required by the typing rule \rref*{\CTyTerm\CSep\CFillLeaf} that the right operand is of age $[[↑]]$ (as always, to prevent scope escape), as we see in typing tree $\rref*{$\ottnt{B}$-ty}$.

Also, as in a regular functional programming language, modes and more specifically ages do \emph{not} show up in the signature of synthetic constructor $\ottsctor{(::)}$; only in its internal typing derivations, because the use of destinations to define $\ottsctor{(::)}$ is, from this point of view, just implementation details hidden to the user.

% $\ottkw{map'}~\pmb{:}~[[(T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①]]$:

% As an exercise, now that we have all the typing rules in place, we can work out the typing tree for the $\ottkw{map'}$ function from \cref{ssec:map-tr}. Now that we are allowed to use ages annotations, the proper signature is
% $\ottkw{map'}~\pmb{:}~[[(T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①]]$:

% \critical{The example is way too big, but it's the smallest real-world example that I could think of. I have no idea at the moment to make it more readable. Suggestions are welcome.}

% \begin{augmentwidth}{34cm}

% $(A)$:
% \[
% \inferrule*[right=\CFillLeaf]{
%   \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ dx : ¹ν ⌊ U ⌋ ¹ν } ⊢ dx : ⌊ U ⌋ ¹ν]]}
%   \inferrule*[right=\CApp]{
%     \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[ω∞]]}{[[{ f : ω∞ T ¹ν → U } ⊢ f : T ¹ν → U]]}
%     \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ x : ¹ν T } ⊢ x : T]]}
%   }{[[{ f : ω∞ T ¹ν → U },{ x : ¹ν T } ⊢ f x : U]]}
% }{[[{ dx : ¹ν ⌊ U ⌋ ¹ν },{ f : ω∞ T ¹ν → U },{ x : ¹↑ T } ⊢ dx ◀ f x : ①]]}
% \]

% $(B)$:
% \[
% \inferrule*[right=\CApp]{
%   \inferrule*[right=\CApp]{
%     \inferrule*[right=\CApp]{
%       \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[ω∞]]}{[[{ rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① } ⊢ rec : (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①]]} \quad
%       \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ f : ¹ν T ¹ν → U } ⊢ f : T ¹ν → U]]}
%     }{[[{ rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U} ⊢ rec f : List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① ]]}\quad
%     \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ xs : ¹ν List T } ⊢ xs : List T]]}
%   }{[[{ rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{ xs : ¹↑ List T } ⊢ rec f xs : ⌊ List U ⌋ ¹ν ¹ν → ①]]}\quad
%   \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ dxs : ¹ν ⌊ List U ⌋ ¹ν } ⊢ dxs : ⌊ List U ⌋ ¹ν]]}
% }{[[{ rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{ xs : ¹↑ List T },{ dxs : ¹ν ⌊ List U ⌋ ¹ν } ⊢ rec f xs dxs : ①]]}
% \]

% $(C)$:
% \[
% \inferrule*[right=\CPatU]{
%   (A) \quad (B)
% }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{ x : ¹↑ T },{ xs : ¹↑ List T },{ dx : ¹ν ⌊ U ⌋ ¹ν },{ dxs : ¹ν ⌊ List U ⌋ ¹ν } ⊢ dx ◀ f x ; rec f xs dxs : ①]]}
% \]

% \[
% \inferrule*[right=\CTySTerm\CSep\CFun]{
%   \inferrule*[right=\CTySTerm\CSep\CFun]{
%     \inferrule*[right=\CTySTerm\CSep\CFun]{
%       \inferrule*[right=\CTySTerm\CSep\CFun]{
%       \inferrule*[]{
%         \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ l : ¹ν List T } ⊢ l : List T]]} \quad
%         \inferrule*[]{
%         \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{ dl : ¹ν ⌊ List U ⌋ ¹ν } ⊢ dl : ⌊ List U ⌋ ¹ν]]}
%         }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{ dl : ¹ν ⌊ List U ⌋ ¹ν } ⊢ dl ⨞ [] : ①]]} \quad
%         \inferrule*[right=\CPatP]{
%           \inferrule*[]{
%             \inferrule*[right=\CVar]{[[¹ν]] ⥶ [[¹ν]]}{[[{ dl : ¹ν ⌊ List U ⌋ ¹ν } ⊢ dl : ⌊ List U ⌋ ¹ν]]}
%           }{[[{ dl : ¹ν ⌊ List U ⌋ ¹ν } ⊢ dl ⨞ (::) : ⌊ U ⌋ ¹ν ⨂ ⌊ List U ⌋ ¹ν]]}\quad
%           (C)
%         }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{ dl : ¹ν ⌊ List U ⌋ ¹ν },{ x : ¹↑ T },{ xs : ¹↑ List T } ⊢ (dl ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ f x ; rec f xs dxs : ①]]}
%       }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{l : ¹↑ List T},{ dl : ¹ν ⌊ List U ⌋ ¹ν } ⊢ l ►case ¹↑ { ˢ[] ⟼ dl ⨞ [], x ˢ:: xs ⟼ (dl ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ f x ; rec f xs dxs } : ①]]}
%       }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U},{l : ¹↑ List T} ⊢ ˢλ dl ¹ν ⟼ l ►case ¹↑ { ˢ[] ⟼ dl ⨞ [], x ˢ:: xs ⟼ (dl ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ f x ; rec f xs dxs } : ⌊ List U ⌋ ¹ν ¹ν → ①]]}
%     }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① },{f : ω∞ T ¹ν → U} ⊢ ˢλ l ¹↑ ⟼ ˢλ dl ¹ν ⟼ l ►case ¹↑ { ˢ[] ⟼ dl ⨞ [], x ˢ:: xs ⟼ (dl ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ f x ; rec f xs dxs } : List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①]]}
%   }{[[{rec : ω∞ (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ① } ⊢ ˢλ f ω∞ ⟼ ˢλ l ¹↑ ⟼ ˢλ dl ¹ν ⟼ l ►case ¹↑ { ˢ[] ⟼ dl ⨞ [], x ˢ:: xs ⟼ (dl ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ f x ; rec f xs dxs } : (T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①]]}
% }{[[{} ⊢ ˢλ rec ω∞ ⟼ ˢλ f ω∞ ⟼ ˢλ l ¹↑ ⟼ ˢλ dl ¹ν ⟼ l ►case ¹↑ { ˢ[] ⟼ dl ⨞ [], x ˢ:: xs ⟼ (dl ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ f x ; rec f xs dxs } : ((T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①) ω∞ → ((T ¹ν → U) ω∞ → List T ¹↑ → ⌊ List U ⌋ ¹ν ¹ν → ①)]]}
% \]

% \end{augmentwidth}

\section{Operational semantics}\label{sec:ectxs-sem}

We give the operational semantics of \destcalculus{} in the \emph{reduction semantics} style that we used in \cref{chap:preli}, that's to say with explicit syntactic manipulation of evaluation contexts. However this time we'll be more thorough. In particular, we'll introduce the grammar of evaluation contexts formally, and we'll also define typing rules for evaluation contexts and commands, so as to prove that typing is preserved throughout the reduction. As before, commands $[[ C[t] ]]$ are used to represent running programs; they're described in detail in \cref{ssec:ty-ectxs-cmd}.
 
But first, we need a class of runtime values (we'll often just say \emph{values}), and their corresponding typing rules, as \destcalculus{} currently lacks any way to represent destinations or holes, or really any kind of value (for instance $[[Inl ()]]$ has been, so far, just syntactic sugar for a term $[[from⧔' (alloc ►map d ⟼ …)]]$). It's a peculiarity of \destcalculus{} that values (in particular, data constructors) are just used during reduction; usually they are also allowed in the term syntax of functional languages as in \CLdm{}.

\subsection{Runtime values and new typing context forms}\label{ssec:runtime-values}

\begin{ottfig}{\caption{Runtime values, new typing context forms, and operators}\label{fig:grammar-val}}
\begin{minipage}{\linewidth}\figtextsize\textit{Grammar extended with values:}\end{minipage}

\bigskip

\[\setlength{\arraycolsep}{0.6ex}\newlength\myskip\setlength{\myskip}{2.38ex}\begin{array}{rrl}
  [[t]], [[u]] &\grammdef& \ldots \grammsep [[v]] \\
  \\
       [[v]] &\grammdef& [[+ h]] \hspace*{\widthof{$[[H ⟨ v2 ❟ v1 ⟩]]$}-\widthof{$[[+ h]]$}}\quad\quad\textit{(hole)} \\
             &|\,& [[- h]] \hspace*{\widthof{$[[H ⟨ v2 ❟ v1 ⟩]]$}-\widthof{$[[- h]]$}}\quad\quad\textit{(destination)} \\
             &|\,& [[H ⟨ v2 ❟ v1 ⟩]] \quad\quad\textit{(ampar value)} \\
             &|\,& [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[ᴇ m v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᵛλ x m ⟼ u]] \\
\end{array}\]

\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Extended grammar of typing contexts:}\end{minipage}

\smallskip

\[\setlength{\myskip}{2.38ex}\begin{array}{rrlcccc}
[[D]] &\grammdef& [[{ }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] &|& [[D1 , D2]] \\% &|& [[D1 + D2]] &|& [[m · D]] \\
[[P]] &\grammdef& [[{ }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] \grammsep [[{ x : m T }]] &|& [[P1 , P2]] \\% &|& [[P1 + P2]] &|& [[m · P]] \\
\hskip \myskip [[G]] &\grammdef& [[{ }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] \grammsep [[{ + h : T n }]] &|& [[G1 , G2]] % &|& [[G1 + G2]] &|& [[m · G]]
\end{array}\]

\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Operations on new typing context forms:}\end{minipage}

\smallskip

{\figtextsize
\bgroup
\renewcommand\arraycolsep{2pt}
%\hfill
\[\begin{array}[c]{rclcc@{\quad}ll}
  [[n']] &\cdot& [[({ + h : T n },G)]] & \btriangleq & [[({ + h : T n' · n }),n'·G]]\\
  [[n']] &\cdot& [[({ - h : m ⌊ T ⌋ n },P)]] & \btriangleq & [[({ - h : n' · m ⌊ T ⌋ n }),n'·P]]&& \phantom{.}^{\dagger}\\
  \\
  [[({ + h : T n }, G1)]] &+& [[G2]] & \btriangleq & [[{ + h : T n },(G1+G2)]] & \textrm{if $[[h]]\notin[[G2]]$}\\
  [[({ + h : T n }, G1)]] &+& [[({ + h : T n' }, G2)]] & \btriangleq & [[{ + h : T n+n' }, (G1+G2)]]\\
  [[({ - h : m ⌊ T ⌋ n }, P1)]] &+& [[P2]] & \btriangleq & [[{ - h : m ⌊ T ⌋ n },(P1+P2)]] & \textrm{if $[[h]]\notin[[P2]]$}&\phantom{.}^{\dagger}\\
  [[({ - h : m ⌊ T ⌋ n }, P1)]] &+& [[({ - h : m' ⌊ T ⌋ n }, P2)]] & \btriangleq & [[{ - h : m+m' ⌊ T ⌋ n }, (P1+P2)]] &&\phantom{.}^{\dagger}\\
  \\
    &&[[-⁻¹({})]] &\btriangleq&  [[{}]]\\
  &&[[-⁻¹({ - h : ¹ν ⌊ T ⌋ n }, D)]] &\btriangleq& [[({ + h : T n }), -⁻¹(D)]] \\
\end{array}\]
% \begin{tabular}[c]{rclcc}
%   $[[n']]$ &$\cdot$& $[[({ + h : T n },G)]]$ & $\btriangleq$ & $[[({ + h : T n' · n }),n'·G]]$\\
%   $[[n']]$ &$\cdot$& $[[({ - h : m ⌊ T ⌋ n },P)]]$ & $\btriangleq$ & $[[({ - h : n' · m ⌊ T ⌋ n }),n'·P]]$ ~~$\phantom{.}^{\dagger}$\\
% \end{tabular}

% \bigskip

% \begin{tabular}[c]{rclcc@{\quad}l}
%   $[[({ + h : T n }, G1)]]$ &$+$& $[[G2]]$ & $\btriangleq$ & $[[{ + h : T n },(G1+G2)]]$ & \textrm{if $[[h]]\notin[[G2]]$}\\
%   $[[({ + h : T n }, G1)]]$ &$+$& $[[({ + h : T n' }, G2)]]$ & $\btriangleq$ & $[[{ + h : T n+n' }, (G1+G2)]]$\\
% \end{tabular}

% \smallskip % N.B.: alignment is not pretty for this last tabular, but we cannot afford more space

% \begin{tabular}[c]{rcl}
%   $[[({ - h : m ⌊ T ⌋ n }, P1)]]$ &$+$& $[[P2]]$  $\btriangleq$  $[[{ - h : m ⌊ T ⌋ n },(P1+P2)]]$  \quad\textrm{if $[[h]]\notin[[P2]]$}~~$\phantom{.}^{\dagger}$\\
%   $[[({ - h : m ⌊ T ⌋ n }, P1)]]$ &$+$& $[[({ - h : m' ⌊ T ⌋ n }, P2)]]$  $\btriangleq$  $[[{ - h : m+m' ⌊ T ⌋ n }, (P1+P2)]]$ ~~$\phantom{.}^{\dagger}$\\
% \end{tabular}
\egroup

\smallskip

% \begin{tabular}[c]{rcl}
%   $[[-⁻¹({})]]$ &$\btriangleq$&  $[[{}]]$\\
%   $[[-⁻¹({ - h : ¹ν ⌊ T ⌋ n }, D)]]$ &$\btriangleq$& $[[({ + h : T n }), -⁻¹(D)]]$ \\
% \end{tabular}

\begin{center}$\phantom{.}^{\dagger}$ : \textit{same rule is also true for $[[G]]$ or $[[D]]$ replacing $[[P]]$}\end{center}

}
\end{ottfig}

\begin{ottfig}{\caption{Typing rules for runtime values (\rref*{\CTyVal})\label{fig:ty-val}}}

\begin{minipage}{\linewidth}\figtextsize\textit{Typing values as terms:}\end{minipage}

\smallskip

\[\bgroup\SetPrefix{\CTyTerm\CSep}
\renewcommand\ottaltinferrule[4]{
      \inferrule*[narrower=0.3,right=#1,#2]
        {#3}
        {#4}
    }
  \drule{Ty-term-Val}\egroup
\]

\bigskip

\bigskip
\hrule
\bigskip

  \bgroup\SetPrefix{}
  \renewcommand\ottaltinferrule[4]{
      \inferrule*[narrower=0.3,right=#1,#2]
        {#3}
        {#4}
    }
  \ottdefnTyXXval{}\egroup
\end{ottfig}

The syntax of runtime values is given in \cref{fig:grammar-val}. It features constructors for all of our basic types, as well as functions (note that in $[[ᵛλ x m ⟼ u]]$, $[[u]]$ is a term, not a value). The more interesting values are holes $[[+ h]]$, destinations $[[- h]]$, and ampars $[[H ⟨ v2 ❟ v1 ⟩]]$, which were not present in \CLdm{}, and that we'll describe in the rest of the section. In order for the operational semantics to use substitution, which requires substituting variables with values, we also extend the syntax of terms to include values, and we add the corresponding typing rule \rref*{\CTyTerm\CSep\CVal} (at the top of \cref{fig:ty-val}).

Destinations and holes are two faces of the same coin, as seen in \cref{ssec:build-up-vocab}, and we must ensure that throughout the reduction, a destination always points to a hole, and a hole is always the target of exactly one destination. Thus, the new idea of our system is to feature \emph{hole bindings} $[[{ + h : T n }]]$ and \emph{destination bindings} $[[{ - h : m ⌊ T ⌋ n }]]$ in typing contexts in addition to the usual variable bindings $[[{ x : m T}]]$. In both cases, we call $[[h]]$ a \emph{hole name}. We complement this by new typing context forms: we now have $[[P]]$, $[[G]]$ and $[[D]]$. $[[P]]$ is the form of typing context used when typing terms; we allow it to contain variable bindings (as before) but also destination bindings. On the other hand, the new context form $[[G]]$ can contain both destination bindings and hole bindings, but \emph{not a destination binding and a hole binding for the same hole name}. $[[G]]$ cannot contain variable bindings either. Finally, a typing context $[[D]]$ can only contain destination bindings.

We extend our previous context operations $+$ and $\cdot$ to act on the new binding forms, as described in \cref{fig:grammar-val}. Context addition is still very partial; for instance, $[[({ + h : T n }) + ({ - h : m ⌊ T ⌋ n' })]]$ is not defined, as $[[h]]$ is present on both sides but with different binding forms.

One of the main goals of \destcalculus{} is to ensure that a hole value is never read. We don't distinguish values with holes from fully-defined values at the syntactic level: instead types prevent holes from being read. The type system maintains this invariant by simply not allowing any hole bindings in the context when typing terms (indeed, typing rules \rref*{\CTyTerm} from \cref{fig:ty-term} still hold, and for instance, typing contexts must be of shape $[[P]]$). In fact, the only place where holes are introduced, is in typing rules for values, in \cref{fig:ty-val}, and more precisely, in the left-hand side $[[v2]]$ of an ampar $[[H ⟨ v2 ❟ v1 ⟩]]$ in rule \rref*{\CTyVal\CSep\CAmpar}.

Specifically, holes come from the operator $\ottshname{\destminus^{\scriptscriptstyle\text{-}1} }$, which represents the hole bindings matching the specified set of destination bindings. It's a partial, pointwise operation on typing contexts $[[D]]$, as defined in \cref{fig:grammar-val}.
Note that $[[-⁻¹D]]$ is undefined if any destination binding in $[[D]]$ has a mode other than $[[¹ν]]$.

Furthermore, in \rref*{\CTyVal\CSep\CAmpar}, the holes $[[-⁻¹D3]]$ and the corresponding destinations $[[D3]]$ of the ampar are bound together and consequently removed from the ampar's typing context: this is how we ensure that, indeed, there's one destination per hole and one hole per destination. We annotate, with subscript $[[H]]$, the set of hole names that an ampar $[[H ⟨ v2 ❟ v1 ⟩]]$ bounds. That's why, in rule \rref*{\CTyVal\CSep\CAmpar}, the resulting ampar carries the set of hole names $[[hnames(D3)]]$ (we use the operator $\ottshname{\mathbf{hnames}(}\smallbullet\ottshname{)}$ to denote the holes names present in a given typing context).

That being said, an ampar can also store destinations that aren't its own, from other scopes; they are represented by $[[¹↑·D1]]$ and $[[D2]]$ in the respective typing contexts of $[[v1]]$ and $[[v2]]$ in\rref*{\CTyVal\CSep\CAmpar}.

Rule \rref*{\CTyVal\CSep\CHole} indicates that a hole must have mode $[[¹ν]]$ in the typing context to be well-typed; in particular mode coercion is not allowed here, and neither is weakening. Only when a hole is behind an exponential, that mode can change to some arbitrary mode $[[n]]$. The mode of a hole constrains which values can be written to it, e.g. in $[[{ + h : T n } ⫦ ᴇ n +h : !n T]]$, only a value with mode $[[n]]$ (more precisely, a value typed in a context of the form $[[n · G]]$) can be written to $[[+h]]$.

Surprisingly, in \rref*{\CTyVal\CSep\CDest}, we see that a destination can be typed with any mode $[[m]]$ coercible to $[[¹ν]]$. We did this to mimic the rule \rref*{\CTyTerm\CSep\CVar} and make the general modal substitution lemma expressible for \destcalculus{}\footnote{Generally, in modal systems, if $[[{ x : m T},P ⊢ u : U]]$ and $[[D ⊢ v : T]]$ then $[[m·D,P ⊢ u[x ≔ v] : U]]$~\cite{bernardy_modality_2020}.\\We have $[[{ x : ω∞ ⌊ T ⌋ n} ⊢ () : ①]]$ and $[[{ -h : ¹ν ⌊ T ⌋ n} ⊢ -h : ⌊ T ⌋ n]]$ so $[[ω∞·({ -h : ¹ν ⌊ T ⌋ n}) ⊢ ()[x ≔ -h] : ①]]$ should be valid.}. We formally proved however that throughout the reduction of a well-typed closed program, $[[m]]$ will never be of multiplicity $[[ω]]$ or age $[[∞]]$---a destination is always linear and of finite age---so mode coercion is never actually used; and we used this at our advantage to make the formal proof of the substitution lemma much easier. The other mode $[[n]]$, appearing in \rref*{\CTyVal\CSep\CDest}, is not the mode of the destination binding; instead it is part of the type $[[⌊ T ⌋ n]]$ and corresponds to the mode of values that we can write to the corresponding $[[+h]]$; so for it no coercion can take place.

\paragraph{Other salient points}
While values are typed in contexts $[[G]]$ allowing both destination and hole bindings, when using a value as a term in \rref*{\CTyTerm\CSep\CVal}, it's only allowed to have free destinations, but no free holes (as we require a typing context of form $[[D]]$ for the value).

Notice, also, that values (used as terms, or not) can't have free variables, since contexts $[[G]]$ only contain hole and destination bindings, but no variable binding. That values are closed is a standard feature of denotational semantics or abstract machine semantics. This is true even for function values (\rref*{\CTyVal\CSep\CFun}), which, also is prevented from containing free holes. Since a function's body is unevaluated, it's unclear what it'd mean for a function to contain holes; at the very least it'd complicate our system a lot, and we are unaware of any benefit supporting free holes in functions could bring.

One might wonder how we can represent a curried function $[[ˢλ x ¹ν ⟼ ˢλ y ¹ν ⟼ x concat y]]$ at the value level, as the inner abstraction captures the free variable $[[x]]$. The answer is that such a function, at value level, is encoded as $[[ᵛλ x ¹ν ⟼ from⧔' (alloc ►map d ⟼ d ⨞ ( λ y ¹ν ⟼ x concat y))]]$, where the inner closure is not yet in value form. As the form $[[d ⨞ ( λ y ¹ν ⟼ x concat y)]]$ is part of term syntax, it's allowed to have free variable $[[x]]$.

\subsection{Evaluation contexts and commands}\label{ssec:ectxs}\label{ssec:ty-ectxs-cmd}

A running program is represented by a \emph{command} $[[ C[t] ]]$, that is, a pair of an evaluation context $[[C]]$, and an (extended) term $[[t]]$ under focus, as in \cref{sec:lincalc-sem}.

The grammar of evaluation contexts is given in \cref{fig:grammar-ectxs}. As for \CLdm{}, an evaluation context $[[C]]$ is the composition of an arbitrary number of focusing components $[[c]]$. It might seem surprising that we don't need a notion of store or state in our semantics to represent the mutation induced by filling destinations. In fact, destination filling only requires a very tame notion of state---so tame that we can simply represent writing to a hole by a substitution \emph{on the evaluation context}, instead of using more heavy store semantics.

It's important to note, however, that unlike in \CLdm{}, command $[[ C[t] ]]$ of \destcalculus{} won't always have a corresponding term. The reason is that the focusing components are all directly derived from the term syntax, except for the \emph{open ampar} focusing component $[[H ᵒᵖ⟨ v2 ❟ ⬜ ⟩]]$ that doesn't have a corresponding term construct. This focusing component indicates that an ampar is currently being processed by $\ottkw{upd}_{\ottkw{\ltimes} }$, with its left-hand side $[[v2]]$ (the structure being built) being attached to the open ampar focusing component, while its right-hand side (containing destinations) is either in subsequent focusing components, or in the term under focus. Ampars being open during the evaluation of $\ottkw{upd}_{\ottkw{\ltimes} }$'s body and closed back afterwards is counterpart to the notion of scopes in typing rules.

\begin{ottfig}{\caption{Grammar of evaluation contexts}\label{fig:grammar-ectxs}}\[{\setlength{\arraycolsep}{1ex}
\begin{array}{rrl}
[[c]] &\grammdef& [[t' ⬜]] \grammsep [[⬜ v]] \grammsep [[⬜ ; u]] \\
      &|\,& [[ ⬜ ►case m { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[⬜ ►case m ( x1 , x2 ) ⟼ u]] \grammsep [[⬜ ►case m ᴇ n x ⟼ u]] \\
      &|\,& [[⬜ ►map x ⟼ t']] \grammsep [[ to⧔ ⬜ ]] \grammsep [[ from⧔ ⬜ ]] \grammsep [[⬜ ⨞· t']] \grammsep [[v ⨞· ⬜]] \grammsep [[⬜ ◀ t']] \grammsep [[v ◀ ⬜]] \\
      &|\,& [[ ⬜ ⨞ () ]] \grammsep [[ ⬜ ⨞ Inl ]] \grammsep [[⬜ ⨞ Inr]] \grammsep [[⬜ ⨞ (,)]] \grammsep [[⬜ ⨞ ᴇ m]] \grammsep [[⬜ ⨞ ( λ x m ⟼ u )]] \\
      &|\,& [[ H ᵒᵖ⟨ v2 ❟ ⬜ ⟩ ]] \quad\quad\textit{(open ampar focusing component)} \\
[[C]] &\grammdef& [[ ⬜ ]] \grammsep [[C ∘ c]] % \grammsep [[C [ h ≔ H v ] ]]
\end{array}
}\]\end{ottfig}

\begin{ottfig}{\caption{Typing rules of evaluation contexts (\rref*{\CTyEctxs}) and commands (\rref*{\CTyCmd})}\label{fig:ty-ectxs}}\begin{minipage}{\linewidth}\begin{augmentwidth}{2cm}
  

    \bgroup\SetPrefix{}
    \renewcommand\ottaltinferrule[4]{
      \inferrule*[narrower=0.3,right=#1,#2]
        {#3}
        {#4}
    }
    \ottdefnTyXXectxs{}\egroup
  \end{augmentwidth}
    \bigskip
    \hrule
    \bigskip
  
    \bgroup\SetPrefix{}
    \renewcommand\ottaltinferrule[4]{
      \inferrule*[narrower=0.3,right=#1,#2]
        {#3}
        {#4}
    }
    \ottdefnTy{}\egroup
\end{minipage}\end{ottfig}

\paragraph{Typing of evaluation contexts}

Evaluation contexts are typed in a context $[[D]]$ that can only contain destination bindings. The typing context $[[D]]$ of an evaluation context $[[C]]$ is exactly the typing context that the term $[[t]]$ has to use to form a valid $[[ C[t] ]]$. In other words, while $[[P ⊢ t : T]]$ \emph{requires} the bindings of $[[P]]$, judgment $[[D ⊣ C : T ↣ U0]]$ \emph{provides} the bindings of $[[D]]$. Typing rules for evaluation contexts, and the sole typing rule for commands, are given in \cref{fig:ty-ectxs}.

An evaluation context has a context type $[[T]]\ottstype{\rightarrowtail}[[U0]]$. The meaning of $[[C]][[:]] [[T]]\ottstype{\rightarrowtail}[[U0]]$ is that given $[[t]][[:]][[T]]$, $[[ C[t] ]]$ returns a value of type $[[U0]]$. Composing an evaluation context $[[C]][[:]][[T]]\ottstype{\rightarrowtail}[[U0]]$ with a new focusing component never affects the type $[[U0]]$ of the future command; only the type $[[T]]$ of the focus is altered.

All typing rules for evaluation contexts can be derived systematically from the ones for the corresponding term (except for the rule \rref*{\CTyEctxs\CSep\COpenAmpar}, as there is no direct term equivalent to the \emph{open ampar} forcusing component). Let's take the rule \rref*{\CTyEctxs\CSep\CPatP} as an example:

\medskip

\sidebysidecodehere{t}{0.55}{
\bgroup\SetPrefix{\CTyEctxs\CSep}
\drule{Ty-ectxs-PatP}\egroup
}{
\bgroup\SetPrefix{\CTyTerm\CSep}
\drule{Ty-term-PatP}\egroup
}

\medskip

\begin{itemize}
  \item the typing context $[[m·D1,D2]]$ in the premise for $[[C]]$ in \rref*{\CTyEctxs\CSep\CPatP} corresponds to $[[m·P1 + P2]]$ in the conclusion of \rref*{\CTyTerm\CSep\CPatP};
  \item the typing context $[[D2,{ x1 : m T1 },{ x2 : m T2 }]]$ in the premise for term $[[u]]$ in \rref*{\CTyEctxs\CSep\CPatP} corresponds to the typing context $[[P2,{ x1 : m T1 },{ x2 : m T2 }]]$ for the same term in \rref*{\CTyTerm\CSep\CPatP};
  \item the typing context $[[D1]]$ in the conclusion for $[[C ∘ (⬜ ►case m (x1 , x2) ⟼ u) ]]$ in \rref*{\CTyEctxs\CSep\CPatP} corresponds to the typing context $[[P1]]$ in the premise for $[[t]]$ in \rref*{\CTyTerm\CSep\CPatP} (the term $[[t]]$ is located where the focus $[[ ⬜]]$ is in \rref*{\CTyEctxs\CSep\CPatP}).
\end{itemize}

We think of the typing rule for an evaluation context as a rotation of the typing rule for the associated term, where the typing contexts of one subterm and the conclusion are swapped, and the typing contexts of the other potential subterms are kept unchanged (with the difference that typing contexts for evaluation contexts are of shape $[[D]]$ instead of $[[P]]$).

\subsection{Reduction semantics}\label{ssec:sem}

% \ExplSyntaxOn
% \NewDocumentCommand{\transformsemname}{m}
% {
%   \tl_set:Nn \l_tmpa_tl { #1 } % Store the input string in a temporary variable
% %  \regex_replace_all:nnN { \\[a-zA-Z]- } { \1 } \l_tmpa_tl
%   \regex_replace_once:nnN { (.+) - Red } { Red - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Focus } { Focus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Unfocus } { Unfocus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Unfocus } { Unfocus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \l_tmpa_tl % Output the transformed string
% }
% \ExplSyntaxOff

% \newlength{\tempwidth}
% \newcommand{\ifnonempty}[2]{%
%   \settowidth{\tempwidth}{#1}%
%   \ifthenelse{\lengthtest{\tempwidth < 1ex}}{}{#2}
% }

% \bgroup
% \renewcommand\arraystretch{1.4}
% \renewcommand\ottaltinferrule[4]{
%   % #4 is conclusion
%   % #3 is premise
%   % #1 is rule name
%   %  & \text{#1}
%   \ensuremath{#4} \ifnonempty{\ensuremath{#3}}{\quad\textit{when}\quad\ensuremath{#3}} \\
% }
% \bgroup\SetPrefix{\CRed\CSep}


Now that every piece is in place, let's focus on the reduction rules of \destcalculus{}, displayed in \cref{fig:sem-full1,fig:sem-full2}. As in \cref{sec:lincalc-sem} for \CLdm{}, focus (F suffix), unfocus (U suffix) and contraction (C suffix) rules of \destcalculus{} are triggered in a purely deterministic fashion. Once a subterm is a value, it cannot be focused on again.

\begin{ottfig}{\caption{Small-step reduction of commands for \destcalculus{} (\rref*{\CSem}, part 1)}\label{fig:sem-full1}}
\begin{minipage}{\linewidth}

\bgroup
\renewcommand{\ottdrulename}[1]{}
\renewcommand\arraystretch{1.5}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \setbox0=\hbox{\ensuremath{#3}\foreverunspace}\ifdim\wd0=0pt ~ \else$\star$\fi & \text{\textsc{#1}} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{lll}}
  {\endarray$}
\makeatother

\begin{augmentwidth}{2.5cm}
\drules{$[[C [ t ] ⟶ C' [ t' ] ]]$}{Small-step evaluation of commands}{%
App-FocusOne,
App-UnfocusOne,
App-FocusTwo,
App-UnfocusTwo,
App-Red,
%
PatU-Focus,
PatU-Unfocus,
PatU-Red,
%
PatS-Focus,
PatS-Unfocus,
PatL-Red,
PatR-Red,
%
PatP-Focus,
PatP-Unfocus,
PatP-Red,
%
PatE-Focus,
PatE-Unfocus,
PatE-Red,
%
ToA-Focus,
ToA-Unfocus,
ToA-Red,
%
FromA-Focus,
FromA-Unfocus,
FromA-Red,
NewA-Red
}
\end{augmentwidth}
\egroup
\begin{center}$\star$~:~only allowed if the term that would become the new focus is not already a value\end{center}

\bigskip
\hrule
\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Name set shift and conditional name shift:}\end{minipage}

\smallskip

$\!\begin{array}{rcl}
[[H ⩲ h']] &\btriangleq& \{ [[h+h']]~|~[[h]]\in [[H]] \}\\
[[h [H ⩲ h'] ]] &\btriangleq& \left\{\begin{array}{ll}[[h+h']] & \text{if}~[[h]]\in[[H]]\\[[h]] & \text{otherwise}\end{array}\right.\end{array}$

\bigskip
\hrule
\bigskip

\begin{minipage}{\linewidth}\figtextsize\textit{Special substitution for open ampars:}\end{minipage}

\smallskip

\hfill$\!\begin{array}{rcll}
  [[ (C ∘ {h} ⨆ H ᵒᵖ⟨ v2 ❟ ⬜ ⟩) [h ≔ H' v'] ]] &=& [[C ∘ H ⨆ H' ᵒᵖ⟨ v2[ h ≔ H' v' ] ❟ ⬜ ⟩ ]] &\\
  [[ (C ∘ c) [h ≔ H' v'] ]] &=& [[ C[h ≔ H' v'] ∘ c ]]&\text{if $[[h]] \notin [[c]]$}
\end{array}$\hfill\phantom{.}

\end{minipage}
\end{ottfig}

\begin{ottfig}{\caption{Small-step reduction of commands for \destcalculus{} (\rref*{\CSem}, part 2)}\label{fig:sem-full2}}\begin{augmentwidth}{2.5cm}\bgroup\renewcommand{\drulesectionhead}[2]{}
\renewcommand{\ottdrulename}[1]{}
\renewcommand\arraystretch{1.5}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \setbox0=\hbox{\ensuremath{#3}\foreverunspace}\ifdim\wd0=0pt ~ \else$\star$\fi & \text{\textsc{#1}} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{lll}}
  {\endarray$}
\makeatother
\drules{}{}{%
UpdA-Focus,
UpdA-Unfocus,
Ampar-Open,
Ampar-Close,
%
FillU-Focus,
FillU-Unfocus,
FillU-Red,
%
FillL-Focus,
FillL-Unfocus,
FillL-Red,
%
FillR-Focus,
FillR-Unfocus,
FillR-Red,
%
FillE-Focus,
FillE-Unfocus,
FillE-Red,
%
FillP-Focus,
FillP-Unfocus,
FillP-Red,
%
FillF-Focus,
FillF-Unfocus,
FillF-Red,
%
FillComp-FocusOne,
FillComp-UnfocusOne,
FillComp-FocusTwo,
FillComp-UnfocusTwo,
FillComp-Red,
%
FillLeaf-FocusOne,
FillLeaf-UnfocusOne,
FillLeaf-FocusTwo,
FillLeaf-UnfocusTwo,
FillLeaf-Red
}\egroup
\vspace*{-0.5cm}
\[
\text{where}\quad\left\{\begin{array}{rcl}
[[h']] &=& [[max(hnames(C) ∪ {h}) + 1]]\\
[[h'']] &=& [[max(H ∪ (hnames(C) ∪ {h})) + 1]] \\
[[h''']] &=& [[max(H ∪ hnames(C)) + 1]]
\end{array}\right.\]

\begin{center}$\star$~:~only allowed if the term that would become the new focus is not already a value\end{center}
\end{augmentwidth}\end{ottfig}

\cref{fig:sem-full1} presents all the rules that don't incur any substitution on the evaluation context.

Reduction rules for function application and pattern-matching are the same as in \CLdm{}. Reduction rules for $\ottkw{to}_{\ottkw{\ltimes} }$ are pretty straightforward: once we have a value at hand, we embed it in a trivial ampar with just unit on the right. Rules for $\ottkw{from}_{\ottkw{\ltimes} }$ are fairly symmetric: once we have an ampar with a value of the shape $[[ᴇ ¹∞ v1]]$ on the right, we can extract both the left and right side out of the ampar shell, into a normal pair.

Finally, $\ottkw{new}_{\ottkw{\ltimes} }$ has only a single rule that transforms it into the ``identity'' ampar object with just one hole on the left, and the corresponding destination on the right.

Rules of \cref{fig:sem-full2} are all related to $\ottkw{upd}_{\ottkw{\ltimes} }$ and destination-filling operators, whose contraction rules modify the evaluation context deeply, instead of just pushing or popping a focusing composant. For that, we need to introduce the special substitution $[[ C[h ≔ H v] ]]$ that is used to update structures under construction, that are attached to open ampar focusing components in the stack. Such a substitution is triggered when a destination $[[-h]]$ is filled in the term under focus, typically in destination-filling primitives reductions, and results in the value $[[v]]$ being written to hole $[[+h]]$. The value $[[v]]$ may contain holes itself (e.g. when the hollow constructor $[[Inl +h'+1]]$ is being written to the hole $[[+h]]$ in \rref*{\CSem\CSep\CFillL\CRed}), hence the set $[[H]]$ tracks the potential hole names introduced by value $[[v]]$, and is used to update the hole name set of the corresponding (open) ampar. Proper definition of $[[ C[h ≔ H v] ]]$ is given at the bottom of \cref{fig:sem-full1}.

\rref*{\CSem\CSep\CFillU\CRed} and \rref*{\CSem\CSep\CFillF\CRed} of \cref{fig:sem-full2} do not create any new hole; they only write a value to an existing one. On the other hand, rules \rref*{\CSem\CSep\CFillL\CRed}, \rref*{\CSem\CSep\CFillR\CRed}, \rref*{\CSem\CSep\CFillE\CRed} and \rref*{\CSem\CSep\CFillP\CRed} all write a hollow constructor to the hole $[[+h]]$ that contains new holes. Thus, we need to generate fresh names for these new holes, and also return a destination for each new hole with a matching name.

The substitution $[[ C[h ≔ H v] ]]$ should only be performed if $[[h]]$ is a globally unique name; otherwise we break the promise of a write-once memory model. To this effect, we allow name shadowing while an ampar is in value form (which is why $\ottkw{new}_{\ottkw{\ltimes} }$ is allowed to reduce to the same $[[{1}⟨+1 ❟ -1⟩]]$ every time), but as soon as an ampar is open (when it becomes an open ampar focusing component), it should have globally unique hole names. This restriction is enforced in rule \rref*{\CTyEctxs\CSep\COpenAmpar} by the premise $[[hnames(C)]] ~\mathtt{\#\#}~ [[hnames(D3)]]$, requiring hole name sets from $[[C]]$ and $[[D3]]$ to be disjoint when an open ampar focusing component is created during reduction of $\ottkw{upd}_{\ottkw{\ltimes} }$. Likewise, any hollow constructor written to a hole should have globally unique hole names. We assume that hole names are natural numbers for simplicity's sake.

To obtain globally fresh names, in the premises of the corresponding rules, we first set\\ $[[h']] = [[max(hnames(C) ∪ {h}) + 1]]$ or similar definitions for $[[h'']]$ and $[[h''']]$ (see at the bottom of \cref{fig:sem-full2}) to find a new unused name\footnote{Sometimes we don't take the smallest unused name possible; it's mostly the result of using whatever made the formal proofs easier. For instance, in the formal proofs with Coq, renaming is implemented in terms of more general permutations, which make it more easily interoperable with typing contexts, represented as functions, but adds a few extra requirements to make lemma hold}. Then we use either the \emph{shifted set} $[[H ⩲ h']]$ or the \emph{conditional shift operator} $[[h [H ⩲ h'] ]]$ as defined at the bottom of \cref{fig:sem-full1} to replace all names or just a specific one with fresh unused name(s).
We extend \emph{conditional shift} $\smallbullet\ottshname{[}[[H ⩲ h']]\ottshname{]}$ to arbitrary values, terms, and typing contexts in the obvious way (keeping in mind that $[[H' ⟨ v2 ❟ v1 ⟩]]$ binds the names in $[[H']]$).

Rules \rref*{\CSem\CSep\CAmparOpen} and \rref*{\CSem\CSep\CAmparClose} dictate how and when an ampar (a value) is converted to an open ampar (a focusing component) and vice-versa, and they make use of the shifting strategy we've just introduced. With \rref*{\CSem\CSep\CAmparOpen}, the hole names bound by the ampar gets renamed to fresh ones, and the left-hand side gets attached to the focusing component $[[H⩲h'' ᵒᵖ⟨ v2[H⩲h''] ❟ ⬜⟩]]$ while the right-hand side (containing destinations) is substituted in the body of the $\ottkw{upd}_{\ottkw{\ltimes} }$ statement (which becomes the new term under focus). The rule \rref*{\CSem\CSep\CAmparClose} triggers when the body of a $\ottkw{upd}_{\ottkw{\ltimes} }$ statement has reduced to a value. In that case, we can close the ampar, by popping the focusing component from the stack $[[C]]$ and merging back with $[[v2]]$ to form an ampar value again.

In rule \rref*{\CSem\CSep\CFillComp\CRed}, we write the left-hand side $[[v2]]$ of an ampar value $[[H ⟨ v2 ❟ v1 ⟩]]$ to a hole $[[+h]]$ that is part of a structure with holes somewhere inside $[[C]]$. This results in the composition of two structures with holes. Because we dissociate $[[v2]]$ and $[[v1]]$ that were previously bound together by the ampar connective ($[[v2]]$ is merged with another structure, while $[[v1]]$ becomes the new focus), their hole names are no longer bound, so we need to make them globally unique, as we do when an ampar is opened with $\ottkw{upd}_{\ottkw{\ltimes} }$. This renaming is carried out by the conditional shift $[[v2[H ⩲ h''] ]]$ and $[[v1[H ⩲ h''] ]]$.

\paragraph{Putting into practice}

Let's reuse the example from \cref{ssec:ty-term} with the synthetic $\ottsctor{(::)}$ \emph{cons} constructor, and see the reduction of expression $[[() ˢ:: Inl ()]]$. Here, $[[Inl ()]]$ is the value form corresponding to \emph{nil} (given our encoding of lists), and we also assume that $[[()]]$ in the expression is the value form of \cref{fig:grammar-val}, not the syntactic constructor defined in \cref{fig:sterm}. Finally, we consider $\ottkw{from}_{\ottkw{\ltimes} }'$ to be a primitive (for brevity) with its contraction rule \rref*{\CSem\CSep\CFromA'\CRed} : $[[C [ from⧔' ({} ⟨ v2 ❟ () ⟩) ] ⟶ C [ v2 ] ]]$ and trivial focus and unfocus rules.

\begin{augmentwidth}{8cm}
\[\bgroup\renewcommand{\arraystretch}{1.5}\begin{array}{cll}
&                 [[ ⬜ [ () ˢ:: Inl () ] ]] & \\
\longrightarrow & [[ ⬜ [ from⧔' (alloc ►map d ⟼ (d ⨞ Inr ⨞ (,)) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()) ] ]] & \text{expanding $\ottsctor{(::)}$ and $[[⨞]]\ottsctor{(::)}$ definitions} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜) [alloc ►map d ⟼ (d ⨞ Inr ⨞ (,)) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl () ] ]] & \rref*{\CFromA'\CFocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ ⬜ ►map d ⟼ (d ⨞ Inr ⨞ (,)) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()) [alloc] ]] & \rref*{\CUpdA\CFocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ ⬜ ►map d ⟼ (d ⨞ Inr ⨞ (,)) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()) [{1} ⟨ +1 ❟ -1 ⟩] ]] & \rref*{\CNewA\CRed} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜) [{1} ⟨ +1 ❟ -1 ⟩ ►map d ⟼ (d ⨞ Inr ⨞ (,)) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()] ]] & \rref*{\CUpdA\CUnfocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {2} ᵒᵖ⟨ +2 ❟ ⬜⟩ ) [(-2 ⨞ Inr ⨞ (,)) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()] ]] & \rref*{\CAmparOpen} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {2} ᵒᵖ⟨ +2 ❟ ⬜⟩ ∘ ⬜ ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()) [-2 ⨞ Inr ⨞ (,) ] ]] & \rref*{\CPatP\CFocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {2} ᵒᵖ⟨ +2 ❟ ⬜⟩ ∘ ⬜ ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl () ∘ ⬜ ⨞ (,)) [-2 ⨞ Inr] ]] & \rref*{\CFillP\CFocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {2} ᵒᵖ⟨ Inr +4 ❟ ⬜⟩ ∘ ⬜ ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl () ∘ ⬜ ⨞ (,)) [-4] ]] & \rref*{\CFillR\CRed}\text{ with sub.  }[[C[2 ≔ {4} Inr +4] ]] \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {4} ᵒᵖ⟨ Inr +4 ❟ ⬜⟩ ∘ ⬜ ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()) [-4 ⨞ (,)] ]] & \rref*{\CFillP\CUnfocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {6,7} ᵒᵖ⟨ Inr (+6, +7) ❟ ⬜⟩ ∘ ⬜ ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()) [(-6, -7)] ]] & \rref*{\CFillP\CRed}\text{ with sub.  }[[C[4 ≔ {6,7} (+6, +7)] ]] \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {6,7} ᵒᵖ⟨ Inr (+6, +7) ❟ ⬜⟩) [(-6, -7) ►case ¹ν (dx, dxs) ⟼ dx ◀ (); dxs ◀ Inl ()] ]] & \rref*{\CPatP\CUnfocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {6,7} ᵒᵖ⟨ Inr (+6, +7) ❟ ⬜⟩) [-6 ◀ (); -7 ◀ Inl ()] ]] & \rref*{\CPatP\CRed} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {6,7} ᵒᵖ⟨ Inr (+6, +7) ❟ ⬜⟩ ∘ ⬜ ; -7 ◀ Inl ()) [-6 ◀ ()] ]] & \rref*{\CPatU\CFocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {7} ᵒᵖ⟨ Inr ((), +7) ❟ ⬜⟩ ∘ ⬜ ; -7 ◀ Inl ()) [()] ]] & \rref*{\CFillLeaf\CRed}\text{ with sub.  }[[C[6 ≔ {} ()] ]] \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {7} ᵒᵖ⟨ Inr ((), +7) ❟ ⬜⟩) [() ; -7 ◀ Inl ()] ]] & \rref*{\CPatU\CUnfocus} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {7} ᵒᵖ⟨ Inr ((), +7) ❟ ⬜⟩) [-7 ◀ Inl ()] ]] & \rref*{\CPatU\CRed} \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜ ∘ {} ᵒᵖ⟨ Inr ((), Inl ()) ❟ ⬜⟩) [()] ]] & \rref*{\CFillLeaf\CRed}\text{ with sub.  }[[C[7 ≔ {} Inl ()] ]] \\
\longrightarrow & [[ (⬜ ∘ from⧔' ⬜) [{} ⟨ Inr ((), Inl ()) ❟ ()⟩] ]] & \rref*{\CAmparClose} \\
\longrightarrow & [[ ⬜ [from⧔' ({} ⟨ Inr ((), Inl ()) ❟ ()⟩)] ]] & \rref*{\CFromA'\CUnfocus} \\
\longrightarrow & [[ ⬜ [Inr ((), Inl ())] ]] & \rref*{\CFromA'\CRed}
\end{array}\egroup\]
\end{augmentwidth}

\bigskip

At the end of the reduction, we get the empty evaluation context with value $[[Inr ((), Inl ())]]$ on focus, which is the value form for the singleton list containing just unit, as expected.

We also see how, throughout the reduction, the contractions of destination-filling operations under focus will trigger global substitutions $[[C[h ≔ H v] ]]$ on the evaluation context $[[C]]$. As said before, these substitutions are responsible for mutating the structure under construction, which is (always) attached on \emph{open ampar} focusing component, somewhere in the evaluation context $[[C]]$.

\subsection{Type safety}\label{ssec:destcalc-type-safety} With the semantics now defined, we can state the usual type safety theorems:

\begin{theorem}[Type preservation]\label{thm:preservation}
  If $[[⊢ C [t] : T]]$ and $[[C[t] ⟶ C'[t'] ]]$ then $[[⊢ C' [t'] : T]]$.
\end{theorem}

\begin{theorem}[Progress]\label{thm:progress}
  If $[[⊢ C [t] : T]]$ and $\forall [[v]], [[C [t] ]] \neq [[ ⬜[v] ]]$ then $\exists [[C']], [[t']].~[[ C [t] ⟶ C' [t'] ]]$.
\end{theorem}

As seen in the example above, a command of the form $[[ ⬜[v] ]]$ cannot be reduced further, as it only contains a fully determined value, and no pending computation. This it is the stopping point of the reduction, and any well-typed command eventually reaches this form.

\section{Formal proof of type safety}\label{sec:formal-proof}

We've proved type preservation and progress theorems with the Coq proof assistant. The artifact containing the formalization of \destcalculus{} and the machine-verified proofs of type-safety (\cref{thm:preservation,thm:progress}) is available at \url{https://doi.org/10.5281/zenodo.14534423}.

Turning to a proof assistant was a pragmatic choice: typing context handling in \destcalculus{} can be quite finicky, and it was hard, without computer assistance, to make sure that we hadn't made mistakes in our proofs. The version of \destcalculus{} that we've proved is written in Ott~\cite{sewell_ott_2007}, and the same Ott file is used as a source for the typing rules displayed in this document, making sure that we've proved the same system as we're presenting; though some simplifications are applied by a script during compilation for visual clarity.

Most of the proof was done by myself with little prior experience with Coq. However the development sped up dramatically thanks to the help of my industrial advisor, who was able to use his long prior experience with Coq to introduce the good core lemmas upon which we could easily base many others.

In total the proof is about 7000 lines long, and contains nearly 500 lemmas. Many of the cases of the type preservation and progress lemmas are similar. To handle such repetitive cases, the use of a large-language-model based autocompletion system has proven quite effective, at the detriment of elegance of the proofs. For instance, we don't have any abstract formalization of semirings: it was more expedient to brute-force the properties we needed by hand.

There are nonetheless a few points of interest in our Coq development, that we present in the next paragraphs.

\paragraph{Representation of typing contexts}

We represent contexts as finite-domain functions, rather than syntactic lists, as this works much better when defining sums of contexts. While there are a handful of finite function libraries in the ecosystem, we needed \emph{finite dependent functions} (because the type of binders depends on whether we're binding a variable name or a hole name). This did not exist, but for our limited purposes, it turned out to be feasible to roll our own (about 1000 lines of proofs). The underlying data type is actual functions: this was simpler to develop, but, as we'll see soon, it makes equality more complex than with a bespoke data type. The definitions of the finite function and context types are provided below:

\begin{unbreakable}
\begin{minted}[linenos,escapeinside=°°]{coq}
Definition Support {A B} (l : list A) (f : forall x:A, option (B x)) : Prop
  := forall (x:A) (y:B x), f x = Some y -> List.In x l.

Record T A B := {
  underlying :> forall x:A, option (B x);
  supported : exists l : list A, Support l underlying;
}.

Inductive name : Type :=
  | name_Var (x:var)
  | name_DH (h:hname).

Definition binding_type_of (n : name) : Type :=
match n with
  | name_Var _ => binding_var
  | name_DH _ => binding_dh
end.

Inductive binding_var : Type :=
  | binding_Var (m:mode) (T:type).

Inductive binding_dh : Type :=
  | binding_Dest (m:mode) (T:type) (n:mode)
  | binding_Hole (T:type) (n:mode).

Definition ctx : Type := T name binding_type_of.
\end{minted}
\end{unbreakable}

\noindent{}The \mintinline{coq}/Support/ predicate indicates that a list is an over-approximation of the effective domain of our partial function. We then use the classical \emph{constructive indefinite description} axiom to retrieve a suitable support list for a given context when needed:
\begin{unbreakable}
\begin{minted}[linenos,escapeinside=°°]{coq}
(* Axiom *)
ClassicalEpsilon.constructive_indefinite_description
  : forall (A : Type) (P : A -> Prop), (exists x : A, P x) -> {x : A | P x}

Definition a_support {A B} (f : T A B) : list A
  := constructive_indefinite_description _ f.(supported).
\end{minted}
\end{unbreakable}
\noindent{}We also introduce \mintinline{coq}/dom ctx/ as a filter over \mintinline{coq}/a_support ctx/ to retain only the elements in the effective domain of our partial function \mintinline{coq}/ctx/.

We also need to reason about equality of typing contexts in proofs. The definition of our partial function type would naturally lend itself well to setoid equality, but unfortunately this is not supported by the inference rules we generate through Ott. As a workaround, we assume the classical \emph{functional extensionality} and \emph{proof irrelevance} axioms, allowing us to state that two contexts are equal as long as they assign the same binding to all variable and hole names:
\begin{unbreakable}
\begin{minted}[linenos,escapeinside=°°]{coq}
(* Axioms *)
ProofIrrelevance.proof_irrelevance : forall (P : Prop) (p1 p2 : P), p1 = p2
FunctionalExtensionality.functional_extensionality_dep
  : forall (A : Type) (B : A -> Type) (f g : forall x : A, B x),
    (forall x : A, f x = g x) -> f = g

Lemma ext_eq' : forall {A B} (f g : T A B), f.(underlying) = g.(underlying) -> f = g.
Proof. intros A B [f f_supp] [g g_supp] h_ext. cbn in *. subst g. f_equal.
  apply proof_irrelevance.
Qed.

Lemma ext_eq : forall {A B} (f g : T A B), (forall x, f x = g x) -> f = g.
Proof. intros * h_ext. apply ext_eq'.
  apply functional_extensionality_dep.
  assumption.
Qed.
\end{minted}
\end{unbreakable}

In effect, we recover a form of extensional equality that behaves morally like setoid equality, but integrates smoothly with the infrastructure we generate through Ott.

\paragraph{Partial addition of contexts}

As seen in \cref{ssec:ty-term,ssec:runtime-values}, addition of contexts is highly partial. For example, two bindings with the same name can only be combined if they also have the same type, and we can't combine a hole binding with a destination binding for the same hole name.

Rather than encoding addition as a binary function returning an optional context---which would integrate poorly with the rules generated by Ott---we model it as a total function to contexts. Instead, to capture failure cases, we introduce a special error mode in our mode ringoid. A binding carrying this error mode signals that the whole context is considered invalid:

\begin{unbreakable}
\begin{minted}[linenos,escapeinside=°°]{coq}
Definition mode : Type := option (mul * age).
Notation "'☠'" := None.
Notation "'¹ν'" := (Some (pair Lin (Fin 0))).
Notation "'¹↑'" := (Some (pair Lin (Fin 1))).
Notation "'¹∞'" := (Some (pair Lin Inf)).
...

Definition mode_plus (m1 m2: mode) : mode :=
match m1, m2 with
  | None, _ => None
  | _, None => None
  | Some (p1, a1), Some (p2, a2) => Some (mul_plus p1 p2, age_plus a1 a2)
end.

Inductive IsValid : mode -> Prop
  := IsValidProof : forall (pa : mul * age), IsValid (Some pa).

Definition ValidOnly (G: ctx) : Prop
  := forall (n : name) (binding: binding_type_of n), G n = Some binding -> IsValid (mode_of binding).
...

Lemma ValidOnly_union_forward
  : forall (G1 G2 : ctx), ValidOnly G1 -> ValidOnly G2 -> G1 # G2 -> ValidOnly (G1 + G2).
Lemma ValidOnly_union_backward
  : forall (G1 G2 : ctx), ValidOnly (G1 + G2) -> ValidOnly G1 /\ ValidOnly G2.
...
\end{minted}
\end{unbreakable}

The cost of this approach is that most typing rules must now include explicit well-formedness conditions---such as \mintinline{coq}/IsValid/ and \mintinline{coq}/ValidOnly/---to ensure that the input contexts do not contain any faulty bindings. Additional lemmas are required to propagate these validity conditions through context operations, using properties such as disjointness to guarantee the absence of errors in the resulting contexts.

\section{Translation of prior work into \destcalculus{} terms and types}\label{sec:transl-minamide}

We announced in \cref{sec:destcalc-intro} that \destcalculus{} subsumes existing systems. Let's see how.

\paragraph{A Functional Representation of Data Structures with a Hole}

Minamide's system~\cite{minamide_functional_1998} introduces two primitives, $\ottkw{happ}$ and $\ottkw{hcomp}$, as well as the hole abstraction form, $\lamh{[[x]]}{[[t]]}$, representing structures with (a) hole, where $[[x]]$ appears exactly once in $[[t]]$.

The paper indicates that hole abstractions cannot have lambda abstraction, application, or case expression operating on the variable of the abstraction, but otherwise allow these constructs to appear in the body of the hole abstraction if they are operating on other variables or values.

In a well-typed, closed program, we can evaluate the body of the hole abstraction (under the $\pmb{\hat{\lambda}}$) as far as possible, until it is almost a value except for the single occurence of $[[x]]$ (we know $[[x]]$ cannot cause this reduction to be stuck, as it cannot appear in the evaluation path of an application or case expression). Fully evaluated hole abstractions then directly translate to our ampar value form: we interpret $\lamh{[[x]]}{[[t]]}$ as $_{[[ {h} ]]}\ottsctor{/}[[t]][ [[x]] \assigneq [[+h]] ]\,\ottsctor{\bbcomma}~[[-h]]\ottsctor{/}$. In other terms, we take the body of the hole abstraction as the left-hand side of the ampar, with the single occurrence of the variable replaced by a hole $[[+h]]$, and just use the corresponding destination $[[-h]]$ as the right-hand side. The type of hole abstractions, $\ottstype{([[T]], [[S]]) hfun}$, is translated as $[[S ⧔ ⌊ T ⌋ ¹ν]]$.

In the general case, where the hole abstraction may not be fully evaluated, we interpret it by the following sequence of operations:

\begin{itemize}
\item spawning a new ampar with $[[alloc]]$;
\item opening it with $\ottkw{upd}_{\ottkw{\ltimes} }$;
\item building the data constructor in which the hole variable appear by a chain of fill expressions $[[d]]\,[[⨞]]\,[[…]]$ or $[[d ◀ …]]$ (following the same recipe that we used to recover normal data constructors or compound constructors from destination-filling operators; see $\ottsctor{(::)}$ at the beginning of \cref{ssec:map-tr});
\item returning, as the result of the body of $\ottkw{upd}_{\ottkw{\ltimes} }$, the destination corresponding to the field of the data constructor in which we want the hole to be;
\item other constructs, like case expressions or applications related to other variables or values are transposed untouched into the body of $\ottkw{upd}_{\ottkw{\ltimes} }$.
\end{itemize}

It's a bit tedious to give a formal description of the translation, but the process is nonetheless systematic, and becomes quite clear on an example, with Minamide's term on the right, and \destcalculus{} one on the left:

\sidebysidecodehere{t}{0.35}{
\newoperator{\ottkw{ha}}{\ottstype{([[T]], [[List T]]) hfun}}
{\ottkw{ha}}{\lamh{[[x]]}{[[ˢ0 ˢ:: x ˢ:: (f ˢ2)]]}}
}{
\newoperator{\ottkw{ha}_{\destcalculus}}{[[(List T) ⧔ ⌊ T ⌋ ¹ν]]}
{\ottkw{ha}_{\destcalculus}}{\!\!\!\begin{array}[t]{l}[[
alloc ►map d ⟼⮒
‥‥(d ⨞(::)) ►case ¹ν ( d0 , dt ) ⟼⮒
‥‥‥‥༼(dt ⨞(::)) ►case ¹ν ( d1 , dt' ) ⟼⮒
‥‥‥‥‥‥d0 ◀ ˢ0 ; dt' ◀ (f ˢ2) ; d1༽
]]\end{array}}}

Once $[[(f ˢ2)]]$ has reduced to a value $[[v]]$, the \destcalculus{}'s version will be evaluated to the promised ampar form $[[H ˢ⟨ˢ0 ˢ:: +h ˢ:: v ❟ -h ⟩]]$.

Because destinations aren't part of~\cite{minamide_functional_1998}, any variable in a program from Minamide's system can be given age $[[∞]]$ (so mode $[[¹∞]]$ or $[[ω∞]]$ depending on whether it is linear or not), except of course the variables bound by hole abstractions.

The translation of $\ottkw{happ}$ and $\ottkw{hcomp}$ is even more direct:

\codehere{
\ottkw{happ}~\pmb{:}~\ottstype{([[T]], [[S]]) hfun~\multimap~[[T]]~\multimap~[[S]]}\\
\newoperator{\ottkw{happ}_{\destcalculus}}{[[S ⧔ ⌊ T ⌋ ¹ν ¹ν → T ¹ν → S]]}
{\ottkw{happ}_{\destcalculus}~[[ampar]]~[[x]]}{[[from⧔' (ampar ►map d ⟼ d ◀ x)]]}\\[\interdefskip]
\ottkw{hcomp}~\pmb{:}~\ottstype{([[T]], [[S]]) hfun~\multimap~([[U]], [[T]]) hfun~\multimap~([[U]], [[S]]) hfun}\\
\newoperator{\ottkw{hcomp}_{\destcalculus}}{[[S ⧔ ⌊ T ⌋ ¹ν ¹ν → T ⧔ ⌊ U ⌋ ¹ν ¹ν → S ⧔ ⌊ U ⌋ ¹ν]]}
{\ottkw{hcomp}_{\destcalculus}~[[ampar1]]~[[ampar2]]}{[[ampar1 ►map d ⟼ d ⨞· ampar2]]}}

With this translation we can completly express Minamide's system in \destcalculus{} with no loss of flexiblity.

\paragraph{The Functional Essence of Imperative Binary Search Trees}

The portion of the work from~\citet{lorenzen_searchtree_2024,leijen_tail_2025} about \emph{first-class constructor contexts} is fairly similar, in both expressiveness and presentation, to Minamide's \emph{hole abstractions}. A hole abstraction $\lamh{[[x]]}{[[t]]}$ is written $\ottkw{ctx}~[[t]] [ [[x]] \assigneq \_ ]$, whose body is the same as $[[t]]$ but with an underscore $\_$ (or a square $\square$ in some versions of their work) in place of the named hole variable $[[x]]$. Similarly, $\ottkw{happ}$ is replaced by the $\mathop{+\!+\!.}$ operator (notice the period), and $\ottkw{hcomp}$ by the $\mathop{+\!+}$ operator. Consequently, the translation for terms from~\cite{lorenzen_searchtree_2024,leijen_tail_2025} system to ours is the same as for Minamide's system.

For types,~\cite{lorenzen_searchtree_2024,leijen_tail_2025} only define $\ottstype{\mathop{ctx\!<\!}[[T]]\mathop{\!>}}$, where the hole and the resulting structure (once the hole will be filled) are of the same type $[[T]]$. This translates to $[[T ⧔ ⌊ T ⌋ ¹ν]]$ in \destcalculus{}.

\section{Implementation of \destcalculus{} using in-place memory mutations}\label{sec:implem-destcalculus}

The formal language presented in \cref{sec:syntax-type-system,sec:ectxs-sem} is not meant to be implemented as-is. Practical implementation of most \destcalculus{}'s ideas will be the focus of \cref{chap:dps-haskell,chap:ext-linear-nonlinear}.

First, \destcalculus{} doesn't have recursion, this would have obscured the formal presentation of the system. However, adding a standard form of recursion doesn't create any complication.

Secondly, ampars are not managed linearly in \destcalculus{}; only destinations are. That is to say that an ampar can be wrapped in an exponential, e.g. $[[ˢᴇ ων {h} ˢ⟨ ༼ˢ0 ˢ:: +h༽ ❟ -h ⟩]]$ (representing a difference list $0 \ottsctor{::} \holesq$ that can be used non-linearly), and then used twice, each time in a different way:

\begin{minipage}{0.50\linewidth}\codehere{[[
༼ˢᴇ ων {h} ˢ⟨ ˢ0 ˢ:: +h ❟ -h ⟩༽ ►case ¹ν ᴇ ων x ⟼⮒
‥‥let x1 ≔ x append ˢ1 in⮒
‥‥let x2 ≔ x append ˢ2 in⮒
‥‥‥‥toList (x1 concat x2)
]]}\end{minipage}$[[⟶*]]\quad[[༼ˢ0 ˢ:: ༼ˢ1 ˢ:: ༼ˢ0 ˢ:: ༼ˢ2 ˢ:: ˢ[]༽༽༽༽ ]]$\\[\interdefskip]

It may seem counter-intuitive at first, but this program is valid and safe in \destcalculus{}. Thanks to the renaming discipline we detailed in \cref{ssec:sem}, every time an ampar is operated over with $\ottkw{upd}_{\ottkw{\ltimes} }$, its hole names are renamed to fresh ones. One way we can implement this in practice is to allocate a fresh copy of $[[x]]$ every time we call $\ottkw{upd}_{\ottkw{\ltimes} }$ on it (recall that $\ottkw{append}$ is implemented in terms of $\ottkw{upd}_{\ottkw{\ltimes} }$), in a \emph{copy-on-write} fashion. This way filling destinations is still implemented as mutation.

However, this is a long way from the efficient implementation promised in \cref{sec:working-with-dests}. Copy-on-write can be optimized using fully-in-place functional programming ~\cite{lorenzen_fp_2023}, where, thanks to reference counting, we don't need to perform a copy when the difference list isn't aliased. This idea is further explored, for structure with holes specifically, in Chapter 7 of their more recent work~\cite{leijen_tail_2025}. But that won't be the direction we will follow in the following development, as we don't want to deal explicitly with reference counting.

An alternative is to refine the linear type system further in order to guarantee that ampars are unique and avoid copy-on-write altogether. We held back from doing that in the formalization of \destcalculus{} as, again, it obfuscates the presentation of the system without adding much in return.

To make ampars linear, we follow the recipe we developed in \cref{sec:linear-scopes}, highly inspired from~\cite{ spiwack_linearly_2022,spiwack_linear_scopes_2023}, and introduce a new built-in type $[[Token]]$, together with the primitive $\ottkw{dup}$, $\ottkw{drop}$, and $\ottkw{withToken}$. We also switch $[[alloc]]$ for $[[allocIP]]$:% :

\codehere{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
\ottkw{dup} ~\pmb{:}~ [[Token ¹ν → Token ⨂ Token]]\\
\ottkw{drop} ~\pmb{:}~ [[Token ¹ν → ①]]\\
\ottkw{withToken} ~\pmb{:}~ [[(Token ¹∞ → !ω∞ T) ¹ν → !ω∞ T]]\\
[[allocIP]] ~\pmb{:}~ [[Token ¹ν → T ⧔ ⌊ T ⌋ ¹ν]]
\end{array}}

Ampars produced by $[[allocIP]]$ have a linear dependency on a $[[Token]]$. If the same ampar, originally created with $[[allocIP tok]]$, were to be used twice in a block $[[t]]$, then $[[t]]$ would require a typing context $\{[[{tok : ω∞ Token}]]\}$, thus the block would be rejected by $\ottkw{withToken}$. In the other hand, duplicating $[[tok]]$ into $[[tok1]]$ and $[[tok2]]$ first, and then using each new token to create a different ampar would be linearly valid.

Now that ampars are managed linearly, we can change the allocation and renaming mechanisms:
\begin{itemize}
  \item the hole name for a new ampar is chosen fresh right from the start (this corresponds to a new heap allocation);
  \item adding a new hollow constructor still requires freshness for its hole names (this corresponds to a new heap allocation too);
  \item Using $\ottkw{upd}_{\ottkw{\ltimes} }$ over an ampar and filling destinations or composing two ampars using $\triangleleft\mycirc$ no longer requires any renaming: we have the guarantee that all the names involved are globally fresh, and can only be used once, so it actually corresponds to an in-place memory update.
\end{itemize}

In \cref{chap:dps-haskell,chap:ext-linear-nonlinear}, dedicated to the implementation of DPS in a functional setting, we will treat ampars as linear resources, in the same style as \destcalculus{} extended with $[[Token]]$s and $[[allocIP]]$.

\paragraph{From binary products and sums to more efficient memory forms}

In \destcalculus{} we only have binary product in sum types. However, it's very straightforward to extend the language and implement destination-based building for n-ary sums of n-ary products, with constructors for each variant having multiple fields directly, instead of each field needing an extra indirection as in the binary sum of products $[[① ⨁ (S ⨂ (T ⨂ U))]]$. In fact, we will do that as soon as next chapter. However, we still require field's values to be represented by pointers. It's a crucial for the semantics of the $\triangleleft\mycirc$ operator.

% TODO: Conclusion
% TODO: utile au rapporteur pour faire son rapport

\section{Conclusion}\label{sec:destcalc-conclusion}

\destcalculus{} is a purely functional calculus which treats destinations as first class values that can be passed around and used in very flexible ways. It supports data structures with multiple holes, and allows both composition of data structures with holes and storing destinations in data structures that, themselves, have holes.

With \destcalculus{}, we can now apply some techniques and algorithms traditionally confined to imperative languages within a purely functional setting, all while preserving safety guarantees. Indeed, thanks to a linear type system augmented with a system of ages, the mutations introduced by destination use are opaque and controlled.

However, we don't anticipate that a system of ages like the one of \destcalculus{} will actually be used in a programming language: it's unlikely that destinations are so central to the design of a programming language that it's worth baking them so deeply in the type system. Perhaps a compiler that makes heavy use of destinations in its optimizer could use \destcalculus{} as a typed intermediate representation. But, more realistically, our expectation is that \destcalculus{} can be used as a theoretical framework to analyze destination-passing systems: if a destination passing system can be expressed in terms of \destcalculus{} primitives, then it is sound.

In the chapters ahead, we aim to determine which (hopefully moderate) restrictions we have to impose on \destcalculus{}'s flexiblity to port most of its fundamental concepts safely into a real-world functional programming language, namely, Haskell---whose type system is not as rich as \destcalculus{}'s one, designed on purpose for destination control.
