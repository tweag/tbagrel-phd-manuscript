\chapter{A formal functional language based on first-class destinations: \destcalculus{}}

In the previous chapter we laid out important building blocks for the functional language that we'll develop here, namely \destcalculus{}. But first, let's introduce destination-passing style programming, and why it matters in a functional setting.

\section{Destination-passing style programming}\label{sec:intro}

As brushed out in the introduction, in destination-passing style, a function doesn't return a value: it takes as an argument a location where the output of the function ought to be written. In this chapter, we will denote destinations by $[[⌊ T ⌋ ¹ν]]$ where $[[T]]$ is the type of what can be stored in the destination. A function of type $[[T → U]]$ would have signature $[[T → ⌊ U ⌋ ¹ν → ①]]$ instead when transformed into destination-passing style, where instead of returning a $[[U]]$, it consumes a destination of type $[[⌊U⌋ ¹ν]]$ instead and returns just \emph{unit} (type $[[①]]$). This style is common in system programming, where destinations $[[⌊ U ⌋ ¹ν]]$ are more commonly known as \emph{out parameters} (in C, $[[⌊ U ⌋ ¹ν]]$ would typically be a pointer of type $\ottstype{U*}$).

The reason why system programs rely on destinations so much is that using destinations can save calls to the memory allocator. If a function returns a $[[U]]$, it has to allocate the space for a $[[U]]$. But with destinations, the caller is responsible for finding space for a $[[U]]$. The caller may simply ask for the space to the memory allocator, in which case we've saved nothing; but it can also reuse the space of an existing $[[U]]$ which it doesn't need anymore, or space in an array, or even space in a region of memory that the allocator doesn't have access to, like a memory-mapped file. In fact, as we'll see in \cref{sec:syntax-type-system}, destination-passing style is always more general than its direct style counterpart; there is no drawbacks to use it as we can always recover regular direct style functions in a systematic way from the destination-passing ones.

So far we mostly considered destination passing in imperative contexts, such as systems programming, but we argue that destination passing also presents many benefits for functional programming languages, even pure ones. Where destinations truly shine in functional programming is that they let us borrow imperative-like programming techniques to get a more expressive language, with more control over processing, and with careful attention, we can still preserve the nice properties of functional languages such as purity and (apparent) immutability.

Thus, the goal here will be to extend a rather-standard functional programming language just enough to be able to build immutable structures by destination passing without endangering purity and memory safety. Destinations in that particular setting become \emph{write-once-only} references into a structure that contains holes and that cannot be read yet. Quite more restrictive than C pointers (that can be used in read and write fashion without restrictions), but still powerful enough to bring new programming techniques into the functional world.

More precisely, there are two key elements that make the extra expressiveness of destination passing in functional contexts:
\begin{itemize}
\item structures can be built in any order. Not only from the leaves to the root, like in ordinary functional programming, but also from the root to the leaves, or any combination thereof. This can be done in ordinary functional programming using function composition in a form of continuation-passing; however destinations act as a very direct optimization of this scheme. This line of work was pioneered by~\citet{minamide_functional_1998};
\item when destinations are first-class values, they can be passed and stored like ordinary values. The consequence is that not only the order in which a structure is built is arbitrary, this order can be determined dynamically during the runtime of the program. Here is the true novelty of our approach.
\end{itemize}

In this chapter, we introduce \destcalculus{}, as a pure functional calculus that is based on the very concept of destinations at its core. We intend \destcalculus{} to serve as a foundational, theoretical calculus to reason about safe destinations in a functional setting; thus, we will only cover very briefly the implementation concerns in this chapter, as \destcalculus{} is not really meant to be implemented as a real programming language. Actual implementation of destination passing in an existing functional language will be the focus of the next chapters. 

\destcalculus{} is supposed to subsume all the functional systems with destinations from the literature, from \cite{minamide_functional_1998} to \cite{lorenzen_searchtree_2024}. As such we expect that these systems or their extensions can be justified simply by giving them a translation into \destcalculus{}, in order to get all the safety results and metatheory of \destcalculus{} for free. In particular, we proved type safety theorems for \destcalculus{} with the Coq proof assistant, as described in \cref{sec:formal-proof}.

\section{Working with destinations}\label{sec:working-with-dests}

Let's introduce and get familiar with \destcalculus{}, our modal, linear, simply typed $\lambda$-calculus with destinations $\lambda$-calculus with destination. We borrow most of the syntax of the previous chapter, especially from \CLdm{}. We still use linear logic's $[[T⨁U]]$ and $[[T⨂U]]$ for sums and products, and linear function arrow $\ottstype{\multimap}$, since \destcalculus{} is linearly typed. The main difference with previous calculi being that \destcalculus{} doesn't have first-class data constructors like $[[ˢInl t]]$ or $[[ˢ(t1, t2)]]$, as they are replaced with the more general destination-filling operators that we'll discover in the next paragraphs.

\subsection{Building up a vocabulary}\label{ssec:build-up-vocab}

\activespaces

In its simplest form, destination passing, much like continuation passing, is using a location, received as an argument, to write a the output value instead of returning it proper. Instead of a linear function with signature $[[T ¹ν → U]]$, in \destcalculus{} you would have $[[T ¹ν → ⌊ U ⌋ ¹ν ¹ν → ①]]$, where $[[⌊ U ⌋ ¹ν]]$ is read “destination for type $[[U]]$”. For instance, here is a destination-passing version of the identity function:

\codehere{\newoperator
{\ottkw{dId}}{[[T ¹ν → ⌊ T ⌋ ¹ν ¹ν → ①]]}
{\ottkw{dId}~[[x]]~[[d]]}{[[d ◀ x]]}}

We think of a destination as a reference to an uninitialized memory location, and $[[d ◀ x]]$ (read “fill $[[d]]$ with $[[x]]$”) as writing $[[x]]$ to the memory location pointed to by $[[d]]$.

The form $[[d ◀ x]]$ is the simplest way to use a destination: with that we fill it with a whole, complete value. But we don't have to fill a destination with a complete value in a single step; instead, we can fill a destination piecemeal by specifying just the outermost constructor that we want to fill it with:

\codehere{\newoperator
{\ottkw{fillWithInl}}{[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]}
{\ottkw{fillWithInl}~[[d]]}{[[d ⨞ Inl]]}}

In this example, we're filling a destination for type $[[T ⨁ U]]$ by setting the outermost constructor to left variant $[[Inl]]$. We think of $[[d ⨞ Inl]]$ (read “fill $[[d]]$ with $[[Inl]]$”) as allocating memory to store a block of the form $[[Inl]]~\holesq$, write the address of that block to the location that $[[d]]$ points to, and return a new destination of type $[[⌊ T ⌋ ¹ν]]$ pointing to the uninitialized argument of $[[Inl]]$. Uninitialized memory, when part of a structure or value, like $\holesq$ in $[[Inl]]~\holesq$, is called a \emph{hole}.

Notice that with $\ottkw{fillWithInl}$ we are constructing the structure from the outermost constructor inward: we've written a value of the form $[[Inl]]~\holesq$ into a hole, but we have yet to describe what goes in the new hole $\holesq$. Such data constructors with uninitialized arguments are called \emph{hollow constructors}\footnote{The full triangle $[[◀]]$ is used to fill a destination with a fully-formed value, while the \emph{hollow} triangle $[[⨞]]$ is used to fill a destination with a \emph{hollow constructor}.}. This is opposite to how functional programming usually works, where values are built from the innermost constructors outward: first we make a value $[[v]]$ and only then can we use $[[Inl]]$ to make $[[Inl v]]$. This is one of the two key ingredients in the expressiveness of destination passing that we mentioned earlier.

Yet, everything we've shown so far could have been done with continuations. So it's worth asking: how are destinations different from continuations? Part of the answer lies in our intention to effectively implement destinations as pointers to uninitialized memory (see~\cref{sec:implementation}). But where destinations really differ from continuations is when one has several destinations at hand. Then they have to fill \emph{all} the destinations; whereas when one has multiple continuations, they can only return to one of them. Multiple destination arises when a destination for a pair gets filled with a hollow pair constructor:

\codehere{\newoperator
{\ottkw{fillWithPair}}{[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]}
{\ottkw{fillWithPair}~[[d]]}{[[d ⨞ (,)]]}}

After using $\ottkw{fillWithPair}$, both the first field \emph{and} the second field must be filled, using the destinations of type $[[⌊ T ⌋ ¹ν]]$ and $[[⌊ U ⌋ ¹ν]]$ respectively. The key remark here is that $\ottkw{fillWithPair}$ couldn't exist if we replaced destinations by continuations, as we couldn't use both returned continuations easily.

We can already note there that there is a sort of duality between destination-filling operators and associated constructors. Usual $[[Inl]]$ constructor has signature $[[T ¹ν → T ⨁ U]]$, while destination-filling $[[⨞]][[Inl]]$ has signature $[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]$. Similarly, pair constructor $[[(,)]]$ has signature $[[T ¹ν → U ¹ν → T⨂U]]$, while destination-filling $[[⨞]][[(,)]]$ has signature $[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]$. Assuming some flexibility with currying, we see that the types of arguments and results switch sides around the arrow, and get wrapped/unwrapped from $\ottstype{\lfloor\smallbullet\rfloor}$ when we go from the constructor to the destination-filling operator and vice-versa. This observation will generalize to all destination-filling operators and the corresponding constructors.

\paragraph{Structures with holes}
It is crucial to note that while a destination is used to build a structure, the destination refers only to a specific part of the structure that hasn't been defined yet; not the structure as a whole. Consequently, the (root) type of the structure being built will often be different from the type of the destination at hand. A destination of type $[[⌊ T ⌋ ¹ν]]$ only indicates that some bigger structure has at least a hole of type $[[T]]$ somewhere in it. The type of the structure itself never appears in the signature of destination-filling functions (for instance, using $\ottkw{fillWithPair}$ only indicates that the structure being operated on has a hole of type $[[T ⨂ U]]$ that is being written to).

Thus, we still need a type to tie the structure under construction --- left implicit by destination-filling primitives --- with the destinations representing its holes. To represent this, \destcalculus{} introduces a type $[[S ⧔ ⌊ T ⌋ ¹ν]]$ for a structure of type $[[S]]$ missing a value of type $[[T]]$ to be complete. There can be several holes in $[[S]]$ --- resulting in several destinations on the right hand side --- and as long as there remains holes in $[[S]]$, it cannot be read. For instance, $[[S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]$ represents a $[[S]]$ that misses both a $[[T]]$ and a $[[U]]$ to be complete (thus to be readable). The right hand side of $\ottstype{\ltimes}$ is not restricted to pair and destination types only; it can be of arbitrarily complex type.

The general form $[[S ⧔ T ]]$ is read “$[[S]]$ ampar $[[T]]$”. The name “ampar” stands for “asymmetric memory par”. The “par” $\ottstype{\parr}$ operator originally comes from (classical) linear logic, and is an associative and commutative operator that can be used in place of linear implication and is slightly more flexible: $[[T ⊸ S]]$ is equivalent to $\ottstype{T^\perp \parr S}$ or $\ottstype{S \parr T^\perp}$. Similarly, $[[T1 ⊸ T2 ⊸ S]]$ is equivalent to $\ottstype{T1^\perp \parr T2^\perp \parr S}$. Function input's types ---which are in negative position--- are wrapped in the dualizing operator $\ottstype{\smallbullet^\perp}$ when a function is put into ``par'' form.
Here we take sort of the same approach: \citet{minamide_functional_1998} first observed that structures with holes are akin to linear functions $[[T ⊸ S]]$, where $[[T]]$ represents the missing part; so we decide to represent them in a more flexible fashion under the form $[[S ⧔ ⌊ T ⌋ ¹ν]]$, so that destinations are made into a first-class type $[[⌊ T ⌋ ¹ν]]$. The asymmetric nature of our memory par is a bit disappointing, but it comes from limitations that we are in an intuitionistic setting, not a classical one like $\ottstype{\parr}$ needs.

Destinations, albeit being first-class, always exist within the context of a structure with holes. A destination is both a witness of a hole present in the structure, and a handle to write to it. Crucially, destinations are otherwise ordinary values. To access the destinations of an ampar, \destcalculus{} provides a $\ottkw{upd}_{\ottkw{\ltimes} }$ construction, which lets us apply a function to the right-hand side of the ampar. It is in the body of $\ottkw{upd}_{\ottkw{\ltimes} }$ that functions operating on destinations can be called to update the structure:

\codehere{
  \newoperator
  {\ottkw{fillWithInl'}}{[[S ⧔ ⌊ T ⨁ U ⌋ ¹ν ¹ν → S ⧔ ⌊ T ⌋ ¹ν]]}
  {\ottkw{fillWithInl'}~[[x]]}{[[x ►map d ⟼ fillWithInl d]]}
  \newoperator
  {\ottkw{fillWithPair'}}{[[S ⧔ ⌊ T ⨂ U ⌋ ¹ν ¹ν → S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]}
  {\ottkw{fillWithPair'}~[[x]]}{[[x ►map d ⟼ fillWithPair d]]}
}

To tie this up, we need a way to introduce and to eliminate structures with holes. Structures with holes are introduced with $[[alloc]]$ which creates a value of type $[[T ⧔ ⌊ T ⌋ ¹ν]]$. $[[alloc]]$ is a bit like the identity function: it is a hole (of type $[[T]]$) that needs a value of type $[[T]]$ to be a complete value of type $[[T]]$. Memory-wise, it is an uninitialized block large enough to host a value of type $[[T]]$, and a destination pointing to it. Conversely, structures with holes are eliminated with\footnote{As the name suggest, there is a more general elimination $\ottkw{from}_{\ottkw{\ltimes} }$. It will be discussed in~\cref{sec:syntax-type-system}.} $\ottkw{from}_{\ottkw{\ltimes} }' : [[S⧔① ¹ν → S]]$: if all the destinations have been consumed and only unit remains on the right side, then $[[S]]$ no longer has holes and thus is just a normal, complete structure.

Equipped with these, we can, for instance, derive traditional constructors from piecemeal filling. In fact, \destcalculus{} doesn't have primitive constructor forms, constructors in \destcalculus{} are syntactic sugar. We show here the definition of $[[Inl]]$ and $[[(,)]]$, but the other constructors are derived similarly. Operator $\patu$, present in second example, is used to chain operations returning unit type $[[①]]$.

\codehere{\newoperator
{[[Inl]]}{[[T ¹ν → T ⨁ U]]}
{[[ˢInl x]]}{[[from⧔' ((alloc @ (T ⨁ U)⧔ ⌊ T ⨁ U ⌋ ¹ν) ►map d ⟼ d ⨞ Inl ◀ x)]]}
\newoperator
{[[(,)]]}{[[T ¹ν → U ¹ν → T ⨂ U]]}
{[[ˢ(x, y)]]}{[[from⧔' ((alloc @ (T ⨂ U)⧔ ⌊ T ⨂ U ⌋ ¹ν) ►map d ⟼ (d ⨞ (,)) ►case ¹ν (d1, d2) ⟼ d1 ◀ x; d2 ◀ y)]]}}


\paragraph{Memory safety and purity}

We must reassure the reader here. Of course, using destinations in an unrestricted fashion is not memory safe. We need a linear discipline on destinations for them to be safe. Otherwise, we can encounter two sorts of issues:

\begin{itemize}
\item if destinations are not written at least once, as in:

  \codehere{\newoperator
  {\ottkw{forget}}{[[T]]}
  {\ottkw{forget}}{[[from⧔' ((alloc @ T ⧔ ⌊ T ⌋ ¹ν) ►map d ⟼ ())]]}}

  then the result of $\ottkw{forget}$ would result in reading the location pointed to by a destination that we never used, in other words, reading uninitialized memory. That's clearly the biggest issue we must prevent;
\item if destinations are written several times, as in:

  \codehere{\newoperator
  {\ottkw{ambiguous1}}{[[Bool]]}
  {\ottkw{ambiguous1}}{[[from⧔' ((alloc @ Bool ⧔ ⌊ Bool ⌋ ¹ν) ►map d ⟼ d ◀ true; d ◀ false)]]}
  \newoperator
  {\ottkw{ambiguous2}}{[[Bool]]}
  {\ottkw{ambiguous2}}{[[from⧔' ((alloc @ Bool ⧔ ⌊ Bool ⌋ ¹ν) ►map d ⟼ let x ≔ (d ◀ false) in d ◀ true; x)]]}}

  then we have $\ottkw{ambiguous1}$ that returns $[[false]]$ and $\ottkw{ambiguous2}$ that returns $[[true]]$ due to evaluation order, so we break \emph{let expansion} that is supposed to be valid in a functional programming language. We ought to keep that property true for \destcalculus{}.
\end{itemize}

Actually, we'll see in \cref{sec:scope-escape-dests} that a linear discipline is not even enough to ensure fully safe use of destinations in \destcalculus{}. But before dealing with that, let's get more familiar with \destcalculus{} keywords and operators through more complex examples.

\subsection{Tail-recursive map}\label{ssec:map-tr}

Let's see how destinations can be used to build usual data structures. For these examples, we suppose that \destcalculus{} has equirecursive types and a fixed-point operator. These aren't part of the formal system of \cref{sec:syntax-type-system} but don't add any complication.

\paragraph{Linked lists}

We define lists as a fixpoint, as usual: $[[List T]] \btriangleq [[① ⨁ (T ⨂ (List T))]]$. For convenience, we also define filling operators $\triangleleft\ottsctor{[]}$ and $\triangleleft\ottsctor{(::)}$, as macros that use the primitive destination-filling operators for sum, product and unit types:

\sidebysidecodehere{b}{0.50}{
\newoperator
  {\triangleleft\ottsctor{[]}}{[[⌊ List T ⌋ ¹ν ¹ν → ①]]}
  {[[d ⨞ [] ]]}{[[d ⨞Inl ⨞()]]}
}{
\newoperator
  {\triangleleft\ottsctor{(::)}}{[[⌊ List T ⌋ ¹ν ¹ν → ⌊ T⌋ ¹ν ⨂ ⌊ List T ⌋ ¹ν]]}
  {[[d ⨞ (::)]]}{[[d ⨞Inr ⨞(,)]]}
}

Just like we did in~\cref{ssec:build-up-vocab} we can recover traditional constructors systematically from destination-filling operators, using $[[alloc]]$, $\ottkw{upd}_{\ottkw{\ltimes} }$ and $\ottkw{from}_{\ottkw{\ltimes} }'$:

\codehere{\newoperator
{\ottsctor{(::)}}{[[T ⨂ (List T) ¹ν → List T]]}
{[[x ˢ:: xs]]}{\!\!\!\begin{array}[t]{l}[[from⧔' ((alloc @ (List T) ⧔ ⌊ List T ⌋ ¹ν) ►map d ⟼⮒
‥‥‥‥‥‥༼(d ⨞ (::)) ►case ¹ν (dx, dxs) ⟼ dx ◀ x ; dxs ◀ xs༽)]]\end{array}}}

\paragraph{A tail-recursive map function}

List being ubiquitous in functional programming, the fact that the most natural way to write a $\ottkw{map}$ function on lists isn't tail recursive (hence consumes unbounded stack space), is unpleasant. Map can be made tail-recursive in two passes: first build the result list in reverse, then reverse it. But thanks to destinations, we'll be able to avoid this two-pass process altogether, as they let us extend the tail of the result list directly.
We give the complete implementation in \cref{fig:impl-map-tr}.

The tail-recursive function is $\ottkw{map'}$, it has type $
[[(T ¹ν → U) ω∞ → List T ¹ν → ⌊ List U ⌋ ¹ν ¹ν → ①]]
$. That is, instead of returning a resulting list, it takes a destination as an input and fills it with the result. At each recursive call, $\ottkw{map'}$ creates a new hollow cons cell to fill the destination. A destination pointing to the tail of the new cons cell is also created, on which $\ottkw{map'}$ is called (tail) recursively. This is really the same algorithm that you could write to implement map on a mutable list in an imperative language. Nevertheless \destcalculus{} is a pure language with only immutable types.

To obtain the regular $\ottkw{map}$ function, all is left to do is to call $[[alloc]]$ to create an initial destination, and $\ottkw{from}_{\ottkw{\ltimes} }'$ when all destinations have been filled to extract the completed list; much like when we make constructors out of filling operators, like $\ottsctor{(::)}$ above.

\begin{codefig}{\caption{Tail-recursive $\ottkw{map}$ function on lists}\label{fig:impl-map-tr}}
\newtype{[[List T]]}{[[① ⨁ (T ⨂ (List T))]]}
\newoperatorb
  {\ottkw{map'}}{[[(T ¹ν → U) ω∞ → List T ¹ν → ⌊ List U ⌋ ¹ν ¹ν → ①]]}
  {[[map' f l dl]]}{\!\!\!\begin{array}[t]{l}[[
l ►case ¹ν {⮒
‥‥ˢ[] ⟼ dl⨞[],⮒
‥‥x ˢ:: xs ⟼ (dl⨞(::)) ►case ¹ν⮒
‥‥‥‥(dx, dxs) ⟼ dx ◀ f x ; map' f xs dxs}]]\end{array}}
\newoperator
  {\ottkw{map}}{[[(T ¹ν → U) ω∞ → List T ¹ν → List U]]}
  {[[map f l]]}{[[from⧔' ((alloc @ (List U) ⧔ ⌊ List U ⌋ ¹ν) ►map dl ⟼ map' f l dl)]]}
\end{codefig}

\subsection{Functional queues, with destinations}\label{ssec:efficient-queue}

Implementations for a tail-recursive map are present in most previous work, from~\cite{minamide_functional_1998}, to recent work~\cite{bour_tmc_2021,leijen_trmc_2023}. Tail-recursive map doesn't need the full power of \destcalculus{}'s first-class destinations: it just needs a notion of structures with a (single) hole. In \cref{sec:bft}, we will build an example which fully uses first-class destinations, but first, we will need some more material.

\paragraph{Difference lists}
\newcommand{\lstcat}{\mathop{+\!+}}
Just like in any language, iterated concatenation of linked lists
$(([[xs1]] \lstcat [[xs2]])\lstcat \ldots)\lstcat [[xs]]_n$
is quadratic in \destcalculus{}. The usual solution to improve that complexity is \emph{difference lists}. The name \emph{difference lists} covers many related implementations for the concept of a ``linked list missing a queue''. The idea is that a difference list carries the same elements as a list would, but can be easily extended by the back in constant time as we retain a way to set a value for its queue later. In pure functional languages, a difference list is usually represented as a function instead~\cite{hughes_dlist_1986}, as we usually don't have write pointers. A singleton difference list is then $\lamnt{[[ys]]}{[[¹ν]]}{[[x ˢ:: ys]]}$, and concatenation of difference lists is function composition. A difference is turned into a list by setting its queue to be the empty list, or in the functional case, by applying it to the empty list. The consequence is that no matter how many compositions we have, each cons cell will be allocated a single time, making the iterated concatenation linear indeed.

The problem is that in the functional implementation, each concatenation still allocates a closure. If we're building a difference list from singletons and composition, there's roughly one composition per cons cell, so iterated composition effectively performs two traversals of the list. In \destcalculus{}, we actually have write pointers, in the form of \emph{destinations}, so we can do better by representing a difference list as a list with a hole, much like in an imperative setting. A singleton difference list becomes $[[x]] \ottsctor{::} \holesq$, and concatenation is filling the hole with another difference list, using composition operator $\mathop{\triangleleft\mycirc}$. The detailed implementation is given on the left of~\cref{fig:impl-dlist-queue}. This encoding for difference lists makes no superfluous traversal: concatenation is just an $O(1)$ in-place update.

\sidebysidecodefig{\caption{Difference list and queue implementation in equirecursive \destcalculus{}}\label{fig:impl-dlist-queue}}{t}{0.49}{
\newtype{[[DList T]]}{[[(List T) ⧔ ⌊ List T ⌋ ¹ν]]}
\newoperatorb
  {\ottkw{append}}{[[DList T ¹ν → T ¹ν → DList T]]}
  {[[ys append y]]}{\!\!\!\begin{array}[t]{l}[[
ys ►map dys ⟼ (dys ⨞ (::)) ►case ¹ν⮒
‥‥(dy, dys') ⟼ dy ◀ y ; dys'
]]\end{array}}
\newoperator
  {\ottkw{concat}}{[[DList T ¹ν → DList T ¹ν → DList T]]}
  {[[ys concat ys']]}{[[ys ►map d ⟼ d ⨞· ys']]}
\newoperator
  {\ottkw{toList}}{[[DList T ¹ν → List T]]}
  {[[toList ys]]}{[[from⧔' (ys ►map d ⟼ d ⨞ [])]]}
}{
\newtype{[[Queue T]]}{[[(List T) ⨂ (DList T)]]}
\newoperator
  {\ottkw{singleton}}{[[T ¹ν → Queue T]]}
  {[[singleton x]]}{[[ˢ(ˢInr (x ˢ:: ˢ[]), (alloc @ DList T))]]}
\newoperatorb
  {\ottkw{enqueue}}{[[Queue T ¹ν → T ¹ν → Queue T]]}
  {[[q enqueue y]]}{[[q ►case ¹ν (xs, ys) ⟼ ˢ(xs, ys append y)]]}
\newoperatorb
  {\ottkw{dequeue}}{[[Queue T ¹ν → ① ⨁ (T ⨂ (Queue T))]]}
  {[[dequeue q]]}{\!\!\!\begin{array}[t]{l}[[
q ►case ¹ν {⮒
‥‥ˢ((x ˢ:: xs), ys) ⟼ ˢInr ˢ(x, ˢ(xs, ys)),⮒
‥‥ˢ(ˢ[], ys) ⟼ (toList ys) ►case ¹ν {⮒
‥‥‥‥ˢ[] ⟼ Inl (),⮒
‥‥‥‥x ˢ:: xs ⟼ ˢInr ˢ(x, ˢ(xs, (alloc @ DList T)))}}]]\end{array}}
}

\paragraph{Efficient queue using difference lists}
In an immutable functional language, a queue can be implemented as a pair of lists $[[ˢ(front, back)]]$~\cite{hood_queue_1981}. $[[back]]$ stores new elements in reverse order ($O(1)$ prepend). We pop elements from $[[front]]$, except when it is empty, in which case we set the queue to $[[ˢ(reverse back, ˢ[])]]$, and pop from the new front.

For their simple implementation, Hood-Melville queues are surprisingly efficient: the cost of the reverse operation is $O(1)$ amortized for a single-threaded use of the queue. Still, it would be better to get rid of this full traversal of the back list. Taking a step back, this $[[back]]$ list that has to be reversed before it is accessed is really merely a representation of a list that can be extended from the back. And we already know an efficient implementation for this: difference lists.

So we can give an improved version of the simple functional queue using destinations. This implementation is presented on the right-hand side of~\cref{fig:impl-dlist-queue}. Note that contrary to an imperative programming language, we can't implement the queue as a single difference list: as mentioned earlier, our type system prevents us from reading the front elements of difference lists. Just like for the simple functional queue, we need a pair of one list that we can read from, and one that we can extend. Nevertheless this implementation of queues is both pure, as guaranteed by the \destcalculus{} type system, and nearly as efficient as what an imperative programming language would afford.

\section{Scope escape of destinations}\label{sec:scope-escape-dests}

In \cref{sec:working-with-dests}, we've been making an implicit assumption: establishing a linear discipline on destinations ensures that all destinations will eventually find their way to the left of a fill operator $\blacktriangleleft$ or $\triangleleft$, so that the associated holes get written to. This turns out to be slightly incomplete.

To see why, let's consider the type $[[⌊ ⌊ T⌋ ¹ν⌋ ¹ν]]$: the type of a destination pointing to a hole where a destination is expected. Think of it as an equivalent of the pointer type $\ottstype{T*\!*}$ in the C language. Destinations are indeed ordinary values, so they can be stored in data structures, and before they get stored, holes stand in their place in the structure. For instance, if we have $[[d]]\pmb{:}[[⌊ T⌋ ¹ν]]$ and $[[dd]]\pmb{:}[[⌊ ⌊ T⌋ ¹ν⌋ ¹ν]]$, we can form $[[dd ◀ d]]$: $[[d]]$ will be stored in the structure pointed to by $[[dd]]$.

Should we count $[[d]]$ as linearly used here? The alternatives don't seem promising:
\vspace{-0.04cm}\begin{itemize}
\item If we count this as a non-linear use of $[[d]]$, then $[[dd ◀ d]]$ is rejected since destinations (represented here by $[[d]]$) can only be used linearly. This choice is fairly limiting, as it would prevent us from storing destinations in structures with holes, as we do, crucially, in \cref{sec:bft}. Nonetheless, that's the option chosen in \cite{bagrel_destination-passing_2024}.
\item If we do not count this use of $[[d]]$ at all, we can write $[[dd ◀ d ; d ◀ v]]$ so that $[[d]]$ is both stored for later use \emph{and} filled immediately (resulting in the corresponding hole being potentially written to twice), which is unsound, as discussed in \cref{ssec:build-up-vocab}.
\end{itemize}\vspace{-0.04cm}
So linear use it is. But it creates a problem: there's no way, within our linear type system, to distinguish between ``a destination has been used on the left of a triangle so its corresponding hole has been filled'' and ``a destination has been stored and its hole still exists at the moment''. This oversight may allow us to read uninitialized memory!

Let's compare two examples. We assume a simple store semantics for now where structures with holes stay in the store until they are completed. We'll need the $\ottkw{alloc} \pmb{:} [[(⌊ T ⌋ ¹ν ¹ν → ①) ¹ν → T]]$ operator. The semantics of $\ottkw{alloc}$ is: allocate a structure with a single root hole in the store, call the supplied function with the destination to the root hole as an argument; when the function has consumed all destinations (so only unit remains), pop the structure from the store to obtain a complete $[[T]]$.

In this snippet, structures with holes are given names $[[v]]$ and $[[vd]]$ in the store; holes are given names too and denoted by $[[+h]]$ and $[[+hd]]$, and concrete destinations are denoted by $[[-h]]$ and $[[-hd]]$.

When the building scope of $[[v]] \pmb{:} [[Bool]]$ is parent to the one of $[[vd]] \pmb{:} [[⌊ Bool ⌋ ¹ν]]$, everything works well because $[[vd]]$, that contains destination pointing to $[[+h]]$, has to be consumed before $[[v]]$ can be read:
\bgroup\setlength{\arraycolsep}{0.5ex}
\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|& [[ alloc' (ˢλ d ¹ν ⟼ (alloc' (ˢλ dd ¹ν ⟼ dd ◀ d) @ ⌊ Bool ⌋ ¹ν) ◀ true)]] \\
[[⟶]] & [[ { v ≔ +h } ]] &|& [[ (alloc' (ˢλ dd ¹ν ⟼ dd ◀ -h) @ ⌊ Bool ⌋ ¹ν) ◀ true ; deref v ]] \\
[[⟶]] & [[ { v ≔ +h, vd ≔ +hd } ]] &|& [[ (-hd ◀ -h ; deref vd) ◀ true ; deref v ]] \\
[[⟶]] & [[ { v ≔ +h, vd ≔ -h } ]] &|& [[ deref vd ◀ true ; deref v ]] \\
[[⟶]] & [[ { v ≔ +h } ]] &|& [[ -h ◀ true ; deref v ]] \\
[[⟶]] & [[ { v ≔ true } ]] &|& [[ deref v ]] \\
[[⟶]] & $\{ \}$ &|& [[ true ]]
\end{array}}

However, when $[[vd]]$'s scope is parent to $[[v]]$'s, we can write a linearly typed yet unsound program:
\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|& [[alloc' (ˢλ dd ¹ν ⟼ (alloc' (ˢλ d ¹ν ⟼ dd ◀ d) @ Bool) ►case ¹ν { true ⟼ (), false ⟼ () })]] \\
[[⟶]] & [[ { vd ≔ +hd } ]] &|& [[(alloc' (ˢλ d ¹ν ⟼ -hd ◀ d)  @ Bool) ►case ¹ν { true ⟼ (), false ⟼ () } ; deref vd]] \\
[[⟶]] & [[ { vd ≔ +hd , v ≔ +h } ]] &|& [[(-hd ◀ -h ; deref v) ►case ¹ν { true ⟼ (), false ⟼ () } ; deref vd]] \\
[[⟶]] & [[ { vd ≔ -h , v ≔ +h } ]] &|& [[ (deref v) ►case ¹ν { true ⟼ (), false ⟼ () } ; deref vd]] \\
[[⟶]] & [[ { vd ≔ -h } ]] &|& [[ +h ►case ¹ν { true ⟼ (), false ⟼ () } ; deref vd]] \qquad\qquad \raisebox{-0.8ex}{\scalebox{0.35}{\bcbombe\bcbombe\bcbombe}}
\end{array}}\egroup
\noindent{}Here the expression $[[dd ◀ d]]$ results in $[[d]]$ escaping its scope for the parent one, so $[[v]]$ is just uninitialized memory (the hole $[[+h]]$) when we dereference it. This example must be rejected by our type system.

Again, the problem is, using purely a linear type system, we can only reject this example if we also reject the first, sound example. In this case, the type $[[⌊ ⌊ T⌋ ¹ν⌋ ¹ν]]$ would become practically useless: such destinations can never be filled. This isn't the direction we want to take: we really want to be able to store destinations in data structures with holes. So we want $[[t]]$ in $[[d ◀ t]]$ to be allowed to be linear. Without further restrictions, it wouldn't be sound, so to address this, we need an extra control system to prevent scope escape; it can't be just linearity. For \destcalculus{}, we decided to use a system of \emph{ages} to represent which scope a resource originates from. Ages are described in detail in \cref{sec:syntax-type-system}; but first, let's see an exemple where storing destinations in data structures really matters.

\section{Breadth-first tree traversal}\label{sec:bft}

As a full-fledged example, which uses the full expressive power of \destcalculus{}, we focus breadth-first tree relabeling:
% \begin{quote}
\emph{``
Given a tree, create a new one of the same shape, but with the values at the nodes replaced by the numbers $1\ldots|T|$ in breadth-first order.
''}
% \end{quote}

This isn't a very natural problem for functional programming, as breadth-first traversal implies that the order in which the structure must be built (left-to-right, top-to-bottom) is not the same as the structural order of a functional tree --- building the leaves first and going up to the root. So it usually requires fancy functional workarounds~\cite{okasaki_bfs_2000,jones_gibbons_linearbfs_93,gibbons_phases_2023}.

It's very tempting to solve this exercise in an efficient imperative-like fashion, where a queue drives the processing order. That's the standard algorithm taught at university, where the next node to process is dequeued from the front of the queue, and its children nodes are enqueued at the back of the queue for later processing, achieving breadth-first traversal. For that, \citet{minamide_functional_1998}'s system where structures with holes are represented as linear functions cannot help. We really need destinations as first-class values to borrow from this imperative power.

\Cref{fig:impl-bfs} presents how we can implement this breadth-first tree traversal in \destcalculus{}, thanks to first-class destinations. We assume the data type $[[Tree T]]$ as been defined, unsurprisingly, as $[[Tree T]]\btriangleq [[① ⨁ (T ⨂ ((Tree T) ⨂ (Tree T)))]]$; and we refer to the constructors of $[[Tree T]]$ as $\ottsctor{Nil}$ and $\ottsctor{Node}$, defined in the obvious way. We also assume some encoding of the type $[[Nat]]$ of natural number. Remember that $[[Queue T]]$ is the efficient queue type from \cref{ssec:efficient-queue}.

The core idea of our algorithm is that we hold a queue of pairs, storing each an input subtree and (a destination to) its corresponding output subtree. When the element $[[ˢ(tree, dtree)]]$ at the front of the queue has been processed, the children nodes of $[[tree]]$ and children destinations of $[[dtree]]$ are enqueued to be processed later, much as the original imperative algorithm.

We implement the actual breadth-first relabeling $\ottkw{relabelDPS}$ as an instance of a more general breadth-first traversal function $\ottkw{mapAccumBFS}$, which applies any state-passing style transformation of labels in breadth-first order. In $\ottkw{mapAccumBFS}$, we create a new destination $[[dtree]]$ into which we will write the result of the traversal, then call $\ottkw{go}$. The $\ottkw{go}$ function is in destination-passing style, but what's remarkable is that $\ottkw{go}$ takes an unbounded number of destinations as arguments, since there are as many destinations as items in the queue. This is where we use the fact that destinations are ordinary values.

% The implementation of \cref{fig:impl-bfs} is very close to the one found in~\cite{bagrel_destination-passing_2024}. The difference is that, because they can't store destinations in structures with holes (see the discussion in \cref{sec:scope-escape-dests}), their implementation can't use the efficient queue implementation from \cref{ssec:efficient-queue}. So they have to revert to using a Hood-Melville queue for breadth-first traversal.

This freshly gained expressivity has a cost though: we need a type system that combines linearity control \emph{and} age control to make the system sound, as exemplified in the previous section. We'll combine both linearity and the aforementioned ages in the same \emph{mode} system\footnote{We said earlier that the modal approach would allow us to combine several control axes efficiently in the same mode system.}. You already know the linearity annotations $[[¹]]$ and $[[ω]]$; here we also introduce the new age annotation $[[∞]]$, that indicates that the associated argument cannot carry destinations.Arguments with no modes displayed on them, or function arrows with no modes, default to the unit of the semiring/ringoid; in particular they are linear, and can capture destinations.

\begin{codefig}{\caption{Breadth-first tree traversal in destination-passing style}\label{fig:impl-bfs}}
\newoperator[~\mathbf{rec}]
{\ottkw{go}}{[[(S ω∞ → T1 ¹ν → (! ω∞ S) ⨂ T2) ω∞→ S ω∞ → Queue (Tree T1 ⨂ ⌊ Tree T2 ⌋ ¹ν) ¹ν → ①]]}
{[[go f st q]]}{\!\!\!\begin{array}[t]{l}[[
(dequeue q) ►case ¹ν {⮒
‥‥Inl () ⟼ (),⮒
‥‥ˢInr ˢ(ˢ(tree, dtree), q') ⟼ tree ►case ¹ν {⮒
‥‥‥‥ˢNil ⟼ dtree ⨞ Nil ; go f st q',⮒
‥‥‥‥ˢNode x tl tr ⟼ ༼(dtree ⨞ Node) ►case ¹ν⮒
‥‥‥‥‥‥ˢ(dy, ˢ(dtl, dtr)) ⟼ ༼(༼f st༽ x) ►case ¹ν⮒
‥‥‥‥‥‥‥‥ˢ(ˢᴇ ω∞ st', y) ⟼⮒
‥‥‥‥‥‥‥‥‥‥dy ◀ y ;⮒
‥‥‥‥‥‥‥‥‥‥go f st' (q' enqueue ˢ(tl, dtl) enqueue ˢ(tr, dtr))༽༽}}
]]\end{array}}
\newoperator
{\ottkw{mapAccumBFS}}{[[(S ω∞ → T1 ¹ν → (! ω∞ S) ⨂ T2) ω∞→ S ω∞ → Tree T1 ¹∞ → Tree T2]]}
{[[mapAccumBFS f st tree]]}{\!\!\!\begin{array}[t]{l}[[
from⧔' ((alloc @ (Tree T2) ⧔ ⌊ Tree T2 ⌋ ¹ν) ►map dtree ⟼⮒
‥‥‥‥‥‥༼go f st (singleton ˢ(tree, dtree))༽)
]]\end{array}}
\newoperator
{\ottkw{relabelDPS}}{[[Tree ① ¹∞ → Tree Nat]]}
{[[relabelDPS tree]]}{\!\!\!\begin{array}[t]{l}[[
mapAccumBFS (ˢλ st ω∞ ⟼ ˢλ un ¹ν ⟼ un ; ˢ(ˢᴇ ω∞ (succ st), st)) ˢ1 tree
]]\end{array}}
\end{codefig}

%------------------------------------------------------------------------------


\section{Type system}\label{sec:syntax-type-system}

\begin{figure}[t]

\begin{minipage}{\linewidth}\small\textit{Core grammar of terms:}\end{minipage}

\smallskip

\hspace*{-0.05\linewidth}\begin{minipage}{\linewidth}\codehere{\setlength{\arraycolsep}{0.6ex}\!\begin{array}{rrl}
[[t]], [[u]] &\grammdef& [[x]] \grammsep [[t' t]] \grammsep [[t ; t']] \\
             &|\,& [[ t ►case m { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case m ( x1 , x2 ) ⟼ u]] \grammsep [[t ►case m ᴇ n x ⟼ u]] \\
             &|\,& [[t ►map x ⟼ t']] \grammsep [[ to⧔ t ]] \grammsep [[ from⧔ t ]] \grammsep [[ alloc ]] \\
             &|\,& [[ t ⨞ () ]] \grammsep [[ t ⨞ Inl ]] \grammsep [[t ⨞ Inr]] \grammsep [[t ⨞ (,)]] \grammsep [[t ⨞ ᴇ m]] \grammsep [[t ⨞ ( λ x m ⟼ u )]] \grammsep [[t ⨞· t']] \grammsep [[t ◀ t']]
\end{array}}\end{minipage}

\bigskip

\begin{minipage}{\linewidth}\small\textit{Syntactic sugar for terms:}\end{minipage}

\smallskip

\hspace*{-0.05\linewidth}\begin{minipage}{\linewidth}\sidebysidecodehere{t}{0.46}{
[[ˢInl t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ Inl ◀ t)
]]\end{array}\\[\interdefskip]
[[ˢInr t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ Inr ◀ t)
]]\end{array}\\[\interdefskip]
[[ˢᴇ m t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ ᴇ m ◀ t)
]]\end{array}\\[\interdefskip]
[[ˢλ x m ⟼ u]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼⮒
‥‥‥‥‥‥d ⨞ ( λ x m ⟼ u ))
]]\end{array}
}{
[[from⧔' t]] \btriangleq \\
\myspace{1}\!\!\!\begin{array}[t]{l}[[
(from⧔ (t ►map un ⟼ un ; ᴇ ¹∞ () )) ►case ¹ν⮒
‥‥( st , ex ) ⟼ ༼ex ►case ¹ν⮒
‥‥‥‥ᴇ ¹∞ un ⟼ un ; st༽
]]\end{array}\\[\interdefskip]
[[ˢ()]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼ d ⨞ () )
]]\end{array}\\[\interdefskip]
[[ˢ( t1 , t2 )]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc ►map d ⟼ (d ⨞ (,)) ►case ¹ν⮒
‥‥‥‥‥‥( d1 , d2 ) ⟼ d1 ◀ t1 ; d2 ◀ t2)
]]\end{array}
}\end{minipage}

\bgroup\renewcommand{\ottdruleTyXXtermXXVal}{}

\renewcommand\ottaltinferrule[4]{
  \inferrule*[narrower=0.3,lab=#1,#2]
    {#3}
    {#4}
}

\bigskip
\hrule
\bigskip

\begin{minipage}{\linewidth}\small\textit{Grammar of types, modes and contexts:}\end{minipage}

\smallskip

\hspace*{-0.05\linewidth}\begin{minipage}{\linewidth}\sidebysidecodehere{t}{0.58}{\setlength{\arraycolsep}{0.6ex}\!\begin{array}{rrl}
[[T]], [[U]], [[S]] &\grammdef& [[⌊ T ⌋ n]] \quad\quad\textit{(destination)} \\
                    &|\,& [[S ⧔ T]] \hspace*{\widthof{$[[⌊ T ⌋ n]]$}-\widthof{$[[U ⧔ T]]$}}\quad\quad\textit{(ampar)} \\
                    &|\,& [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! m T]] \grammsep [[T m → U]] \\
&&\\
[[P]] &\grammdef& [[{ }]] \grammsep [[{ x : m T }]] \grammsep [[P1 , P2]]% \grammsep [[P1 + P2]] \grammsep [[m · P]]
\end{array}}{\setlength{\arraycolsep}{0.6ex}\!\begin{array}[b]{rrl}
[[m]], [[n]] &\grammdef& [[p a]] \hspace*{\widthof{$[[⌊ T ⌋ m]]$}-\widthof{$[[p a]]$}}\quad\textit{(pair of multiplicity and age)} \\
  [[p]] &\grammdef& [[¹]] \grammsep [[ω]] \\
  [[a]] &\grammdef& [[↑^ka]] \grammsep [[∞]]
\end{array}\\[\interdefskip]
[[ν]] \btriangleq [[↑^0]] \quad [[↑]] \btriangleq [[↑^1]]}\end{minipage}

\bigskip

\begin{minipage}{\linewidth}\small\textit{Ordering on modes:}\end{minipage}

{\small
\vspace*{-0.3cm}

\hfill $[[p a ⥶ p' a']] \Longleftrightarrow [[p]] \pleq [[p']] \land~[[a]] \aleq [[a']]$ \hfill \begin{tikzpicture}[baseline=(current bounding box.center), clip]
% Define nodes
\node (omega) at (0,1) {$[[ω]]$};
\node (oneA) at (0,0) {$[[¹]]$};

% Draw edge with label
\draw (omega) -- node[midway, below, rotate=90] {$\pleq$} (oneA);

\node (inf) at (4,1) {$[[∞]]$};
\node (zero) at (2,0) {$[[↑^0]]$};
\node (oneB) at (3,0) {$[[↑^1]]$};
\node (dots1) at (4,0) {\ldots};
\node (k) at (5,0) {$[[↑^ka]]$};
\node (dots2) at (6,0) {\ldots};

% Draw edges with labels
\draw (inf) -- node[midway, above, rotate=30, inner sep=1pt] {$\aleq$} (zero);
\draw (inf) -- node[midway, below, rotate=45, inner sep=1pt] {$\aleq$} (oneB);
\draw (inf) -- node[midway, above, rotate=135, yscale=-1, inner sep=1pt] {$[[⥶]]^{\reflectbox{$\scriptscriptstyle{\pmb{\mathsf{a}}}$}}$} (k);
\end{tikzpicture}\hfill\phantom{.}
}

\vspace*{-0.2cm}

\begin{minipage}{\linewidth}\small\textit{Operations on modes:}\end{minipage}

\smallskip

{\small
\hfill\begin{tabular}{|c||c|c|}\hline
$\ottsmode{+}$ & $[[¹]]$ & $[[ω]]$ \\\hhline{|=#=|=|}
$[[¹]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hfill
\begin{tabular}{|c||c|c|}\hline
$[[·]]$        & $[[¹]]$ & $[[ω]]$ \\\hhline{|=#=|=|}
$[[¹]]$        & $[[¹]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hfill
\vrule width 0.5pt % Vertical rule of 1pt width
\hfill
\begin{tabular}{|c||c|c|}\hline
$\ottsmode{+}$ & $[[↑^ka]]$               & $[[∞]]$ \\\hhline{|=#=|=|}
$[[↑^ja]]$     & $\text{if }\ottsmodee{k} = \ottsmodee{j}\text{ then }[[↑^ka]]\text{ else }[[∞]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\hfill
\begin{tabular}{|c||c|c|}\hline
$[[·]]$        & $[[↑^ka]]$    & $[[∞]]$ \\\hhline{|=#=|=|}
$[[↑^ja]]$     & $[[↑^ka+ja]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$       & $[[∞]]$ \\\hline
\end{tabular}\hfill\phantom{.}
\smallskip

\hfill\hspace*{-1.5cm}
$[[(p a) · (p' a')]] \btriangleq [[(p · p') (a · a')]]$
\hfill\hspace*{0.5cm}
$[[(p a) + (p' a')]] \btriangleq [[(p + p') (a + a')]]$
\hfill\phantom{.}}

\bigskip

\begin{minipage}{\linewidth}\small\textit{Operations on typing contexts:}\end{minipage}

\smallskip

{\small
\bgroup
\renewcommand\tabcolsep{2pt}
\hfill\begin{tabular}[c]{rclcc@{\qquad}l}
  $[[n]]$ &$\cdot$& $[[{}]]$ & $\btriangleq$ & $[[{}]]$\\
  $[[n]]$ &$\cdot$& $[[({ x : m T },P)]]$ & $\btriangleq$ & $[[({ x : n · m T }),n·P]]$\\
\end{tabular}
\hfill\hspace*{0.02cm}
\vrule width 0.5pt % Vertical rule of 1pt width
\hfill
\begin{tabular}[c]{rclcc@{\qquad}l}
  $[[{}]]$ &$+$& $[[P]]$ & $\btriangleq$ & $[[P]]$\\
  $[[({ x : m T }, P1)]]$ &$+$& $[[P2]]$ & $\btriangleq$ & $[[{ x : m T },(P1+P2)]]$ & \textrm{if $[[x]]\notin[[P2]]$}\\
  $[[({ x : m T }, P1)]]$ &$+$& $[[({ x : m' T }, P2)]]$ & $\btriangleq$ & $[[{x : m+m' T}, (P1+P2)]]$
\end{tabular}\hfill\phantom{.}
\egroup
}

\egroup

\caption{Terms, types and modes of \destcalculus{}}\label{fig:grammar}\label{fig:sterm}\label{fig:mul-age-tables}
\end{figure}

\destcalculus{} is a simply typed $\lambda$-calculus with unit ($[[①]]$), product ($\ottstype{\otimes}$) and sum ($\ottstype{\oplus}$) types, inspired from \CLdm{} from \cref{chap:preli}. Its most distinctive features are the destination $[[⌊ T ⌋ m]]$ and ampar $[[S ⧔ T]]$ types which we've introduced in \cref{sec:working-with-dests,sec:scope-escape-dests,sec:bft}.

To ensure that destinations are used soundly, we need both to enforce the linearity of destination but also to prevent destinations from escaping their scope, as discussed in \cref{sec:scope-escape-dests}. To that effect, \destcalculus{} tracks the \emph{age} of destinations, that is how many nested scope have been open between the current expression and the scope from which a destination originates. We'll see in \cref{ssec:ty-term-val} that scopes are introduced by the $[[t ►map x ⟼ t']]$ construct. For instance, we have a term $[[t1 ►map x1 ⟼ t2 ►map x2 ⟼ t3 ►map x3 ⟼ x1]]$, then wil will say that the innermost occurrence of $[[x1]]$ has age $[[↑]]^{\ottsmodee{2}}$ because two nested $\ottkw{upd}_{\ottkw{\ltimes} }$ separate the definition and use site of $[[x1]]$.

For \destcalculus{}, we take the same modal approach as for \CLdm{}, but we enrich our mode ringoid to have an \emph{age} axis. Thanks to the algebraic nature of the modal approach, for most typing rules, we'll be able to reuse those of \CLdm{} without any modification as just the elements of the ringoid change, not the properties nor the structure of the ringoid.

The syntax of \destcalculus{} terms is presented in~\cref{fig:grammar}, including the syntactic sugar that we've been using in \cref{sec:working-with-dests,sec:scope-escape-dests,sec:bft}.

\subsection{Modes and the age semiring}\label{ssec:age-control}

% The precise algebraic structure that we'll be needing on modes is both a commutative additive semigroup $\ottsmode{+}$ and a multiplicative monoid $(\ottsmode{[[·]]}\,, \ottsmode{1})$, with the usual distributivity law $[[n·(m1+m2)]] = [[n·m1 + n·m2]]$. In addition we require a partial order $⥶$, such that $\ottsmode{+}$ and $\ottsmode{[[·]]}$ are order-preserving. In other words, we'll be dealing with ordered semirings\footnote{There terminology dispute where some prefer to use the term ``semiring'' when the additive semigroup has a zero. This terminology is arguably more popular, but leaves no term for the version without a zero. We'll follow the convention, in this article, that semirings with a zero are called ``rigs''.}. In the rest of the article, we'll just say ``semiring''. In practice, all our semirings will be commutative, and we won't be paying attention to the order of factors in mode multiplication.

Our mode ringoid (more precisely, a commutative additive semigroup $\ottsmode{+}$ and a multiplicative monoid $(\ottsmode{[[·]]}\,, \ottsmode{1})$, with the usual distributivity law $[[n·(m1+m2)]] = [[n·m1 + n·m2]]$, and equipped with a partial order $⥶$, such that $\ottsmode{+}$ and $\ottsmode{[[·]]}$ are order-preserving) is, as promised, the product of a multiplicity ringoid, to track linearity, and an age ringoid, to prevent scope escape. The resulting compound structure is also a ringoid.

The multiplicity ringoid has elements $[[¹]]$ (linear) and $[[ω]]$ (unrestricted), it's the same ringoid as in~\cite{qtt_2018} or~\cite{bernardy_linear_2018}, and the same as described in detail in \cref{chap:preli}. The full description of the multiplicity ringoid is given again in~\cref{fig:mul-age-tables}.

Ages are more interesting. We write ages as $[[↑^ka]]$ (with $[[ka]]$ a natural number), for ``defined $[[ka]]$ scopes ago''.
We also have an age $[[∞]]$ for variables that don't originate from a $[[t ►map x ⟼ t']]$ construct i.e. that aren't destinations, and can be freely used in and returned by any scope. The main role of age $[[∞]]$ is thus to act as a guarantee that a value doesn't contain destinations. Finally, we will write $[[ν]] \btriangleq [[↑^0]]$  (``now'') for the age of destinations that originate from the current scope; and $[[↑]] \btriangleq [[↑^1]]$.

The operations or order on ages aren't the usual ones on natural numbers though. It is crucial that \destcalculus{} tracks the precise age of variables. Variables from two scopes ago cannot be used as if they were from one scope ago, or vice-versa. The ordering reflects this with finite ages being arranged in a flat order, with $[[∞]]$ being bigger than all of them. Multiplication of ages reflects nesting of scope, as such, (finite) ages are multiplied by adding their numerical exponents $[[↑^ka · ↑^ja]] = [[↑^ka + ja]]$. In the typing rules, the most common form of scope nesting is opening a scope, which is represented by multiplying the age of existing bindings by $[[↑]]$ (that is, adding $1$ to the ages seen as a natural numbers). When a same variable is shared between two subterms, $\ottsmode{+}$ takes the least upper bound for the age order above, in other terms, the variable must be at the same age in both subterms, or have age $[[∞]]$ otherwise, which let it assume whichever age it needs in each subterm.

The unit of the new mode ringoid is the pair of the units from the multiplicity and age ringoids, ie. $[[¹ν]]$. We will usually omit the mode annotations when the mode is that unit.

Finally, as in \CLm{} and \CLdm{} from \cref{chap:preli}, operations and modes are lifted to typing contexts, still following the insights from \cite{ghica_bounded_2014} (see \cref{fig:mul-age-tables}).

\subsection{Typing rules}\label{ssec:ty-term-val}

Typing rules of \destcalculus{} are given in \cref{fig:ty-term-sterm}. Rules for elimination of $\ottstype{\multimap}$, $[[①]]$, $\ottstype{\oplus}$, $\ottstype{\otimes}$ and $\ottstype{!_{[[m]]}}$ are the same as in \CLdm{} of \cref{chap:preli}, so we won't cover them again here.

\begin{ottfig}{\caption{Typing rules of \destcalculus{}}\label{fig:ty-term-sterm}}\bgroup\renewcommand{\ottdruleTyXXtermXXVal}{}

\bgroup\SetPrefix{\CTyTerm\CSep}
\ottdefnTyXXterm{}
\egroup

\bigskip
\hrule
\bigskip

\renewcommand\ottaltinferrule[4]{
  \inferrule*[fraction={===},narrower=0.3,lab=#1,#2]
    {#3}
    {#4}
}
\bgroup\SetPrefix{\CTySTerm\CSep}
\ottdefnTyXXsterm{}\egroup
\egroup
\end{ottfig}

One notable difference with \CLdm{} though, is, as announced, that introduction rules for data structures, aka. data constructors for types $[[①]]$, $\ottstype{\oplus}$, $\ottstype{\otimes}$ and $\ottstype{!_{[[m]]}}$, are derived from elimination rules of destinations, and thus are not part of the core language. They are presented on the distinct bottom part of \cref{fig:ty-term-sterm}. The introduction form for functions \rref*{\CTySTerm\CSep\CFun} is also derived from elimination form \rref*{\CTyTerm\CSep\CFillF} for the corresponding destinations of function, although functions are not totally ``data''.

Rules \rref*{\CTyTerm\CSep\CVar}, \rref*{\CTyTerm\CSep\CNewA} and \rref*{\CTySTerm\CSep\CUnit}, that are the potential leaves of the typing tree, is where weakening for unrestricted variables is allowed to happen. That's why they allow to discard any typing context of the form $[[ων·P]]$. It's very similar to what happened in \rref*{\CLdm\CTy\CSep\CId} or \rref*{\CLdm\CTy\CSep\CUnit}, except that $[[P]]$ is scaled by $[[ων]]$ instead of $[[ɷ]]$ now.

Rule \rref*{\CTyTerm\CSep\CVar}, in addition to weakening, allows for coercion of the mode $[[m]]$ of the variable used, with ordering constraint $[[¹ν ⥶ m]]$ as defined in \cref{fig:mul-age-tables}. Unlike in \CLdm{}, here not all modes $[[m]]$ are compatible with $[[¹ν]]$. Notably, mode coercion still doesn't allow for a finite age to be changed to another, as $[[↑^ja]]$ and $[[↑^ka]]$ are not comparable w.r.t. $\aleq$ when $[[ja]]\neq[[ka]]$. So for example we cannot use a variable with mode $[[¹↑]]$ in most contexts.

For pattern-matching, with rules \rref*{\CTyTerm\CSep\CPatS}, \rref*{\CTyTerm\CSep\CPatP} and \rref*{\CTyTerm\CSep\CPatE}, we keep the same \emph{deep mode} approach as in \CLdm{}: $\ottkw{case}$ expressions are parametrized by a mode $[[m]]$ by which the typing context $[[P1]]$ of the scrutinee is multiplied. The variables which bind the subcomponents of the scrutinee then inherit this mode.

Let's focus now on the real novelties of the typing rules of \destcalculus{}.

\paragraph{Rules for scoping}

As destinations always exist in the context of a structure with holes, and must stay in that context, we need a formal notion of \emph{scope}. Scopes are created by \rref*{\CTyTerm\CSep\CUpdA}, as destinations are only ever accessed through $\ottkw{upd}_{\ottkw{\ltimes} }$. More precisely, $[[t ►map x ⟼ t']]$ creates a new scope which spans over $[[t']]$. In that scope, $[[x]]$ has age $[[ν]]$ (now), and the ages of the existing bindings in $[[P2]]$ are multiplied by $[[↑]]$ (i.e. we add $1$ to ages seen as a numbers). That is represented by $[[t']]$ typing in $[[¹↑·P2,{ x : ¹ν T }]]$ while the parent term $[[t ►map x ⟼ t']]$ types in unscaled contexts $[[P1+P2]]$. This difference of age between $[[x]]$ --- introduced by $\ottkw{upd}_{\ottkw{\ltimes} }$, containing destinations --- and $[[P2]]$ lets us see what originates from older scopes. Specifically, distinguishing the age of destinations is crucial when typing filling primitives to avoid the pitfalls of \cref{sec:scope-escape-dests}.
%
\Cref{fig:scope-rules} illustrates scopes introduced by $\ottkw{upd}_{\ottkw{\ltimes} }$, and how the typing rules for $\ottkw{upd}_{\ottkw{\ltimes} }$ and $\blacktriangleleft$ interact.

\begin{figure}[t]
  \scalebox{0.85}{\tikzfig{schemas/mapscopes}}
  \vspace*{-0.5cm}
  \caption{Scope rules for $\ottkw{upd}_{\ottkw{\ltimes} }$ in \destcalculus{}}
  \label{fig:scope-rules}
\end{figure}

Anticipating \cref{ssec:runtime-values}, ampar values are pairs with a structure with holes on the left, and destinations on the right. With $\ottkw{upd}_{\ottkw{\ltimes} }$ we enter a new scope where the destinations are accessible, but the structure with holes remains in the outer scope. As a result, when filling a destination with \rref*{\CTyTerm\CSep\CFillLeaf}, for instance $[[d11 ◀ x0]]$ in~\cref{fig:scope-rules}, we type $[[d11]]$ in the new scope, while we type $[[x0]]$ in the outer scope, as it’s being moved to the structure with holes on the left of the ampar, which lives in the outer scope too. This is the opposite of the scaling that $\ottkw{upd}_{\ottkw{\ltimes} }$ does: while $\ottkw{upd}_{\ottkw{\ltimes} }$ creates a new scope for its body, operator $\blacktriangleleft$, and similarly, $\triangleleft\mycirc$ and $[[⨞]](\lamnt{[[x]]}{[[m]]}{[[u]]})$, transfer their right operand to the outer scope. In other words, the right-hand side of $[[◀]]$ or $[[⨞]]$ is an enclave for the parent scope.

When using $\ottkw{from}_{\ottkw{\ltimes} }'$ (rule \rref*{\CTySTerm\CSep\CFromA'}), the left of an ampar is extracted to the current scope (as seen at the bottom of~\cref{fig:scope-rules} with $[[x22]]$): this is the fundamental reason why the left of an ampar has to ``take place'' in the current scope. We know the structure is complete and can be extracted because the right-hand side is of type unit ($[[①]]$), and thus no destination on the right-hand side means no hole can remain on the left. $\ottkw{from}_{\ottkw{\ltimes} }'$ is implemented in terms of $\ottkw{from}_{\ottkw{\ltimes} }$ in~\cref{fig:sterm} to keep the core calculus tidier (and limit the number of typing rules, evaluation contexts, etc), but it can be implemented much more efficiently in a real-world implementation.

When an ampar is eliminated with the more general $\ottkw{from}_{\ottkw{\ltimes} }$ in rule \rref*{\CTyTerm\CSep\CFromA} however, we extract both sides of the ampar to the current scope, even though the right-hand side is normally in a different scope. This is only safe to do because the right-hand side is required to have type $[[! ¹∞ T]]$, which means it is scope-insensitive: it can't contain any scope-controlled resource. This also ensures that the right-hand side cannot contain destinations, so the structure is ready to be read.

In \rref*{\CTyTerm\CSep\CToA}, on the other hand, there is no need to bother with scopes: the operator $\ottkw{to}_{\ottkw{\ltimes} }$ embeds an already completed structure in an ampar whose left side is the structure (that continues to type in the current scope), and right-hand side is unit.

The remaining operators $[[⨞]][[()]], [[⨞]][[Inl]], [[⨞]][[Inr]], [[⨞]]\,\expcons{[[m]]}, [[⨞]][[(,)]]$ from rules \IfFancyRuleNames{of the form \textsc{\CTyTerm\CSep$\ottstype{\lfloor}~\ottstype{\rfloor}$E}}{\textsc{Ty-term-Fill$*$}} are the other destination-filling primitives. They write a hollow constructor to the hole pointed by the destination operand, and return the potential new destinations that are created for new holes in the hollow constructor (or unit if there is none).

\section{Operational semantics}\label{sec:ectxs-sem}

We give the operational semantics of \destcalculus{} in the \emph{reduction semantics} style that we used in \cref{chap:preli}, that's to say with explicit syntactic manipulation of evaluation contexts. However in this section we'll be more thorough. In particular, we'll give grammar of evaluation contexts properly, and we'll also define typing rules for evaluation contexts, so as to prove that typing is preserved throughout the reduction.

For that, we'll need a few ingredients. Commands $[[ C[t] ]]$ are used to represent a running program; they're described in \cref{ssec:ty-ectxs-cmd}. We'll also need a class of runtime values (we'll often just say \emph{values}), as \destcalculus{} currently lacks any way to represent destinations or holes, or really any kind of value (for instance $[[Inl ()]]$ has been, so far, just syntactic sugar for a term $[[from⧔' (alloc ►map d ⟼ …)]]$). It's a peculiarity of \destcalculus{} that values (in particular, data constructors) only exist during the reduction; usually they are part of the term syntax of functional languages as in \CLdm{}. Our runtime values are described in described in \cref{ssec:runtime-values}. As announced, we'll also extend the type system to cover evaluation contexts, values and commands, so as to be able to state and prove type safety theorems.

\subsection{Runtime values and new typing context forms}\label{ssec:runtime-values}

\begin{ottfig}{\caption{Runtime values and new typing context forms}\label{fig:grammar-val}\label{fig:ty-val}}
\hspace*{-0.1\linewidth}\begin{minipage}{\linewidth}\sidebysidecodehere{c}{0.39}{\begin{minipage}{\linewidth}
\begin{minipage}{\linewidth}\small\textit{Grammar extended with values:}\end{minipage}

\bigskip

$\setlength{\arraycolsep}{0.6ex}\newlength\myskip\setlength{\myskip}{2.38ex}\!\begin{array}{rrl}
  [[t]], [[u]] &\grammdef& \ldots \grammsep [[v]] \\
  \\
       [[v]] &\grammdef& [[+ h]] \hspace*{\widthof{$[[H ⟨ v2 ❟ v1 ⟩]]$}-\widthof{$[[+ h]]$}}\quad\quad\textit{(hole)} \\
             &|\,& [[- h]] \hspace*{\widthof{$[[H ⟨ v2 ❟ v1 ⟩]]$}-\widthof{$[[- h]]$}}\quad\quad\textit{(destination)} \\
             &|\,& [[H ⟨ v2 ❟ v1 ⟩]] \quad\quad\textit{(ampar value)} \\
             &|\,& [[()]] \grammsep [[ᵛλ x m ⟼ u]] \grammsep [[Inl v]] \\
             &|\,& [[Inr v]] \grammsep [[ᴇ m v]] \grammsep [[( v1 , v2 )]] \\
\end{array}$

\bigskip\bigskip
\hrule
\bigskip

\begin{minipage}{\linewidth}\small\textit{Typing values as terms:}\end{minipage}

\smallskip

\[\bgroup\SetPrefix{\CTyTerm\CSep}
  \drule{Ty-term-Val}\egroup
\]

\bigskip

\end{minipage}}{\begin{minipage}{\linewidth}

\begin{minipage}{\linewidth}\small\textit{Extended grammar of typing contexts:}\end{minipage}

\smallskip

$\setlength{\myskip}{2.38ex}\!\begin{array}{rrlcccc}
[[D]] &\grammdef& [[{ }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] &|& [[D1 , D2]] \\% &|& [[D1 + D2]] &|& [[m · D]] \\
[[P]] &\grammdef& [[{ }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] \grammsep [[{ x : m T }]] &|& [[P1 , P2]] \\% &|& [[P1 + P2]] &|& [[m · P]] \\
\hskip \myskip [[G]] &\grammdef& [[{ }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] \grammsep [[{ + h : T n }]] &|& [[G1 , G2]] % &|& [[G1 + G2]] &|& [[m · G]]
\end{array}$

\bigskip

\begin{minipage}{\linewidth}\small\textit{Operations extended to new typing context forms:}\end{minipage}

\smallskip

{\small
\bgroup
\renewcommand\tabcolsep{2pt}
%\hfill
% \begin{tabular}[c]{rclcc@{\quad}l}
%   $[[n']]$ &$\cdot$& $[[({ + h : T n },G)]]$ & $\btriangleq$ & $[[({ + h : T n' · n }),n'·G]]$\\
%   $[[n']]$ &$\cdot$& $[[({ - h : m ⌊ T ⌋ n },P)]]$ & $\btriangleq$ & $[[({ - h : n' · m ⌊ T ⌋ n }),n'·P]]$& $\phantom{.}^{\dagger}$\\
%   \\
%   $[[({ + h : T n }, G1)]]$ &$+$& $[[G2]]$ & $\btriangleq$ & $[[{ + h : T n },(G1+G2)]]$ & \textrm{if $[[h]]\notin[[G2]]$}\\
%   $[[({ + h : T n }, G1)]]$ &$+$& $[[({ + h : T n' }, G2)]]$ & $\btriangleq$ & $[[{ + h : T n+n' }, (G1+G2)]]$\\
%   $[[({ - h : m ⌊ T ⌋ n }, P1)]]$ &$+$& $[[P2]]$ & $\btriangleq$ & $[[{ - h : m ⌊ T ⌋ n },(P1+P2)]]$ & \textrm{if $[[h]]\notin[[P2]]$}~~$\phantom{.}^{\dagger}$\\
%   $[[({ - h : m ⌊ T ⌋ n }, P1)]]$ &$+$& $[[({ - h : m' ⌊ T ⌋ n }, P2)]]$ & $\btriangleq$ & $[[{ - h : m+m' ⌊ T ⌋ n }, (P1+P2)]]$ &$\phantom{.}^{\dagger}$\\
% \end{tabular}
\begin{tabular}[c]{rclcc}
  $[[n']]$ &$\cdot$& $[[({ + h : T n },G)]]$ & $\btriangleq$ & $[[({ + h : T n' · n }),n'·G]]$\\
  $[[n']]$ &$\cdot$& $[[({ - h : m ⌊ T ⌋ n },P)]]$ & $\btriangleq$ & $[[({ - h : n' · m ⌊ T ⌋ n }),n'·P]]$ ~~$\phantom{.}^{\dagger}$\\
\end{tabular}

\bigskip

\begin{tabular}[c]{rclcc@{\quad}l}
  $[[({ + h : T n }, G1)]]$ &$+$& $[[G2]]$ & $\btriangleq$ & $[[{ + h : T n },(G1+G2)]]$ & \textrm{if $[[h]]\notin[[G2]]$}\\
  $[[({ + h : T n }, G1)]]$ &$+$& $[[({ + h : T n' }, G2)]]$ & $\btriangleq$ & $[[{ + h : T n+n' }, (G1+G2)]]$\\
\end{tabular}

\smallskip % N.B.: alignment is not pretty for this last tabular, but we cannot afford more space

\begin{tabular}[c]{rcl}
  $[[({ - h : m ⌊ T ⌋ n }, P1)]]$ &$+$& $[[P2]]$  $\btriangleq$  $[[{ - h : m ⌊ T ⌋ n },(P1+P2)]]$  \quad\textrm{if $[[h]]\notin[[P2]]$}~~$\phantom{.}^{\dagger}$\\
  $[[({ - h : m ⌊ T ⌋ n }, P1)]]$ &$+$& $[[({ - h : m' ⌊ T ⌋ n }, P2)]]$  $\btriangleq$  $[[{ - h : m+m' ⌊ T ⌋ n }, (P1+P2)]]$ ~~$\phantom{.}^{\dagger}$\\
\end{tabular}
\egroup

\smallskip

\smallskip

\begin{tabular}[c]{rcl}
  $[[-⁻¹({})]]$ &$\btriangleq$&  $[[{}]]$\\
  $[[-⁻¹({ - h : ¹ν ⌊ T ⌋ n }, D)]]$ &$\btriangleq$& $[[({ + h : T n }), -⁻¹(D)]]$ \\
\end{tabular}

\begin{center}$\phantom{.}^{\dagger}$ : \textit{same rule is also true for $[[G]]$ or $[[D]]$ replacing $[[P]]$}\end{center}

}\end{minipage}}
\end{minipage}

\bigskip
\hrule
\bigskip

  \bgroup\SetPrefix{\CTyVal\CSep}
  \ottdefnTyXXval{}\egroup
\end{ottfig}

The syntax of runtime values is given in \cref{fig:grammar-val}. It features constructors for all of our basic types, as well as functions (note that in $[[ᵛλ x m ⟼ u]]$, $[[u]]$ is a term, not a value). The more interesting values are holes $[[+ h]]$, destinations $[[- h]]$, and ampars $[[H ⟨ v2 ❟ v1 ⟩]]$, which we'll describe in the rest of the section. In order for the operational semantics to use substitution, which requires substituting variables with values, we also extend the syntax of terms to include values through rule \rref*{\CTyTerm\CSep\CVal}.

Destinations and holes are two faces of the same coin, as seen in~\cref{ssec:build-up-vocab}, and we must ensure that throughout the reduction, a destination always points to a hole, and a hole is always the target of exactly one destination. Thus, the new idea of our system is to feature \emph{hole bindings} $[[{ + h : T n }]]$ and \emph{destination bindings} $[[{ - h : m ⌊ T ⌋ n }]]$ in typing contexts in addition to the usual variable bindings $[[{ x : m T}]]$. In both cases, we call $[[h]]$ a \emph{hole name}. By definition, a context $[[G]]$ can contain both destination bindings and hole bindings, but \emph{not a destination binding and a hole binding for the same hole name}.

We extend our previous context operations $+$ and $\cdot$ to act on the new binding forms, as described in \cref{fig:grammar-val}. Context addition is still very partial; for instance, $[[({ + h : T n }) + ({ - h : m ⌊ T ⌋ n' })]]$ is not defined, as $[[h]]$ is present on both sides but with different binding forms.

One of the main goals of \destcalculus{} is to ensure that a hole value is never read. The type system maintains this invariant by simply not allowing any hole bindings in the context when typing terms (see \cref{fig:grammar-val} for the different type of contexts used in the typing judgment). In fact, the only place where holes are introduced, is the left-hand side $[[v2]]$ in an ampar $[[H ⟨ v2 ❟ v1 ⟩]]$, in \rref*{\CTyVal\CSep\CAmpar}.

Specifically, holes come from the operator $\ottshname{\destminus^{\scriptscriptstyle\text{-}1} }$, which represents the matching hole bindings for a set of destination bindings. It's a partial, pointwise operation on typing contexts $[[D]]$, as defined in \cref{fig:grammar-val}.
Note that $[[-⁻¹D]]$ is undefined if any destination binding in $[[D]]$ has a mode other than $[[¹ν]]$.

Furthermore, in \rref*{\CTyVal\CSep\CAmpar}, the holes $[[-⁻¹D3]]$ and the corresponding destinations $[[D3]]$ are bound together and consequently removed from the ampar's typing context: this is how we ensure that, indeed, there's one destination per hole and one hole per destination. That being said, both sides of the ampar may also contain stored destinations from other scopes, represented by $[[¹↑·D1]]$ and $[[D2]]$ in the respective typing contexts of $[[v1]]$ and $[[v2]]$.

Rule \rref*{\CTyVal\CSep\CHole} indicates that a hole must have mode $[[¹ν]]$ in typing context to be well-typed; in particular mode coercion is not allowed here, and neither is weakening. Only when a hole is behind an exponential, that mode can change to some arbitrary mode $[[n]]$. The mode of a hole constrains which values can be written to it, e.g. in $[[{ + h : T n } ⫦ ᴇ n +h : !n T]]$, only a value with mode $[[n]]$ (more precisely, a value typed in a context of the form $[[n · G]]$) can be written to $[[+h]]$.

Surprisingly, in \rref*{\CTyVal\CSep\CDest}, we see that a destination can be typed with any mode $[[m]]$ coercible to $[[¹ν]]$. We did this to mimic the rule \rref*{\CTyTerm\CSep\CVar} and make the general modal substitution lemma expressible for \destcalculus{}\footnote{Generally, in modal systems, if $[[{ x : m T},P ⊢ u : U]]$ and $[[D ⊢ v : T]]$ then $[[m·D,P ⊢ u[x ≔ v] : U]]$~\cite{bernardy_modality_2020}.\\We have $[[{ x : ω∞ ⌊ T ⌋ n} ⊢ () : ①]]$ and $[[{ -h : ¹ν ⌊ T ⌋ n} ⊢ -h : ⌊ T ⌋ n]]$ so $[[ω∞·({ -h : ¹ν ⌊ T ⌋ n}) ⊢ ()[x ≔ -h] : ①]]$ should be valid.}. We formally proved however that throughout a well-typed closed program, $[[m]]$ will never be of multiplicity $[[ω]]$ or age $[[∞]]$ --- a destination is always linear and of finite age --- so mode coercion is never actually used; and we used this result during the formal proof of the substitution lemma to make it quite easier. The other mode $[[n]]$, appearing in \rref*{\CTyVal\CSep\CDest}, is not the mode of the destination binding; instead it is part of the type $[[⌊ T ⌋ n]]$ and corresponds to the mode of values that we can write to the corresponding $[[+h]]$; so for it no coercion can take place.

\paragraph{Other salient points}
We don't distinguish values with holes from fully-defined values at the syntactic level: instead types prevent holes from being read. In particular, while values are typed in contexts $[[G]]$ allowing both destination and hole bindings, when using a value as a term in \rref*{\CTyTerm\CSep\CVal}, it's only allowed to have free destinations, but no free holes.

Notice, also, that values can't have free variables, since contexts $[[G]]$ only contain hole and destination bindings, no variable binding. That values are closed is a standard feature of denotational semantics or abstract machine semantics. This is true even for function values (\rref*{\CTyVal\CSep\CFun}), which, also is prevented from containing free holes. Since a function's body is unevaluated, it's unclear what it'd mean for a function to contain holes; at the very least it'd complicate our system a lot, and we are unaware of any benefit supporting free holes in functions could bring.

One might wonder how we can represent a curried function $[[ˢλ x ¹ν ⟼ ˢλ y ¹ν ⟼ x concat y]]$ at the value level, as the inner abstraction captures the free variable $[[x]]$. The answer is that such a function, at value level, is encoded as $[[ᵛλ x ¹ν ⟼ from⧔' (alloc ►map d ⟼ d ⨞ ( λ y ¹ν ⟼ x concat y))]]$, where the inner closure is not yet in value form. As the form $[[d ⨞ ( λ y ¹ν ⟼ x concat y)]]$ is part of term syntax, it's allowed to have free variable $[[x]]$.

\subsection{Evaluation contexts and commands}\label{ssec:ectxs}\label{ssec:ty-ectxs-cmd}

A running program is represented by a pair $[[ C[t] ]]$ of an evaluation context $[[C]]$, and an (extended) term $[[t]]$ under focus. We call such a pair $[[ C[t] ]]$ a \emph{command}, borrowing the terminology from~\citet{herbelin_curien_2000}.

The grammar of evaluation contexts is given in~\cref{fig:grammar-ty-ectxs}. As in \CLdm{}, an evaluation context $[[C]]$ is the composition of an arbitrary number of focusing components $[[c]]$. It might seem surprising that we don't need a notion of store or state in our semantics to represent the mutation induced by filling destinations. In fact, destination filling only require a very tame notion of state --- so tame that we can simply represent writing to a hole by a substitution \emph{on the evaluation context}, instead of using more heavy store semantics.

It's important to remember that the command $[[ C[t] ]]$ is formally a pair, so it won't always have a corresponding term. The reason is that focusing components are all directly derived from the term syntax, except for the \emph{open ampar} focusing component $[[H ᵒᵖ⟨ v2 ❟ ⬜ ⟩]]$ that doesn't have a corresponding term construct. This focusing component indicates that an ampar is currently being processed by $\ottkw{upd}_{\ottkw{\ltimes} }$, with its left-hand side $[[v2]]$ (the structure being built) being attached to the open ampar focusing component, while its right-hand side (containing destinations) is either in subsequent focusing components, or in the term under focus. Ampars being open during the evaluation of $\ottkw{upd}_{\ottkw{\ltimes} }$'s body and closed back afterwards is counterpart to the notion of scopes in typing rules.

\begin{ottfig}[p]{\caption{Evaluation contexts and their typing rules}\label{fig:grammar-ty-ectxs}}{\setlength{\arraycolsep}{1ex}
\hspace*{-0.05\linewidth}\begin{minipage}{\linewidth}\codehere{\hspace*{-0.3cm}\begin{minipage}{\linewidth}
\begin{minipage}{\linewidth}\small\textit{Grammar of evaluation contexts:}\end{minipage}

\smallskip

$\!\begin{array}{rrl}
[[c]] &\grammdef& [[t' ⬜]] \grammsep [[⬜ v]] \grammsep [[⬜ ; u]] \\
      &|\,& [[ ⬜ ►case m { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[⬜ ►case m ( x1 , x2 ) ⟼ u]] \grammsep [[⬜ ►case m ᴇ n x ⟼ u]] \\
      &|\,& [[⬜ ►map x ⟼ t']] \grammsep [[ to⧔ ⬜ ]] \grammsep [[ from⧔ ⬜ ]] \grammsep [[⬜ ⨞· t']] \grammsep [[v ⨞· ⬜]] \grammsep [[⬜ ◀ t']] \grammsep [[v ◀ ⬜]] \\
      &|\,& [[ ⬜ ⨞ () ]] \grammsep [[ ⬜ ⨞ Inl ]] \grammsep [[⬜ ⨞ Inr]] \grammsep [[⬜ ⨞ (,)]] \grammsep [[⬜ ⨞ ᴇ m]] \grammsep [[⬜ ⨞ ( λ x m ⟼ u )]] \\
      &|\,& [[ H ᵒᵖ⟨ v2 ❟ ⬜ ⟩ ]] \quad\quad\textit{(open ampar focusing component)} \\
[[C]] &\grammdef& [[ ⬜ ]] \grammsep [[C ∘ c]] % \grammsep [[C [ h ≔ H v ] ]]
\end{array}$\end{minipage}}\end{minipage}

\bigskip
\hrule
\bigskip

\begin{augmentwidth}{1.2cm}
\bgroup\SetPrefix{\CTyEctxs\CSep}
\ottdefnTyXXectxs{}\egroup
\end{augmentwidth}

}\end{ottfig}

Evaluation contexts are typed in a context $[[D]]$ that can only contain destination bindings. As we will later see in rule \rref*{\CTyCmd} of \cref{fig:sem-full1}, $[[D]]$ is exactly the typing context that the term $[[t]]$ has to use to form a valid $[[ C[t] ]]$. In other words, while $[[P ⊢ t : T]]$ \emph{requires} the bindings of $[[P]]$, judgment $[[D ⊣ C : T ↣ U0]]$ \emph{provides} the bindings of $[[D]]$. Typing rules for evaluation contexts are given in~\cref{fig:grammar-ty-ectxs}.

An evaluation context has a context type $[[T]]\ottstype{\rightarrowtail}[[U0]]$. The meaning of $[[C]][[:]] [[T]]\ottstype{\rightarrowtail}[[U0]]$ is that given $[[t]][[:]][[T]]$, $[[ C[t] ]]$ returns a value of type $[[U0]]$. Composing an evaluation context $[[C]][[:]][[T]]\ottstype{\rightarrowtail}[[U0]]$ with a new focusing component never affects the type $[[U0]]$ of the future command; only the type $[[T]]$ of the focus is altered.

All typing rules for evaluation contexts can be derived systematically from the ones for the corresponding term (except for the rule \rref*{\CTyEctxs\CSep\COpenAmpar} that is a truly new form). Let's take the rule \rref*{\CTyEctxs\CSep\CPatP} as an example:

\medskip

\sidebysidecodehere{t}{0.55}{
\bgroup\SetPrefix{\CTyEctxs\CSep}
\drule{Ty-ectxs-PatP}\egroup
}{
\bgroup\SetPrefix{\CTyTerm\CSep}
\drule{Ty-term-PatP}\egroup
}

\medskip

\begin{itemize}
  \item the typing context $[[m·D1,D2]]$ in the premise for $[[C]]$ in \rref*{\CTyEctxs\CSep\CPatP} corresponds to $[[m·P1 + P2]]$ in the conclusion of \rref*{\CTyTerm\CSep\CPatP};
  \item the typing context $[[D2,{ x1 : m T1 },{ x2 : m T2 }]]$ in the premise for term $[[u]]$ in \rref*{\CTyEctxs\CSep\CPatP} corresponds to the typing context $[[P2,{ x1 : m T1 },{ x2 : m T2 }]]$ for the same term in \rref*{\CTyTerm\CSep\CPatP};
  \item the typing context $[[D1]]$ in the conclusion for $[[C ∘ (⬜ ►case m (x1 , x2) ⟼ u) ]]$ in \rref*{\CTyEctxs\CSep\CPatP} corresponds to the typing context $[[P1]]$ in the premise for $[[t]]$ in \rref*{\CTyTerm\CSep\CPatP} (the term $[[t]]$ is located where the focus $[[ ⬜]]$ is in \rref*{\CTyEctxs\CSep\CPatP}).
\end{itemize}

We think of the typing rule for an evaluation context as a rotation of the typing rule for the associated term, where the typing contexts of one subterm and the conclusion are swapped, and the typing contexts of the other potential subterms are kept unchanged (with the difference that typing contexts for evaluation contexts are of shape $[[D]]$ instead of $[[P]]$).

\subsection{Reduction semantics}\label{ssec:sem}

% \ExplSyntaxOn
% \NewDocumentCommand{\transformsemname}{m}
% {
%   \tl_set:Nn \l_tmpa_tl { #1 } % Store the input string in a temporary variable
% %  \regex_replace_all:nnN { \\[a-zA-Z]- } { \1 } \l_tmpa_tl
%   \regex_replace_once:nnN { (.+) - Red } { Red - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Focus } { Focus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Unfocus } { Unfocus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Unfocus } { Unfocus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \l_tmpa_tl % Output the transformed string
% }
% \ExplSyntaxOff

\newlength{\tempwidth}
\newcommand{\ifnonempty}[2]{%
  \settowidth{\tempwidth}{#1}%
  \ifthenelse{\lengthtest{\tempwidth < 1ex}}{}{#2}
}

\bgroup
\renewcommand\arraystretch{1.4}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} \ifnonempty{\ensuremath{#3}}{\quad\textit{when}\quad\ensuremath{#3}} \\
}
\bgroup\SetPrefix{\CRed\CSep}


Now that every piece is in place, let's focus on reduction rules of \destcalculus{}. As in \CLdm{}, focus, unfocus and contraction rules of \destcalculus{} (see \cref{fig:sem-full1,fig:sem-full2}) are triggered in a purely deterministic fashion. Once a subterm is a value, it cannot be focused on again.

\begin{ottfig}[h]{\caption{Small-step reduction of commands for \destcalculus{} (part 1)}\label{fig:sem-full1}}\begin{augmentwidth}{2.5cm}

\begin{minipage}{\linewidth}\sidebysidecodehere{t}{0.6}{\begin{minipage}{\linewidth}
\bgroup\SetPrefix{}
\ottdefnTy{}\egroup
\end{minipage}}{\begin{minipage}{\linewidth}
\begin{minipage}{\linewidth}\small\textit{Name set shift and conditional name shift:}\end{minipage}

\bigskip\bigskip

$\!\begin{array}{rcl}
[[H ⩲ h']] &\btriangleq& \{ [[h+h']]~|~[[h]]\in [[H]] \}\\
[[h [H ⩲ h'] ]] &\btriangleq& \left\{\begin{array}{ll}[[h+h']] & \text{if}~[[h]]\in[[H]]\\[[h]] & \text{otherwise}\end{array}\right.\end{array}$
\end{minipage}}\end{minipage}

\bigskip
\hrule
\bigskip

\begin{minipage}{\linewidth}\small\textit{Special substitution for open ampars:}\end{minipage}

\smallskip

\hfill$\!\begin{array}{rcll}
  [[ (C ∘ {h} ⨆ H ᵒᵖ⟨ v2 ❟ ⬜ ⟩) [h ≔ H' v'] ]] &=& [[C ∘ H ⨆ H' ᵒᵖ⟨ v2[ h ≔ H' v' ] ❟ ⬜ ⟩ ]] &\\
  [[ (C ∘ c) [h ≔ H' v'] ]] &=& [[ C[h ≔ H' v'] ∘ c ]]&\text{if $[[h]] \notin [[c]]$}
\end{array}$\hfill\phantom{.}

\bigskip
\hrule
\bigskip

\bgroup
\renewcommand{\ottdrulename}[1]{\CSem\CSep}
\renewcommand\arraystretch{1.5}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \setbox0=\hbox{\ensuremath{#3}\foreverunspace}\ifdim\wd0=0pt ~ \else$\star$\fi & \text{\textsc{#1}} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{lll}}
  {\endarray$}
\makeatother
\drules{$[[C [ t ] ⟶ C' [ t' ] ]]$}{Small-step evaluation of commands}{%
App-FocusOne,
App-UnfocusOne,
App-FocusTwo,
App-UnfocusTwo,
App-Red,
%
PatU-Focus,
PatU-Unfocus,
PatU-Red,
%
PatS-Focus,
PatS-Unfocus,
PatL-Red,
PatR-Red,
%
PatP-Focus,
PatP-Unfocus,
PatP-Red,
%
PatE-Focus,
PatE-Unfocus,
PatE-Red,
%
ToA-Focus,
ToA-Unfocus,
ToA-Red,
%
FromA-Focus,
FromA-Unfocus,
FromA-Red,
NewA-Red
}
\egroup
\begin{center}$\star$~:~only allowed if the term under focus is not already a value\end{center}

\end{augmentwidth}\end{ottfig}

\begin{ottfig}[h]{\caption{Small-step reduction of commands for \destcalculus{} (part 2)}\label{fig:sem-full2}}\begin{augmentwidth}{2.5cm}\bgroup\renewcommand{\drulesectionhead}[2]{}
\renewcommand{\ottdrulename}[1]{\CSem\CSep}
\renewcommand\arraystretch{1.5}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \setbox0=\hbox{\ensuremath{#3}\foreverunspace}\ifdim\wd0=0pt ~ \else$\star$\fi & \text{\textsc{#1}} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{lll}}
  {\endarray$}
\makeatother
\drules{}{}{%
UpdA-Focus,
UpdA-Unfocus,
Ampar-Open,
Ampar-Close,
%
FillU-Focus,
FillU-Unfocus,
FillU-Red,
%
FillL-Focus,
FillL-Unfocus,
FillL-Red,
%
FillR-Focus,
FillR-Unfocus,
FillR-Red,
%
FillE-Focus,
FillE-Unfocus,
FillE-Red,
%
FillP-Focus,
FillP-Unfocus,
FillP-Red,
%
FillF-Focus,
FillF-Unfocus,
FillF-Red,
%
FillComp-FocusOne,
FillComp-UnfocusOne,
FillComp-FocusTwo,
FillComp-UnfocusTwo,
FillComp-Red,
%
FillLeaf-FocusOne,
FillLeaf-UnfocusOne,
FillLeaf-FocusTwo,
FillLeaf-UnfocusTwo,
FillLeaf-Red
}\egroup
\vspace*{-0.5cm}
\[
\text{where}\quad\left\{\begin{array}{rcl}
[[h']] &=& [[max(hnames(C) ∪ {h}) + 1]]\\
[[h'']] &=& [[max(H ∪ (hnames(C) ∪ {h})) + 1]] \\
[[h''']] &=& [[max(H ∪ hnames(C)) + 1]]
\end{array}\right.\]

\begin{center}$\star$~:~only allowed if the term under focus is not already a value\end{center}
\end{augmentwidth}\end{ottfig}

\cref{fig:sem-full1} presents all the rules that don't incur any substitution on the evaluation context.

Reduction rules for function application and pattern-matching are the same as in \CLdm{}. Reduction rules for $\ottkw{to}_{\ottkw{\ltimes} }$ are pretty straightforward: once we have a value at hand, we embed it in a trivial ampar with just unit on the right. Rules for $\ottkw{from}_{\ottkw{\ltimes} }$ are fairly symmetric: once we have an ampar with a value of the shape $[[ᴇ ¹∞ v1]]$ on the right, we can extract both the left and right side out of the ampar shell, into a normal pair.

Finally, $\ottkw{new}_{\ottkw{\ltimes} }$ has only a single rule that transforms it into the ``identity'' ampar object with just one hole on the left, and the corresponding destination on the right.

Rules of \cref{fig:sem-full2} are all related to $\ottkw{upd}_{\ottkw{\ltimes} }$ and destination-filling operators, whose contraction rules modify the evaluation context deeply, instead of just pushing or popping a focusing composant. For that, we need to introduce the special substitution $[[ C[h ≔ H v] ]]$ that is used to update structures under construction, that are attached to open ampar focusing components in the stack. Such a substitution is triggered when a destination $[[-h]]$ is filled in the term under focus, typically in destination-filling primitives reductions, and results in the value $[[v]]$ being written to hole $[[+h]]$. The value $[[v]]$ may contain holes itself (e.g. when the hollow constructor $[[Inl +h'+1]]$ is being written to the hole $[[+h]]$ in \rref*{\CFillL\CSep\CRed}), hence the set $[[H]]$ tracks the potential hole names introduced by value $[[v]]$, and is used to update the hole name set of the corresponding (open) ampar. Proper definition of $[[ C[h ≔ H v] ]]$ is given at the top of \cref{fig:sem-full1}.

\rref*{\CFillU\CSep\CRed} and \rref*{\CFillF\CSep\CRed} of \cref{fig:sem-full2} do not create any new hole; they only write a value to an existing one. On the other hand, rules \rref*{\CFillL\CSep\CRed}, \rref*{\CFillR\CSep\CRed}, \rref*{\CFillE\CSep\CRed} and \rref*{\CFillP\CSep\CRed} all write a hollow constructor to the hole $[[+h]]$ that contains new holes. Thus, we need to generate fresh names for these new holes, and also return a destination for each new hole with a matching name.

The substitution $[[ C[h ≔ H v] ]]$ should only be performed if $[[h]]$ is a globally unique name; otherwise we break the promise of a write-once memory model. To this effect, we allow name shadowing while an ampar is closed (which is why $\ottkw{new}_{\ottkw{\ltimes} }$ is allowed to reduce to the same $[[{1}⟨+1 ❟ -1⟩]]$ every time), but as soon as an ampar is open, it should have globally unique hole names. This restriction is enforced in rule \rref*{\CTyEctxs\CSep\COpenAmpar} by premise $[[hnames(C)]] ~\mathtt{\#\#}~ [[hnames(D3)]]$, requiring hole name sets from $[[C]]$ and $[[D3]]$ to be disjoint when an open ampar focusing component is created during reduction of $\ottkw{upd}_{\ottkw{\ltimes} }$. Likewise, any hollow constructor written to a hole should have globally unique hole names. We assume that hole names are natural numbers for simplicity's sake.

To obtain globally fresh names, in the premises of the corresponding rules, we first set\\ $[[h']] = [[max(hnames(C) ∪ {h}) + 1]]$ or similar definitions for $[[h'']]$ and $[[h''']]$ (see at the bottom of \cref{fig:sem-full2}) to find the next unused name. Then we use either the \emph{shifted set} $[[H ⩲ h']]$ or the \emph{conditional shift operator} $[[h [H ⩲ h'] ]]$ as defined at top of \cref{fig:sem-full1} to replace all names or just specific one with fresh unused names.
We extend \emph{conditional shift} $\smallbullet\ottshname{[}[[H ⩲ h']]\ottshname{]}$ to arbitrary values, terms, and typing contexts in the obvious way (keeping in mind that $[[H' ⟨ v2 ❟ v1 ⟩]]$ binds the names in $[[H']]$).

Rules \rref*{\CAmpar\COpen} and \rref*{\CAmpar\CClose} dictate how and when a closed ampar (a value) is converted to an open ampar (a focusing component) and vice-versa, and they make use of the shifting strategy we've just introduced. With \rref*{\CAmpar\COpen}, the hole names bound by the ampar gets renamed to fresh ones, and the left-hand side gets attached to the focusing component $[[H⩲h' ᵒᵖ⟨ v2[H⩲h'''] ❟ ⬜⟩]]$ while the right-hand side (containing destinations) is substituted in the body of the $\ottkw{upd}_{\ottkw{\ltimes} }$ statement (which becomes the new term under focus). The rule \rref*{\CAmpar\CClose} triggers when the body of a $\ottkw{upd}_{\ottkw{\ltimes} }$ statement has reduced to a value. In that case, we can close the ampar, by popping the focusing component from the stack $[[C]]$ and merging back with $[[v2]]$ to form a closed ampar again.

In rule \rref*{\CFillComp\CSep\CRed}, we write the left-hand side $[[v2]]$ of a closed ampar $[[H ⟨ v2 ❟ v1 ⟩]]$ to a hole $[[+h]]$ that is part of a structure with holes somewhere inside $[[C]]$. This results in the composition of two structures with holes. Because we dissociate $[[v2]]$ and $[[v1]]$ that were previously bound together by the ampar connective ($[[v2]]$ is merged with another structure, while $[[v1]]$ becomes the new focus), their hole names are no longer bound, so we need to make them globally unique, as we do when an ampar is opened with $\ottkw{upd}_{\ottkw{\ltimes} }$. This renaming is carried out by the conditional shift $[[v2[H ⩲ h''] ]]$ and $[[v1[H ⩲ h''] ]]$.

\paragraph{Type safety} With the semantics now defined, we can state the usual type safety theorems:

\begin{theorem}[Type preservation]\label{thm:preservation}
  If $[[⊢ C [t] : T]]$ and $[[C[t] ⟶ C'[t'] ]]$ then $[[⊢ C' [t'] : T]]$.
\end{theorem}

\begin{theorem}[Progress]\label{thm:progress}
  If $[[⊢ C [t] : T]]$ and $\forall [[v]], [[C [t] ]] \neq [[ ⬜[v] ]]$ then $\exists [[C']], [[t']].~[[ C [t] ⟶ C' [t'] ]]$.
\end{theorem}

A command of the form $[[ ⬜[v] ]]$ cannot be reduced further, as it only contains a fully determined value, and no pending computation. This it is the stopping point of the reduction, and any well-typed command eventually reaches this form.

\section{Formal proof of type safety}\label{sec:formal-proof}

We've proved type preservation and progress theorems with the Coq proof assistant. The artifact containing the formalization of \destcalculus{} and the machine-verified proofs of type-safety (\cref{thm:preservation,thm:progress}) is available at \url{https://doi.org/10.5281/zenodo.14534423}.

Turning to a proof assistant was a pragmatic choice: typing context handling in \destcalculus{} can be quite finicky, and it was hard, without computer assistance, to make sure that we hadn't made mistakes in our proofs. The version of \destcalculus{} that we've proved is written in Ott~\cite{sewell_ott_2007}, the same Ott file is used as a source for this article, making sure that we've proved the same system as we're presenting; though some visual simplification is applied by a script to produce the version in the article.

Most of the proof was done by myself with little prior experience with Coq. This goes to show that Coq is reasonably approachable even for non-trivial development. However the development sped up dramatically thanks to the help of my industrial advisor, who was able to use his long prior experience with Coq to introduce the good core lemmas upon which we could easily base many others.

In total the proof is about 7000 lines long, and contains nearly 500 lemmas. Many of the cases of the type preservation and progress lemmas are similar. To handle such repetitive cases, the use of a large-language-model based autocompletion system has proven quite effective. Also the proofs aren't particularly elegant. For instance, we don't have any abstract formalization of semirings: it was more expedient to brute-force the properties we needed by hand.

There are nonetheless a few points of interest in our Coq development. First, we represent contexts as finite-domain functions, rather than as syntactic lists. This works much better when defining sums of context. There are a handful of finite-function libraries in the ecosystem, but we needed finite dependent functions (because the type of binders depend on whether we're binding a variable name or a hole name). This didn't exist, but for our limited purpose, it ended up not being too costly rolling our own (about 1000 lines of proofs). The underlying data type is actual functions: this was simpler to develop, but in exchange equality gets more complex than with a bespoke data type.

Secondly, Addition of context is partial since we can only add two binding of the same name if they also have the same type. Instead of representing addition as a binary function to an optional context, we represent addition as a total function to contexts, but we change contexts to allow faulty bindings on some names. This works well better for our Ott-written rules, at the cost of needing well-formedness preconditions in the premises of typing rules as well as some lemmas.

Finally, we decided to assume a few axioms. The issue we encountered was that the inference rules produced by Ott weren't conducive to using setoid equality, which turned out to be problematic with our type for finite function:

\begin{verbatim}
Record T A B := {
    underlying :> forall x:A, option (B x);
    supported : exists l : list A, Support l underlying;
  }.
\end{verbatim}
where \verb+Support l f+ means that \verb+l+ contains the domain of \verb+f+. To make the equality of finite function be strict equality \verb+eq+, we assumed functional extensionality and proof irrelevance. In some circumstances, we've also needed to list the finite functions' domains. But in the definition, the domain is sealed behind a proposition, so we also assumed classical logic as well as indefinite description:

\begin{verbatim}
Axiom constructive_indefinite_description :
    forall (A : Type) (P : A->Prop), (exists x, P x) -> { x : A | P x }.
\end{verbatim}
Together, they let us extract the domain from the proposition. Again this isn't particularly elegant, we could have avoided some of these axioms at the price of more complex development. But we decided to favor expediency over elegance, given how much time the formal development had already consumed from the three years allowed to complete the PhD work.


\section{Implementation of \destcalculus{} using in-place memory mutations}\label{sec:implementation}

The formal language presented in~\cref{sec:syntax-type-system,sec:ectxs-sem} is not meant to be implemented as-is. Practical implementation of most \destcalculus{}'s ideas will be the focus of \cref{chap:dps-haskell,chap:ext-linear-nonlinear}.

First, \destcalculus{} doesn't have recursion, this would have obscured the formal presentation of the system. However, adding a standard form of recursion doesn't create any complication.

Secondly, ampars are not managed linearly in \destcalculus{}; only destinations are. That is to say that an ampar can be wrapped in an exponential, e.g. $[[ˢᴇ ων {h} ˢ⟨ ༼ˢ0 ˢ:: +h༽ ❟ -h ⟩]]$ (representing a difference list $0 \ottsctor{::} \holesq$ that can be used non-linearly), and then used twice, each time in a different way:

\begin{minipage}{0.50\linewidth}\codehere{[[
༼ˢᴇ ων {h} ˢ⟨ ˢ0 ˢ:: +h ❟ -h ⟩༽ ►case ¹ν ᴇ ων x ⟼⮒
‥‥let x1 ≔ x append ˢ1 in⮒
‥‥let x2 ≔ x append ˢ2 in⮒
‥‥‥‥toList (x1 concat x2)
]]}\end{minipage}$[[⟶*]]\quad[[༼ˢ0 ˢ:: ༼ˢ1 ˢ:: ༼ˢ0 ˢ:: ༼ˢ2 ˢ:: ˢ[]༽༽༽༽ ]]$\\[\interdefskip]

It may seem counter-intuitive at first, but this program is valid and safe in \destcalculus{}. Thanks to the renaming discipline we detailed in~\cref{ssec:sem}, every time an ampar is operated over with $\ottkw{upd}_{\ottkw{\ltimes} }$, its hole names are renamed to fresh ones. One way we can support this is to allocate a fresh copy of $[[x]]$ every time we call $\ottkw{append}$ (which is implemented in terms of $\ottkw{upd}_{\ottkw{\ltimes} }$), in a copy-on-write fashion. This way filling destinations is still implemented as mutation.

However, this is a long way from the efficient implementation promised in \cref{sec:working-with-dests}. Copy-on-write can be optimized using fully-in-place functional programming ~\cite{lorenzen_fp_2023}, where, thanks to reference counting, we don't need to perform a copy when the difference list isn't aliased; but that won't be the direction we will follow in the following development, as we don't want to deal explicitly with reference counting.

An alternative is to refine the linear type system further in order to guarantee that ampars are unique and avoid copy-on-write altogether. We held back from doing that in the formalization of \destcalculus{} as, again, it obfuscates the presentation of the system without adding much in return.

To make ampars linear, we follow a recipe proposed by~\citet{spiwack_linearly_2022} and introduce a new type $[[Token]]$, together with primitives $\ottkw{dup}$ and $\ottkw{drop}$. We also switch $[[alloc]]$ for $[[allocIP]]$:% :

\codehere{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
\ottkw{dup} ~\pmb{:}~ [[Token ¹ν → Token ⨂ Token]]\\[\interdefskip]
\ottkw{drop} ~\pmb{:}~ [[Token ¹ν → ①]]\\[\interdefskip]
[[allocIP]] ~\pmb{:}~ [[Token ¹ν → T ⧔ ⌊ T ⌋ ¹ν]]
\end{array}}

For the in-place system to work, we consider that a linear root token variable, $[[tok0]]$, is available to a program. ``Closed'' programs can now typecheck in the non-empty context $\{[[{tok0 : ¹∞ Token}]]\}$. $[[tok0]]$ can be used to create new tokens $[[tokk]]$ via $\ottkw{dup}$, but each of these tokens still has to be used linearly.

Ampar produced by $[[allocIP]]$ have a linear dependency on a variable $[[tokk]]$. If an ampar produced by $[[allocIP tokk]]$ were to be used twice in a block $[[t]]$, then $[[t]]$ would require a typing context $\{[[{tokk : ων Token}]]\}$, that itself would require $[[tok0]]$ to have multiplicity $[[ω]]$ too. Thus the program would be rejected.

Instead of providing a root token to any closed program, which is a rather exotic modification of the type system, we can alternatively add a primitive function\\$\ottkw{withToken} ~\pmb{:}~ [[(Token ¹∞ → !ω∞ T) ¹ν → !ω∞ T]]$ that let the user use a token in a delimited scope, and the token cannot leak outside of it. This approach is similarly powerful and safe, while also being easier to bake in an existing functional language without modyfing the type system too much.

Now that ampars are managed linearly, we can change the allocation and renaming mechanisms:
\begin{itemize}
  \item the hole name for a new ampar is chosen fresh right from the start (this corresponds to a new heap allocation);
  \item adding a new hollow constructor still require freshness for its hole names (this corresponds to a new heap allocation too);
  \item Using $\ottkw{upd}_{\ottkw{\ltimes} }$ over an ampar and filling destinations or composing two ampars using $\triangleleft\mycirc$ no longer require any renaming: we have the guarantee that the all the names involved are globally fresh, and can only be used once, so it actually corresponds to an in-place memory update.
\end{itemize}

\TODO{Cite new lorenzen paper here about non-linear first-class contexts}

In implementation concerns of \cref{chap:dps-haskell,chap:ext-linear-nonlinear}, we will focus only on \destcalculus{} extended with $[[Token]]$s and $[[allocIP]]$, in which ampars are linear resources.

\paragraph{From purely linked structures to more efficient memory forms}

In \destcalculus{} we only have binary product in sum types. However, it's very straightforward to extend the language and implement destination-based building for n-ary sums of n-ary products, with constructors for each variant having multiple fields directly, instead of each field needing an extra indirection as in the binary sum of products $[[① ⨁ (S ⨂ (T ⨂ U))]]$. We will do that as soon as next chapter.

However, it's better for field's values to still be represented by pointers. Indeed, composition of incomplete structures relies on the idea that destinations pointing to holes of a structure $[[v]]$ will still be valid if $[[v]]$ get assigned to a field $[[f]]$ of a bigger structure $[[v']]$. That's true indeed if just the address of $[[v]]$ is written to $[[v']].[[f]]$. However, if $[[v]]$ is moved into $[[v']]$ completly (i.e. if $[[f]]$ is an in-place/unpacked field), then the pointers representing destinations of $[[v]]$ are now invalid. Our early experiments around DPS support for unpacked fields seem to indicate that we would need two classes of destinations, one supporting composition (for indirected fields) and one disallowing it (for unpacked fields). That won't be covered here.

\section{\destcalculus{}: from theory to practice}\label{sec:conclusion}

Using a system of ages in addition to linearity, \destcalculus{} is a purely functional calculus which supports destinations in an extremely flexible way. It subsumes existing calculi from the literature for destination passing, allowing both composition of data structures with holes and storing destinations in data structures. Data structures are allowed to have multiple holes, and destinations can be stored in data structures that, themselves, have holes. The latter is the main reason to introduce ages and is key to \destcalculus{}'s flexibility.

We don't anticipate that a system of ages like the one of \destcalculus{} will actually be used in a programming language: it's unlikely that destinations are so central to the design of a programming language that it's worth baking them so deeply in the type system. Perhaps a compiler that makes heavy use of destinations in its optimizer could use \destcalculus{} as a typed intermediate representation. But, more realistically, our expectation is that \destcalculus{} can be used as a theoretical framework to analyze destination-passing systems: if an API can be defined in \destcalculus{} then it's sound.

Our path forward, in next chapters, is to see which small restrictions we can impose on \destcalculus{}'s flexiblity to make it possible to port most of its core ideas in a real-world functional programming language, namely, Haskell.
