 
\chapter{Linear $\lambda$-calculus}\label{chap:preli}

\section{From $\lambda$-calculus to linear $\lambda$-calculus}

At the end of the 30s, Church introduced the untyped $\lambda$-calculus as a formal mathematical model of computation. Untyped $\lambda$-calculus is based on the concept of function abstraction and application, and is Turing-complete: in other terms, it has the same expressive power as the empirical model of computation that Turing machines represent.

In 1940, Church defined a typed variant of its original calculus, the simply typed $\lambda$-calculus, or STLC, that give up Turing-completeness but become strongly-normalizing: every well-typed term eventually reduces to a normal form. STLC assign types to terms, and restricts the application of functions to terms of the right type. This restriction is enforced by the typing rules of the calculus. In the following we assume that the reader is familiar with the simply typed lambda calculus, its typing rules, and usual semantics. We also assume some degree of familiarity with natural deduction.

It has been observed by Howard in 1969 that the intuitionistic variant of natural deduction proof system is isomorphic to the simply typed $\lambda$-calculus. This observation relates to prior work by Curry where the former observed that the typed fragment of combinatory logic, another model of computation with similar power, is isomorphic to proof systems like implicational logic and Hilbert deduction systems. These observations led to the Curry-Howard isomorphism, which states that types in a typed $\lambda$-calculus correspond to formulas in a proof system, and terms correspond to proofs of these formulas. The Curry-Howard isomorphism has been later extended to other logics and calculi, and has been a fruitful source of inspiration for research on both the logical and computational side.

In that sense, Girard first introduced Linear Logic, in 1987, and only later studied the corresponding calculus, aka. linear $\lambda$-calculus. Linear logic follows from the observation that in sequent calculus\footnote{a very popular deduction system that is an alternative to natural deduction and that has also been introduced by Gentzen in the 30s}, hypotheses are duplicated or discarded using explicit rules of the system, named \emph{contraction} and \emph{weakening}, in contrast to natural deduction where all that happens implicitly, as it is part of the meta-theory. As a result, it is possible to track the number of times a formula is used by counting the use of these structural rules. Linear logic take this idea further, and deliberately restrict contraction and weakening, so by default, every hypothesis must be used exactly once. Consequently, logical implication $[[T]]\,\ottstype{\to}\,[[U]]$ is not part of linear logic, but is replaced by linear implication $[[T ⊸ U]]$, where $[[T]]$ must be used exactly once to prove $[[U]]$. Linear logic also introduces a modality $\ottstype{!}$, pronounced \emph{of course} or \emph{bang}, to allow weakening and contraction on specific formulas: $[[!T]]$ denotes that $[[T]]$ can be used an arbitrary number of times (we say it is \emph{unrestricted}). We say that linear logic is a \emph{substructural} logic because it restricts the use of structural rules of usual logic.

We present in \cref{fig:linlog} the natural deduction formulation of intuitionistic linear logic (ILL), as it lends itself well to a computational interpretation as a linear $\lambda$-calculus with usual syntax. We borrow the \emph{sequent style} notation of sequent calculus for easier transition into typing rules of terms later. However, as we are in an intuitionistic setting, rules only derive a single conclusion from a multiset of formulas.

\bgroup
\renewcommand\ottaltinferrule[4]{
  \inferrule*[narrower=0.3,right=#1,#2]
    {#3}
    {#4}
}

All the rules of ILL, except the ones related to the $\ottstype{!}$ modality, are directly taken (and slightly adapted) from natural deduction. $\ottstype{\oplus}$ denotes (additive) disjunction, and $\ottstype{\otimes}$ denotes (multiplicative) conjunction. Hypotheses are represented by multisets $[[P]]$. These multisets keep track of how many times each formula appear in them. The comma operator in $[[P1,P2]]$ is multiset union, so it sums the number of occurrences of each formula in $[[P1]]$ and $[[P2]]$.

Let's focus on the four rules for the $\ottstype{!}$ modality now. The promotion rule ILL\,/\,$\ottstype{!}$P states that a formula $[[T]]$ can become an unrestricted formula $[[!T]]$ if it only depends on formulas that are themselves unrestricted. This is denoted by the (potentially empty) multiset $[[!P]]$. The dereliction rule ILL\,/\,$\ottstype{!}$D states that an unrestricted formula $[[!T]]$ can be used in a place expecting a normal i.e. linear formula $[[T]]$. The contraction rule ILL\,/\,$\ottstype{!}$C and  weakening rule ILL\,/\,$\ottstype{!}$W states respectively that an unrestricted formula $[[!T]]$ can be cloned or discarded at any time.

The linear logic system might appear very restrictive, but we can always\unsure{We might need other connectives for that, such as \emph{with} $\ottstype{\&}$} simulate the usual non-linear natural deduction in ILL. Girard gives precise rules for such a translation in Section 2.2.6 of~\cite{girard_linear_1995}, whose main idea is to prefix most formulas with $\ottstype{!}$ and encode the non-linear implication $[[T]]\,\ottstype{\to}\,[[U]]$ as $[[!T ⊸ U]]$.

\begin{ottfig}{\caption{Natural deduction formulation of intuitionistic linear logic (sequent-style)}\label{fig:linlog}}
\bgroup\renewcommand{\ottdrulename}[1]{ILL\,/\,}
\ottdefnLinMonXXLog{}\egroup
\end{ottfig}

\section{Linear $\lambda$-calculus: a computational interpretation of linear logic}

There exists several possible interpretations of linear logic as a linear $\lambda$-calculus. The first one, named \emph{monadic} presentation of linear $\lambda$-calculus in~\cite{andreoli_linlog_1992}, and denoted $\lambda_{L_1}$ in this document, is a direct term assignment of the natural deduction rules of ILL given in \cref{fig:linlog}. The syntax and typing rules of this presentation, inspired greatly from the work of~\cite{bierman_linlog_phd_1994}, are given in \cref{fig:linmon-grammar,fig:linmon-ty-term}.

\begin{codefig}{\caption{Grammar of linear $\lambda$-calculus in monadic presentation ($\lambda_{L_1}$)}\label{fig:linmon-grammar}}{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
       [[v]] &::=& [[λ x ⟼ u]] \grammsep [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᴇ v]] \\
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[ˢInl t]] \grammsep [[ˢInr t]] \grammsep [[ˢ( t1 , t2 )]] \grammsep [[ᴇ t]] \grammsep [[t t']] \grammsep [[t ; t']] \\
&|\,& [[ t ►case ¹ν { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case ¹ν ( x1 , x2 ) ⟼ u]] \\ &|\,& [[dup t as x1, x2 in u]] \grammsep [[drop t in u]] \grammsep [[derelict t]] \\
&&\\
[[T]], [[U]] &::=& [[T ⊸ U]] \grammsep [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! T]] \\
&&\\
[[P]] &::=& [[{ }]] \grammsep [[{ x ⫶ T }]] \grammsep [[P1 , P2]] \\
\end{array}
}\end{codefig}

\begin{ottfig}{\caption{Typing rules for linear $\lambda$-calculus in monadic presentation ($\lambda_{L_1}$)}\label{fig:linmon-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{$\lambda_{L_1}$\,/\,}
\ottdefnLinMonXXTyXXterm{}\egroup
\end{ottfig}

In $\lambda_{L_1}$, $[[P]]$ is now a finite map of variables to types, that can be represented as a set of variable bindings $[[{x ⫶ T}]]$. As usual, duplicated variable names are not allowed in a context $[[P]]$. The comma operator denotes disjoint union for finite maps.

Term grammar for $\lambda_{L_1}$ borrows most of simply typed lambda calculus grammar. In addition, the language has a data constructor/wrapper for unrestricted terms and values, denoted by $[[ᴇ t]]$ and $[[ᴇ v]]$. Elimination of unit type $[[①]]$ is made with $\patu$ operator. Pattern-matching on sum and product types is made with the $\ottkw{case}$ keyword. Finally, we have new operators $\ottkw{dup}$, $\ottkw{drop}$ and $\ottkw{derelict}$ for respective contraction, weakening and dereliction of unrestricted terms of type $[[!T]]$. Promotion of a term to an unrestricted form is made by direct application of constructor $\expcons{}$. For easier presentation, we also introduce syntactic sugar $[[let x ≔ t in u]]$ and encode it as $[[(λ x ⟼ u)(t)]]$.

\section{Implicit structural rules for unrestricted resources}

In the monadic presentation $\lambda_{L_1}$, the use of unrestricted terms can become very verbose and unhandy because of the need for explicit contraction, weakening and dereliction. A second and equivalent presentation of our linear $\lambda$-calculus, named \emph{dyadic} presentation or $\lambda_{L_2}$, tend to alleviate this issue by using two typing contexts on each judgment, one for linear variables and one for unrestricted variables. The syntax and typing rules of $\lambda_{L_2}$ are given in \cref{fig:lindya-grammar,fig:lindya-ty-term}.

\begin{codefig}{\caption{Grammar of linear $\lambda$-calculus in dyadic presentation ($\lambda_{L_2}$)}\label{fig:lindya-grammar}}{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
       [[v]] &::=& [[λ x ⟼ u]] \grammsep [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᴇ v]] \\
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[ˢInl t]] \grammsep [[ˢInr t]] \grammsep [[ˢ( t1 , t2 )]] \grammsep [[ᴇ t]] \grammsep [[t t']] \grammsep [[t ; t']] \\
&|\,& [[ t ►case ¹ν { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case ¹ν ( x1 , x2 ) ⟼ u]] \grammsep [[t ►case ¹ν ᴇ x ⟼ u]] \\
&&\\
[[T]], [[U]] &::=& [[T ⊸ U]] \grammsep [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! T]] \\
&&\\
[[P]] &::=& [[{ }]] \grammsep [[{ x ⫶ T }]] \grammsep [[P1 , P2]] \\
[[Ur]] &::=& [[{ }]] \grammsep [[{ x ⫶ T }]] \grammsep [[Ur1 , Ur2]]
\end{array}
}\end{codefig}

\begin{ottfig}{\caption{Typing rules for linear $\lambda$-calculus in dyadic presentation ($\lambda_{L_2}$)}\label{fig:lindya-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{$\lambda_{L_2}$\,/\,}
\ottdefnLinDyaXXTyXXterm{}\egroup
\end{ottfig}

In $\lambda_{L_2}$ there is no longer rules for contraction, weakening, and dereliction of unrestricted resources. Instead, each judgment is equipped with a second context $[[Ur]]$ that holds variable bindings that can be used in an unrestricted fashion. The $[[t ►case ¹ν ᴇ x ⟼ u]]$ construct is used to bind a term of type $[[!T]]$ as a variable binding $[[{x ⫶ T}]]$ in the unrestricted context $[[Ur]]$. It's important to note that $[[t ►case ¹ν ᴇ x ⟼ u]]$ is not dereliction: one can still use $[[x]]$ several times within body $[[u]]$, or recreate $[[t]]$ by wrapping $[[x]]$ back as $[[ᴇ x]]$; while that wouldn't be possible in $[[let x ≔ derelict t in u]]$ of $\lambda_{L_1}$. Morally, we can view the pair of contexts $[[P]] ; [[Ur]]$ of $\lambda_{L_2}$ as a single context $[[P,!Ur]]$ of $\lambda_{L_1}$, where $[[!Ur]]$ is the context with the same variable bindings as $[[Ur]]$, except that all types are prefixed by $\ottstype{!}$.

In $\lambda_{L_2}$, contraction for unrestricted resources happens implicitly every time a rule has two subterms as premises. Indeed, the unrestricted context $[[Ur]]$ is duplicated in both premises, unlike the linear context $[[P]]$ that must be split into two disjoint parts. All unrestricted variable bindings are thus propagated to the leaves of the typing tree, that is, the rules with no premises $\lambda_{L_2}$\,/\,Id\textsubscript{Lin}, $\lambda_{L_2}$\,/\,Id\textsubscript{Ur}, and $\lambda_{L_2}$\,/\,$[[①]]$I. These three rules discard all bindings of the unrestricted context $[[Ur]]$ that aren't used, performing several implicit weakening steps. Finally, this system has two identity rules. The first one, $\lambda_{L_2}$\,/\,Id\textsubscript{Lin}, is the usual linear identity: it asks for the variable $[[x]]$ to be in the linear typing context, and later $[[x]]$ cannot be reused in another subterm. The second one, $\lambda_{L_2}$\,/\,Id\textsubscript{Ur}, is the unrestricted identity: it lets us use the variable $[[x]]$ from the unrestricted typing context in a place where a linear variable is expected, performing a sort of implicit dereliction.

\cite{andreoli_linlog_1992} has a detailed proof that both $\lambda_{L_1}$ and $\lambda_{L_2}$ are equivalent, in other terms, that a program $[[t]]$ types in the pair of contexts $[[P]] ; [[Ur]]$ in $\lambda_{L_2}$ if and only if it types in context $[[P,!Ur]]$ in $\lambda_{L_1}$.

\section{Back to a single context with the graded modal approach}

In~\cite{girard_bounded_1992}, Girard introduced a \emph{bounded} version of linear logic, where the $\ottstype{!}$ modality is accompanied by an index $[[m]]$ that specifies how many times an hypothesis can be used. This \emph{graded} modality $\ottstype{!}_{[[m]]}$ gives more flexibility than the binary choice we had before, where resources could either be strictly linear or fully unrestricted. The index $[[m]]$ --- that we will call \emph{mode} from now on --- takes its values in an arbitrary set equipped with a semiring or ringoid structure (it can be integers for precise use count; extended integers to take unrestricted use into account; or even intervals to represent minimum and maximum uses).

Having an arbitrary number of graded modalities $\left(\ottstype{!}_{[[m]]}\right)_{[[m]] \in \ottsmode{M}}$ instead of the single $\ottstype{!}$ modality of original linear logic means that we cannot really have a distinct context for each of them as in the dyadic presentation.

Since \cite{girard_bounded_1992}, the line of work of graded modalities has been fruitfully developed, but most bounded/graded linear calculi kept a presentation close to the monadic presentation $\lambda_{L_1}$, in which there is a clear distinction between linear resources and modality-annotated ones, and modality-annotated resources are kept "boxed" with their modality $\ottstype{!}_{[[m]]}$ inside the typing context $[[P]]$, and are only "deboxed" at the latest, through an explicit use of the dereliction rule.

However, that changed in 2014, when both \cite{ghica_bounded_2014} and \cite{petricek_coeffects_2014} introduced calculi where judgments have a single typing context, in which \emph{all} variable bindings carry a mode $[[m]]$ representing how they must be used. For instance, linear bindings $[[{x ⫶ T}]]$ are replaced by modal binding requiring \textcolor{modecolor}{one} use $[[{x : ˥ T}]]$. As a result, thanks to the variable bindings enriched with modes, modality-annotated resources $[[t]]\pmb{:}[[! m T]]$ can be deboxed very early --- as in the dyadic presentation --- in the form of a binding $[[{x : m T}]]$, without loosing any information and flexibility. We recover a system in which contraction, weakening, and dereliction can be made conveniently implicit, without loosing any control power over resource use! For that, we just have to define a few operators on \emph{modes}, that we will then lift to variable bindings, and then to typing contexts themselves.

In the spirit of \cite{ghica_bounded_2014} and subsequent \cite{bernardy_linear_2018,bernardy_modality_2020}, we'll now show what the concrete \emph{modal} presentation for intuitionistic $\lambda$-calculus or $\lambda_{L_m}$ looks like. We will take a very simple ringoid for modes, that is enough to model linearity equivalently as the previous presentations $\lambda_{L_1}$ and $\lambda_{L_2}$: $[[˥]]$ for variables that must be managed in strict linear fashion, and $[[ɷ]]$ for unrestricted ones. We give the following operation tables:

%\begin{figure}[t]
\begin{center}
\begin{tabular}{|c|c|c|}\hline
$\ottsmode{+}$ & $[[¹]]$ & $[[ω]]$ \\\hline
$[[¹]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hspace{1cm}
\begin{tabular}{|c|c|c|}\hline
$[[·]]$        & $[[¹]]$ & $[[ω]]$ \\\hline
$[[¹]]$        & $[[¹]]$ & $[[∞]]$ \\\hline
$[[ω]]$        & $[[∞]]$ & $[[∞]]$ \\\hline
\end{tabular}
\end{center}

We lift \emph{times} so that it can scale a variable binding by a mode $[[n]]$: we pose $[[n·({ x : m T })]] \btriangleq [[{ x : n · m T }]]$. We then extend this operator point-wise to act on whole typing contexts as in $[[n·P]]$, not just single bindings.

We also define typing context \emph{plus} as a partial operation where $[[({ x : m T }) + ({ x : m' T })]] = [[x]]:\!_{\![[m]]\ottsmode{+}[[m']]}[[T]]$ and  $[[({ x : m T }) + P]] = [[{ x : m T },P]]$\hspace*{0.6em}if $[[x]] \notin [[P]]$.

Equipped with those, we can move to the grammar and typing rules of $\lambda_{L_m}$, given in \cref{fig:linmod-grammar,fig:linmod-ty-term}. Constructor for $[[! m T]]$ is now $[[ˢᴇ n t]]$ (instead of $[[ᴇ t]]$ for $[[! T]]$).

\begin{codefig}{\caption{Grammar of linear $\lambda$-calculus in modal presentation ($\lambda_{L_m}$)}\label{fig:linmod-grammar}}{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
       [[v]] &::=& [[ˢλ x m ⟼ u]] \grammsep [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᴇ n v]] \\
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[ˢInl t]] \grammsep [[ˢInr t]] \grammsep [[ˢ( t1 , t2 )]] \grammsep [[ˢᴇ n t]] \grammsep [[t t']] \grammsep [[t ; t']] \\
&|\,& [[ t ►case ¹ν { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case ¹ν ( x1 , x2 ) ⟼ u]] \grammsep [[t ►case ¹ν ᴇ n x ⟼ u]] \\
&&\\
[[T]], [[U]] &::=& [[T m → U]] \grammsep [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! n T]] \\
[[m]], [[n]] &::=& [[˥]] \grammsep [[ɷ]] \grammsep [[m · n]] \\
&&\\
[[P]] &::=& [[{ }]] \grammsep [[{ x : m T }]] \grammsep [[P1 , P2]] \grammsep [[P1 + P2]] \grammsep [[m·P]] \\
\end{array}
}\end{codefig}

\begin{ottfig}{\caption{Typing rules for linear $\lambda$-calculus in modal presentation ($\lambda_{L_m}$)}\label{fig:linmod-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{$\lambda_{L_m}$\,/\,}
\ottdefnLinModXXTyXXterm{}\egroup
\end{ottfig}

In $\lambda_{L_m}$ we're back to a single identity rule $\lambda_{L_m}$\,/\,Id, that asks for $[[x]]$ to be in the typing context with a mode $[[m]]$ that must be \emph{compatible with a single linear use} (we note $[[˥ ⥶ m]]$). In our ringoid, with only two elements, we have both $[[˥ ⥶ ˥]]$ and $[[˥ ⥶ ɷ]]$, so $[[x]]$ can actually have any mode (either linear or unrestricted, so encompassing both $\lambda_{L_2}$\,/\,Id\textsubscript{Lin} and $\lambda_{L_2}$\,/\,Id\textsubscript{Ur}), but in more complex modal systems, not all modes have to be compatible with the unit of the ringoid\footnote{Actually, in the type system for \destcalculus{} detailed in \cref{sec:sec:syntax-type-system}, mode $[[¹↑]]$ is not compatible with unit $[[¹ν]]$.}. Rules $\lambda_{L_m}$\,/\,Id and $\lambda_{L_m}$\,/\,$[[①]]$I also allows to discard any context composed only of unrestricted bindings, denoted by $[[ɷ·P]]$ (equivalent to notation $[[!P]]$ in $\lambda_{L_1}$), so they are performing implicit weakening as in $\lambda_{L_2}$.

Every rule of $\lambda_{L_m}$ that mentions two subterms uses the newly defined $+$ operator on typing contexts in the conclusion of the rule. If a same variable $[[x]]$ is required in both $[[P1]]$ and $[[P2]]$ (either with mode $[[˥]]$ or $[[ɷ]]$, it doesn't matter), then $[[P1+P2]]$ will contain binding $[[{x : ɷ T}]]$. Said differently, the parent term will automatically deduce wether $[[x]]$ needs to be linear or unrestricted based on how (many) subterms use $[[x]]$, thanks to the $+$ operator. It's a vastly different approach than the linear context split for subterms in $\lambda_{L_{\{1,2\}}}$ and unrestricted context duplication for subterms in $\lambda_{L_2}$. I would argue that it makes the system smoother, and allows for easy extension without changing the typing rules much (compared to the monadic / dyadic presentation that are very specialized to handle linearity/unrestrictedness, and that alone).

Much as in $\lambda_{L_2}$, the exponential modality $\ottstype{!}_{[[n]]}$ is eliminated by a $[[t ►case ¹ν ᴇ n x ⟼ u]]$ expression, that binds the payload to a new variable $[[x]]$ with modality $[[n]]$. The original boxed value can be recreated if needed with $[[ˢᴇ n x]]$; indeed, as in $\lambda_{L_2}$, elimination for $\ottstype{!}_{[[n]]}$ is \emph{not} dereliction.

The last specificity of this system is that the function arrow $\ottstype{\multimap}$ now has a mode $[[m]]$ to which it consumes its argument. Actually that doesn't change the expressivity of the system compared to previous presentations; we would have been just fine with only a purely linear arrow, but because we have to assign a mode to any variable binding in this presentation, then it makes sense to allow this mode $[[m]]$ to be whatever the programmer wants, and not just default to $[[˥]]$. In application rule $\lambda_{L_m}$\,/\,$\ottstype{\multimap}$E, the typing context $[[P1]]$ required to type the argument is scaled in the conclusion of the rule by the mode $[[m]]$ that represent the "number" of use of the argument by the function.

\section{Deep modes: projecting modes through fields of data structures}

In original linear logic from Girard (see ILL and presentation $\lambda_{L_1}$ above), there is only a one-way morphism between $[[!T ⨂ !U]]$ and $[[!(T⨂U)]]$; we cannot go from $[[!(T⨂U)]]$ to $[[!T ⨂ !U]]$. In other terms, an unrestricted pair $[[!(T⨂U)]]$ doesn't allow for unrestricted use of its components. The pair can be duplicated or discarded at will, but to be used, it needs to be derelicted first, to become $[[T⨂U]]$, that no longer allow to duplicate or discard any of $[[T]]$ or $[[U]]$. As a result, $[[T]]$ and $[[U]]$ will have to be used exactly the same number of times, even though they are part of an unrestricted pair!

Situation is no different in $\lambda_{L_2}$ (resp. $\lambda_{L_m}$): although the pair of type $[[!(T⨂U)]]$ can be bound in an unrestricted binding $[[{x ⫶ T⨂U}]] \in [[Ur]]$ (resp. $[[{x : ɷ T⨂U}]]$) and be used as this without need for dereliction, when it will be pattern-matched on with $\lambda_{L_{\{2,m\}}}$\,/\,$\ottstype{\otimes}$E, implicit dereliction will still happen, and linear-only bindings will be made for its components: $[[{x1 ⫶ T},{x2 ⫶ U}]]$ in the linear typing context (resp. $[[{x1 : ˥ T},{x2 : ˥ U}]]$).

It's in fact useful to lift this limitation. One motivation for \emph{deep modes} (in the sense that they propagate throughout a data structure) is that they make it more convenient to write non-linear programs in a linear language. Indeed, in a modal linear $\lambda$-calculus with deep modes ($\lambda_{L_{dm}}$), functions of type $[[T ɷ → U]]$ have exactly the same expressive power has ones with type $[[T]]\,\ottstype{\to}\,[[U]]$ in STLC. In other terms, any program that is valid in STLC but doesn't abide by linearity can still type in $\lambda_{L_{dm}}$ without any restructuration needed!

Let's take the $\ottkw{fst}$ function as an example, that extracts the first component of a pair. With deep modes in $\lambda_{L_{dm}}$ we are able to write

\codehere{
\newoperator
  {\ottkw{fst}_{\ottkw{dm}}}{[[T⨂U ɷ → T]]}
  {\ottkw{fst}_{\ottkw{dm}}}{[[ˢλ x ɷ ⟼ x ►case ɷ (x1, x2) ⟼ x1]]}
}

as we would do in a non-linear language, like STLC, in which $\ottkw{fst}$ would have type $[[T⨂U]]\,\ottstype{\to}\,[[T]]$. On the other hand, in $\lambda_{L_m}$ as presented in \cref{fig:linmod-grammar,fig:linmod-ty-term}, we need to indicate individually for each component of the pair that we want them unrestricted (at least for the second one that we intent on discarding), and add extra elimination for the $\ottstype{!}$ modality:

\codehere{
\newoperator
  {\ottkw{fst}_{\ottkw{m}}}{[[T⨂(!ɷ U) ˥ → T]]}
  {\ottkw{fst}_{\ottkw{m}}}{\!\!\!\begin{array}[t]{l}[[
ˢλ x ˥ ⟼ x ►case ¹ν (x1, ux2) ⟼⮒
‥‥༼ux2 ►case ¹ν ᴇ ɷ x2 ⟼ x1༽
]]\end{array}}
}

Adding deep modes to a practical linear language is not a very original take; it has been done in Linear Haskell~\cite{bernardy_linear_2018} and \cite{lorenzen_oxidizing_2024}.
With deep modes, we get the following equivalences:

\codehere{\!\!\!\begin{array}[t]{rcl}
[[!m (T⨂U)]] &\simeq& [[(!m T) ⨂ (!m U)]] \\
[[!m (T⨁U)]] &\simeq& [[(!m T) ⨁ (!m U)]] \\
[[!m (!n T)]] &\simeq& [[!(m·n) T]]
\end{array}}

The only change needed on the grammar is that $\ottkw{case}$ constructs now take a mode $[[m]]$ to which they consume the scrutinee, that is propagated to the field(s) of the scrutinee in the body of the $\ottkw{case}$. The new typing rules for $\ottkw{case}$ are presented in \cref{fig:lindeepmod-ty-term}, all the other rules of linear $\lambda$-calculus with deep modes, $\lambda_{L_{dm}}$, are supposed identical to \cref{fig:linmod-ty-term}.

\begin{ottfig}{\caption{Altered typing rules for deep modes in linear $\lambda$-calculus in modal presentation ($\lambda_{L_{dm}}$)}\label{fig:lindeepmod-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{$\lambda_{L_{dm}}$\,/\,}
\ottdefnLinDeepModXXTyXXterm{}\egroup
\end{ottfig}

We observe that the typing context $[[P1]]$ in which the scrutinee types is scaled by $[[m]]$ in the conclusion of all the $\ottkw{case}$ rules. This is very analog to the application rule $\lambda_{L_{m}}$\,/\,$\ottstype{\multimap}$E: it makes sure that the resources required to type $[[t]]$ are consumed $[[m]]$ times if we want to use $[[t]]$ at mode $[[m]]$. Given that $[[{ x2 : ˥ T2 } ⊢ ˢ((), x2) : ① ⨂ T2]]$ (the pair uses $[[x2]]$ exactly once), if we want to extract the pair components with mode $[[ɷ]]$ to drop $[[x2]]$ (as in $\ottkw{fst}_{\ottkw{dm}}$), then $[[ˢ((), x2) ►case ɷ (x1, x2) ⟼ x2]]$ will require context $[[ɷ·({ x2 : ˥ T2 })]]$ i.e. $[[{ x2 : ɷ T2 }]]$. In other terms, we cannot use parts of a structure made using linear resources in an unrestricted way (as that would break linearity).

The linear $\lambda$-calculus with deep modes, $\lambda_{L_{dm}}$, will be the basis for our core contribution that follows in next chapter: the destination calculus \destcalculus{}.

\section{Semantics of linear $\lambda$-calculus (with deep modes)}

Many semantics presentations exist for lambda calculus. In \cref{fig:linsem} I present a small-step reduction system for $\lambda_{L_{dm}}$, in \emph{reduction semantics} style inspired from~\cite{felleisen_calculi_1987} and subsequent~\cite{danvy_refocusing_2004,biernacka_syntactic_2007}, that is, a semantics in which the evaluation context $[[C]]$ is manipulated explicitly and syntactically as a stack.

\def\foreverunspace{%
  \ifnum\lastnodetype=11
    \unskip\foreverunspace
  \else
    \ifnum\lastnodetype=12
      \unkern\foreverunspace
    \else
      \ifnum\lastnodetype=13
        \unpenalty\foreverunspace
      \fi
    \fi
  \fi
}

\begin{ottfig}{\caption{Small-step semantics for $\lambda_{L_{dm}}$}\label{fig:linsem}}
\bgroup
\renewcommand{\ottdrulename}[1]{$\lambda_{L_{dm}}$\,/\,}
\renewcommand\arraystretch{1.5}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \setbox0=\hbox{\ensuremath{#3}\foreverunspace}\ifdim\wd0=0pt ~ \else$\star$\fi & \text{#1} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{lll}}
  {\endarray$}
\makeatother
\drules{$[[C [ t ] ⟶ C' [ t' ] ]]$}{Small-step evaluation}{%
LinFocus-Left,
LinUnfocus-Left,
LinFocus-Right,
LinUnfocus-Right,
LinFocus-ProdOne,
LinUnfocus-ProdOne,
LinFocus-ProdTwo,
LinUnfocus-ProdTwo,
LinFocus-Exp,
LinUnfocus-Exp,
LinFocus-AppOne,
LinUnfocus-AppOne,
LinFocus-AppTwo,
LinUnfocus-AppTwo,
LinRed-App,
LinFocus-PatU,
LinUnfocus-PatU,
LinRed-PatU,
LinFocus-PatS,
LinUnfocus-PatS,
LinRed-PatL,
LinRed-PatR,
LinFocus-PatP,
LinUnfocus-PatP,
LinRed-PatP,
LinFocus-PatE,
LinUnfocus-PatE,
LinRed-PatE}
\egroup

\vspace{-0.5cm}

\begin{center}
$\star$~:~only allowed if $[[t]]$ is not already a value
\end{center}

\end{ottfig}

\egroup

Small-step evaluation rules are of three kinds:
\begin{itemize}
\item focusing rules (F) that split the current term under focus in two parts: an evaluation context component that is pushed on the stack $[[C]]$ for later use, and a subterm that is put under focus;
\item unfocusing rules (U) that recreate a larger term once the term under focus is a value, by popping the most recent evaluation component from the stack and merging it with the value;
\item contraction rules (C) that do not operate on the stack $[[C]]$ but just transform the term under focus when it's a redex.
\end{itemize}

To have a fully deterministic and and straightforward reduction system, focusing rules can only trigger when the subterm in question is not already a value (denoted by $\star$ in \cref{fig:linsem}).

Data constructors only have focusing and unfocusing rules, as they do not describe computations that can be reduced. In that regard, $[[ˢᴇ n t]]$ is treated as an unary data constructor. Once a data constructor is focused, it is evaluated fully to a value form.

In most cases, unfocusing and contraction rules could be merged into a single step. For example, a contraction could be triggered as soon as we have $[[(C ∘ (⬜ ►case m (x1,x2) ⟼ u))[(v1, v2)] ]]$, without needing recreate the term $[[(v1, v2) ►case m (x1,x2) ⟼ u]]$. However, I prefer the presentation in three distinct steps, that despite being more verbose, clear shows that contraction rules do not modify the evaluation context.

The rest of the system is very standard. In particular, contraction rules of $\lambda_{L_{dm}}$ --- that capture the essence of the calculus --- are very similar to those of strict lambda-calculi with sum and product types.
