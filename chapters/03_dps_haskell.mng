\chapter{A first implementation of destination passing in Haskell}\label{chap:dps-haskell}

In the previous chapter, we saw how we can build a $\lambda$-calculus from the ground up with first-class support for destination passing, and that still achieves memory safety. It's now time to put that into practice in an broadly used programming language.

This chapter revisits and refines the results presented in \cite{bagrel_destination-passing_2024}, which was published at JFLA 2024 prior to the completion of the theoretical work presented in \cref{chap:dest-calculus}.

\section{Implementation target: Linear Haskell}\label{intro-dps-haskell}

Designing an industrial-grade programming language \emph{just} to add support for destinations would be a gigantic task, with much risk to fail. Instead, we base our work on an existing functional programming language, for instance, Haskell\footnote{There aren't many other industrial-grade functional programming languages, and most of them don't have support for linear types, which are the first piece required to make a system like \destcalculus{} safe. Thanks to recent work from~\citet{lorenzen_oxidizing_2024}, OCaml will soon have support for \emph{affinity}, but not for \emph{linearity} (despite the aforementioned work using the term linearity). Affinity ensures that a resource can be used at most once, but does not guarantee it will be used exactly once; it may be discarded without use. This would be problematic for us, since we rely on the guarantee that destinations cannot be dropped: it ensures that when an \texttt{ampar} no longer contains destinations on the right, all holes in the corresponding structure have been filled.}, and we aim to implement as many ideas from \destcalculus{} (see \cref{chap:dest-calculus}) as possible in this language. Haskell is equipped with support for linear types through its main compiler, \emph{GHC}, since version 9.0.1~\cite{bernardy_linear_2018}. We kindly redirect the reader to \cref{sec:intro-linearity} for a primer on Linear Haskell and how to use it to control resource use. Moreover, Haskell is a \emph{pure} functional language, which means our destination-passing API will have to be pure, with (naturally impure) memory writes safely encapsulated within it, as explained in the introduction, so that no mutation or incomplete structure can be observed by the user.

The main challenge is that Haskell, albeit incorporating linear types, doesn't have any concept of scope or age control. So we won't be able to avoid the fundamental issues presented in \cref{sec:scope-escape-dests} without imposing limitations on the flexibility of the system.

\section{Safe destination passing with just linear types}\label{sec:restrict-dps-linhask}

As we've seen in \cref{sec:scope-escape-dests}, linear types are not enough, alone, to make \destcalculus{} safe. We definitely need ages and scope control. In this chapter, we will take a radical decision: we restrict destinations to only be used to build non-linear (a.k.a. unrestricted) data structures, so that the issue of scope escape disappears completely. This also rules out the potential interaction between sibling destinations also described in \cref{sec:scope-escape-dests}. In exchange, we won't be able to store destinations inside data structures built using ampars. We further use the name \emph{DPS Haskell} to refer to this prototype destination-passing API we develop in Haskell.

In practice, it means that a destination can only be filled with an unrestricted value (which forbids filling a destination with another destination, as destinations are linear resources). We will use the Haskell type \mintinline{haskellc}/data UDest t/ to represent destinations for unrestricted values, and type \mintinline{haskellc}/data UAmpar s t/ to represent ampars where the \mintinline{haskellc}/s/ side is unrestricted.

Because there is no difference when creating the spine\footnote{The spine refers to the main chain of connected data constructors that form the overall shape of a data structure, connecting its ``leaves''.} of a data structures that is linear or unrestricted, our fill functions that add a new hollow constructor to an existing structure will be exactly the same as in \destcalculus{}. Only the signature of fill operators acting on the leaves of data structures will change in the implementation compared to the formal definition. The mapping of operators and types between \destcalculus{} and DPS Haskell is given in \cref{fig:mapping-destcalculus-dpshaskell}

In \destcalculus{}, destinations always have finite ages. As a result, we use age $[[∞]]$ as a marker that something doesn't contain destinations. This way, we can still enforce scope control independently of linearity. This is particularly visible in rule \rref*{\CTyTerm\CSep\CFromA} (\cref{fig:ty-term}), where a linear resource can be extracted from the right of an ampar as long as it has age $[[∞]]$.

In DPS Haskell however, we don't have ages. So we use the fact that destinations are always linear too, and we employ the $[[ω]]$ multiplicity as a marker that something doesn't contain destinations. Of course, this doesn't come for free; by doing so we conflate the absence of destination with non-linearity. Controlling scopes that way means we must be more conservative to preserve safety, which restrains the possibilities of a few operators. % In DPS Haskell, mode $[[ɷ]]$ is equivalent to the pair $[[ω∞]]$ of \destcalculus{}, as there is no no construct in our language that can have unrestricted multiplicity with an intrinsically finite age, it makes sense to map the mode $[[ɷ]]$ of Linear Haskell. Consequently, $[[ˢᴇ ω∞ term]]$ corresponds to \mintinline{haskellc}/Ur term/.


\begin{figure}

\emph{Destination-filling operators:}

\begin{center}
\begin{tabular}{|l|lcl|}\hline
\destcalculus{} & DPS Haskell && \\\hline\hline
$[[d ⨞ ()]]$ & \mintinline{haskellc}/fill @'() d/ &or& \mintinline{haskellc}/d &fill @'()/ \\\hline
$[[d ⨞ Inl]]$ & \mintinline{haskellc}/fill @'Inl d/ &or& \mintinline{haskellc}/d &fill @'Inl/ \\\hline
$[[d ⨞ Inr]]$ & \mintinline{haskellc}/fill @'Inr d/ &or& \mintinline{haskellc}/d &fill @'Inr/ \\\hline
$[[d ⨞ (,)]]$ & \mintinline{haskellc}/fill @'(,) d/ &or& \mintinline{haskellc}/d &fill @'(,)/ \\\hline
$[[d ⨞ ᴇ ω∞]]$ & \mintinline{haskellc}/fill @'Ur d/ &or& \mintinline{haskellc}/d &fill @'Ur/ \\\hline
$[[d ⨞ ( λ x m ⟼ term )]]$ & \mintinline{haskellc}/fillLeaf (\x → term) d/ &or& \mintinline{haskellc}/d &fillLeaf (\x → term)/ \\\hline
$[[d ⨞· term]]$ & \mintinline{haskellc}/fillComp term d/ &or& \mintinline{haskellc}/d &fillComp term/ \\\hline
$[[d ◀ term]]$ & \mintinline{haskellc}/fillLeaf term d/ &or& \mintinline{haskellc}/d &fillLeaf term/ \\\hline

\end{tabular}

\medskip

(one can see \mintinline{haskellc}/&fill @'/ as the equivalent of $[[⨞]]$ in \destcalculus{}, 
\mintinline{haskellc}/&fillLeaf/ as $[[◀]]$, and \mintinline{haskellc}/&fillComp/ as $\mathop{\triangleleft\mycirc}$)

\end{center}

\smallskip

\emph{Types:}

\begin{center}
\begin{tabular}{|l|l|}\hline
\destcalculus{} & DPS Haskell \\\hline\hline
$[[⌊ T ⌋ ω∞]]$ & \mintinline{haskellc}/UDest t/ \\\hline
$[[! ω∞ S ⧔ T]]$ & \mintinline{haskellc}/UAmpar s t/ \\\hline
\end{tabular}
\end{center}

\caption{Mappings between \destcalculus{} and DPS Haskell}\label{fig:mapping-destcalculus-dpshaskell}
\end{figure}

Now, let's see how programs look like in DPS Haskell. We'll start by reimplementing examples from the previous chapter, to see how they take form in a practical setting.

\section{A glimpse of DPS Haskell programming}\label{sec:motivating-examples}

The following subsections present three typical cases in which DPS programming brings expressiveness or performance benefits over a more traditional functional implementation. We'll start by revisiting some examples from \cref{sec:working-with-dests}.

\subsection{Efficient difference lists}\label{ssec:dpshaskell-dlist}

Linked lists are a staple of functional programming, but they aren't efficient for concatenation, as we've seen in \cref{ssec:efficient-queue}, especially when the concatenation calls are nested to the left. For more efficient concatenation, we can define destination-based difference lists, which are effectively lists with a hole at the end.

This example will be the opportunity to show how DPS Haskell operates on memory. So far, with \destcalculus{}, we worked on a theoretical memory model with global substitutions (on the whole evaluation context) and no representation of allocations. Here, with DPS Haskell, we assume that we have a proper heap. We pay no attention right now to garbage collection / deallocation; instead we will deal with this topic in \cref{ssec:impl-compact-regions}.

Following the mapping tables of \cref{sec:restrict-dps-linhask}, we know the DPS Haskell representation of difference lists is \mintinline{haskellc}/type DList t = UAmpar [t] (UDest [t])/. As announced previously, because our difference lists are built using \mintinline{haskellc}/UAmpar/s, they cannot host linear resources. That's the major limitation of this first approach of DPS programming for Haskell (in particular, some examples developed in \cref{chap:dest-calculus}, like the breadth-first tree traversal, won't type as is in DPS Haskell without modifications).

As with ampars in \destcalculus{}, the left side of the \mintinline{haskellc}/UAmpar/ carries the structures being built, while the right side carries the destinations of the structure: the \mintinline{haskellc}/UDest [t]/ must be filled (with an unrestricted \mintinline{haskellc}/[t]/) to get a readable (unrestricted) \mintinline{haskellc}/[t]/.

The implementation of destination-backed difference lists is presented in \cref{table:impl-dlist} (we recall that the \destcalculus{}'s version is presented in \cref{fig:impl-dlist-queue}). Note that Haskell uses \mintinline{haskellc}/⩴/ for typing, and \mintinline{haskellc}/(:)/ for the list \emph{cons} constructor, which is the opposite of what these symbols mean in \destcalculus{}.
% Doesn't bring much more compared to fig:schema-dlist-concat
% \begin{figure}\centering
%   \includegraphics[width=10cm]{fillComp.png}
%   \caption{Memory behavior of \mintinline{haskellc}/fillComp ⩴ UAmpar s t ⊸ UDest s ⊸ t/}
%   \label{fig:schema-fillComp}
% \end{figure}

\begin{listing}[t]
\figtextsize
\begin{minted}[linenos]{haskellc}
-- we recall here the definition of list type in Haskell
data [t] = []          -- nil constructor
           | (:) t [t]  -- cons constructor

-- we reuse the same linear token API as in Section 1.8
withToken ⩴ (Token ⊸ Ur t) ⊸ Ur t

type DList t = UAmpar [t] (UDest [t])

newDList ⩴ Token ⊸ DList t
newDList = newUAmpar @[t]

append ⩴ DList t ⊸ t → DList t
dlist `append` x =
  dlist `updWith` \d → case (d &fill @'(:)) of
    (dh, dt) → (dh &fillLeaf x) ; dt

concat ⩴ DList t ⊸ DList t ⊸ DList t
dlist1 `concat` dlist2 = dlist1 `updWith` \dt1 → (dt1 &fillComp dlist2)

toList ⩴ DList t ⊸ Ur [t]
toList dlist = fromUAmpar' (dlist `updWith` \dt → dt &fill @'[])
\end{minted}
\caption{Implementation of difference lists in DPS Haskell}
\label{table:impl-dlist}
\end{listing}

% TODO: update syntax
\begin{figure}\centering
  \scalebox{\figscale}{\tikzfig{schemas/alloc}}
  \caption{Memory representation of \texttt{newUAmpar tok} in DPS Haskell}
  \label{fig:schema-alloc}
\end{figure}

\begin{figure}\centering
  \scalebox{\figscale}{\tikzfig{schemas/dlist-append}}
  \caption{Memory behavior of \texttt{append newUAmpar 1} in DPS Haskell}
  \label{fig:schema-dlist-append}
\end{figure}

\begin{figure}\centering
  \scalebox{\figscale}{\tikzfig{schemas/dlist-concat}}
  \caption{Memory behavior of \texttt{concat dlist1 dlist2} in DPS Haskell (based on \texttt{fillComp})}
  \label{fig:schema-dlist-concat}
\end{figure}

\begin{figure}\centering
  \scalebox{\figscale}{\tikzfig{schemas/dlist-toList}}
  \caption{Memory behavior of \texttt{toList dlist} in DPS Haskell}
  \label{fig:schema-dlist-toList}
\end{figure}

\begin{itemize}
  \item \mintinline{haskellc}/newDList/ is just an alias for \mintinline{haskellc}/newUAmpar/, to linearly exchange a token for a new \mintinline{haskellc}/UAmpar [t] (UDest [t])/---that is, exactly \mintinline{haskellc}/DList t/. Recall that UAmpars are treated as linear resources now, and are created in exchange of a linear token, following the principles described in \cref{sec:linear-scopes,sec:implem-destcalculus}. There is no data
    in this new UAmpar yet, it's just a hole and a destination pointing to it, as we see in \cref{fig:schema-alloc};

  \item \mintinline{haskellc}/append/ (\cref{fig:schema-dlist-append}) adds an element at the tail
    position of a difference list. For this, it first uses
    \mintinline{haskellc}/fill @'(:)/ to fill the hole at the end of the list represented by
    \mintinline{haskellc}/d ⩴ UDest [t]/ with a hollow \mintinline{haskellc}/(:)/ constructor that has two new holes, pointed to by \mintinline{haskellc}/dh ⩴ UDest t/ and \mintinline{haskellc}/dt ⩴ UDest [t]/. Then,
    \mintinline{haskellc}/fillLeaf/ fills the hole represented by
    \mintinline{haskellc}/dh/ with the value
    of type \mintinline{haskellc}/t/
    to append. The hole at the end of the resulting difference list is the one pointed by \mintinline{haskellc}/dt ⩴ UDest [t]/ which hasn't been filled yet, and stays on the right side of the resulting UAmpar.

  \item \mintinline{haskellc}/concat/ (\cref{fig:schema-dlist-concat}) concatenates two difference lists,
    \mintinline{haskellc}/dlist1/ and \mintinline{haskellc}/dlist2/. It uses \mintinline{haskellc}/fillComp/ to fill the destination \mintinline{haskellc}/dt1/
    of the first difference list with the
    root of the second difference list \mintinline{haskellc}/dlist2/. The resulting \mintinline{haskellc}/UAmpar/
    object hence has the same root as the first list, holds the
    elements of both lists, and inherits the hole of the second list. Memory-wise,
    \mintinline{haskellc}/concat/ just writes the address of the second list into the hole of the first one.

  \item \mintinline{haskellc}/toList/ (\cref{fig:schema-dlist-toList}) completes the UAmpar structure by plugging a new \emph{nil} \mintinline{haskellc}/[]/ constructor into its hole with \mintinline{haskellc}/fill @'[]/, and then removes the \mintinline{haskellc}/UAmpar/ wrapper as the structure is now complete, using \mintinline{haskellc}/fromUAmpar'/. The completed list is wrapped in \mintinline{haskellc}/Ur/, as it is allowed to be used non-linearly---it's always the case with UAmpars---and so that it can also escape the linear scope created by \mintinline{haskellc}/withToken/ if needed.
\end{itemize}

Linearity of UAmpars, enforced by the linear token technique, is essential\footnote{In \destcalculus{} we could choose to use a smart renaming technique for hole names (morally equivalent to copy-on-write) so that ampars don't have to be managed linearly. Here, as we aim for an efficient implementation, the idea of copying ampars and updating the target of destinations on-the-go doesn't seem promising.} to allow safe, in-place, memory updates when destination-filling operations occur, as shown in \cref{fig:schema-alloc,fig:schema-dlist-append,fig:schema-dlist-concat,fig:schema-dlist-toList}. Otherwise, we could first complete a difference list with \mintinline{haskellc}/let Ur l = toList dlist/, then add a new cons cell to \mintinline{haskellc}/dlist/ with \mintinline{haskellc}/append dlist x/ (actually reusing the destination inside \mintinline{haskellc}/dlist/ for the second time). This would create a hole inside \mintinline{haskellc}/l/, although it is of type \mintinline{haskellc}/[t]/ so we would be able to pattern-match on it, and might get a segfault!

The performance of this implementation (compared to a functional encoding of difference lists) is discussed in \cref{ssec:benchmark-dlist}.

% We could expect the implementation of difference list described above to be more efficient than the functional encoding (where a difference list \mintinline{haskellc}/x :/\,$\holesq$ is represented by the function \mintinline{haskellc}/\ys → x : ys/). We'll see in \cref{sec:benchmark} that the prototype implementation of DPS Haskell primitives (covered in \cref{sec:implementation}) cannot yet demonstrate these performance improvements.

\subsection{Breadth-first tree traversal}\label{ssec:bf-tree-traversal}

Let's move on now to breadth-first tree traversal. We want to traverse a binary tree and update its node values so that they are numbered in a breadth-first order.

In \cref{sec:bft}, we took full advantage of \destcalculus{}'s flexibility, and implemented the breadth-first traversal using a queue to drive the processing order, as we would do in an imperative programming language. This queue contained both input subtrees and destinations to output subtrees, in breadth-first order. Furthermore, the queue was implemented efficiently using a pair of a queue and a ampar-based difference list.

In DPS Haskell though, destinations cannot be filled with linear elements; so, an ampar-based difference list wouldn't be able to store the destinations we need to store (the one representing the output subtrees), and consequently, neither would an ampar-based efficient queue.

So we have no choice than to revert to the regular, non-ampar-based data structures of Haskell to build a suitable Hood-Melville~\cite{hood_queue_1981} queue. We see here that destination passing in Haskell cannot fully replace constructors unlike in \destcalculus{}; both data constructors and destination-filling primitives need to coexist\footnote{Destination-based data structure building is more general than constructor-based structure building as long as we can fill destinations with linear resources, as in \destcalculus{}. When we add the restriction that destination can only be filled with unrestricted elements, as we do in DPS Haskell, then it is no longer more general than constructor-based structure building, and we need to conserve both building ways in the language to stay at the same level of expressiveness.}. The implementation of Hood-Melville queues is presented in \cref{table:impl-hm-queue}.

As before, for the breadth-first traversal, we'll keep a queue of pairs of a tree to be relabeled and of the destination where the relabeled result is expected, and process each of them when their turn comes. The only thing that changes is the implementation for the queue. The DPS Haskell implementation of breadth-first tree traversal is provided in \cref{table:impl-bfs-tree-traversal}.

% TODO: add implem of hood melville queue, and revisit code snippet

\begin{listing}
\figtextsize
\begin{minted}[linenos]{haskellc}
data HMQueue t = ¤HMQueue [t] [t]

newHMQueue :: HMQueue t
newHMQueue = ¤HMQueue [] []

singleton :: t ⊸ HMQueue t
singleton x = ¤HMQueue [x] []

toList :: HMQueue t ⊸ [t]
toList (¤HMQueue front backRev) = front ++ reverse backRev

enqueue :: HMQueue t ⊸ t ⊸ HMQueue t
enqueue (¤HMQueue front backRev) x = ¤HMQueue front (x : backRev)

dequeue :: HMQueue t ⊸ Maybe (t, HMQueue t)
dequeue (¤HMQueue front backRev) = case front of
  [] → case reverse backRev of
    [] → Nothing
    (x : front') → Just (x, ¤HMQueue front' [])
  (x : front') → Just (x, ¤HMQueue front' backRev)
\end{minted}
\caption{Implementation of Hood-Melville queue in Haskell}
\label{table:impl-hm-queue}
\end{listing}

\begin{listing}
\figtextsize
\begin{minted}[linenos]{haskellc}
data Tree t = ¤Nil | ¤Node t (Tree t) (Tree t)

relabelDps ⩴ Token ⊸ Tree t → Tree Int
relabelDps tree = fst (mapAccumBfs (\st _ → (st + 1, st)) 1 tree)

mapAccumBfs ⩴ ∀ s t u. Token ⊸ (s → t → (s, u)) → s → Tree t → Ur (Tree u, s)
mapAccumBfs tok f s0 tree =
  fromUAmpar (newUAmpar @(Tree u) tok `updWith` \dtree → go s0 (singleton (¤Ur tree, dtree)))
  where
    go ⩴ s → Queue (Ur (Tree t), UDest (Tree u)) ⊸ Ur s
    go st q = case dequeue q of
      ¤Nothing → ¤Ur st
      ¤Just ((utree, dtree), q') → case utree of
        ¤Ur ¤Nil → dtree &fill @'Nil ; go st q'
        ¤Ur (¤Node x tl tr) → case (dtree &fill @'Node) of
          (dy, dtl, dtr) →
            let q'' = q' `enqueue` (¤Ur tl, dtl) `enqueue` (¤Ur tr, dtr)
                (st', y) = f st x
              in dy &fillLeaf y ; go st' q''
\end{minted}
\caption{Implementation of breadth-first tree traversal in DPS Haskell}
\label{table:impl-bfs-tree-traversal}
\end{listing}

Except from the choice of queue implementation, and the fact that ampars must be created from \mintinline{haskellc}/Token/s, this implementation is very much the same as in \cref{sec:bft}. In the signature of \mintinline{haskellc}/go/ function, the linear arrow enforces the fact that every destination ever put in the queue is eventually consumed at some point, which guarantees that the output tree is complete after the function has run.

As we will see in \cref{sec:benchmark}, this imperative-style implementation of breadth-first traversal in DPS Haskell, despite not using efficient queue representations such as difference lists, still shows substantial performance gains compared to the functional implementation from~\citet{gibbons_phases_2023}, which we must admit, while pretty elegant, was not designed with performance as a primary focus.

\section{DPS Haskell API and design concerns}\label{sec:api}

\cref{table:destination-api} presents the pure API of DPS Haskell. This API is sufficient to implement all the examples of \cref{sec:motivating-examples}. This section explains its various parts in detail and how it compares to the calculus of \cref{chap:dest-calculus}.

\begin{listing}[t]
\figtextsize
\begin{minted}[linenos]{haskellc}
data Token
dup ⩴ Token ⊸ (Token, Token)
drop ⩴ Token ⊸ ()
withToken ⩴ ∀ t. (Token ⊸ Ur t) ⊸ Ur t

data UAmpar s t
newUAmpar ⩴ ∀ s. Token ⊸ UAmpar s (UDest s)
tokenBesides ⩴ ∀ s t. UAmpar s t ⊸ (UAmpar s t, Token)
toUAmpar ⩴ ∀ s. Token ⊸ s → UAmpar s ()
fromUAmpar ⩴ ∀ s t. UAmpar s (Ur t) ⊸ Ur (s, t)
fromUAmpar' ⩴ ∀ s. UAmpar s () ⊸ Ur s
updWith ⩴ ∀ s t u. UAmpar s t ⊸ (t ⊸ u) ⊸ UAmpar s u

data UDest t
type family UDestsOf lCtor t  -- returns dests associated to fields of constructor
fill ⩴ ∀ lCtor t. UDest t ⊸ UDestsOf lCtor t
fillComp ⩴ ∀ s t. UAmpar s t ⊸ UDest s ⊸ t
fillLeaf ⩴ ∀ t. t → UDest t ⊸ ()
\end{minted}
\caption{DPS Haskell API, initial version}
\label{table:destination-api}
\end{listing}

\subsection{The UAmpar type}

As with Ampars from the previous chapter, UAmpar structures, that serve as a wrapper for structures with holes, can be freely passed around and stored, but need to be completed before any reading i.e. pattern-matching can be made on them. As a result, \mintinline{haskellc}/UAmpar s t/ is defined as an opaque data type, with no public constructor.

In \mintinline{haskellc}/UAmpar s t/, \mintinline{haskellc}/s/ stands for the type of the structure being built, and \mintinline{haskellc}/t/ is the type of what needs to be linearly consumed before the structure can be read. Eventually, when complete, the structure will be wrapped in \mintinline{haskellc}/Ur/; this illustrates the fact that it has been made only from unrestricted elements.

The \mintinline{haskellc}/newUAmpar/ operator is the main way for a user of the API to create a new UAmpar. Its signature is equivalent to the one of $[[allocIP]]$ from \cref{sec:implem-destcalculus}.

Also, sometimes, when programming in DPS style, we won't have a \mintinline{haskellc}/Token/ at hand, but only an existing \mintinline{haskellc}/UAmpar/. In this case, we can piggyback on the linearity of that existing UAmpar (as it linearly depends, directly or indirectly, on a token) to get a new linear \mintinline{haskellc}/Token/ so that we can spawn a new UAmpar, or any other linear resource really. This is the goal of the \mintinline{haskellc}/tokenBesides/ function.

The operator \mintinline{haskellc}/tokenBesides/ is particularly useful when implementing efficient queues in DPS Haskell (as in \cref{ssec:efficient-queue}):

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
type DList t = UAmpar [t] (UDest [t])
data EffQueue t = ¤EffQueue [t] (DList t)

newEffQueue ⩴ Token ⊸ EffQueue t
newEffQueue tok = ¤EffQueue [] (newUAmpar @[t] tok)

dequeue ⩴ EffQueue t ⊸ Maybe (t, EffQueue t)
dequeue (¤EffQueue front back) = case front of
  [] → case (toList back) of
    Ur [] → Nothing
    Ur (x : xs) → Just (x, (¤EffQueue xs °\mold{(newUAmpar \textcolor{typcolor}{\textbf{@[t]}} tok)}°))
  (x : xs) → Just (x, (¤EffQueue xs back))
\end{minted}
}
\end{unbreakable}

In the second branch of the inner-most \mintinline{haskellc}/case/ in \mintinline{haskellc}/dequeue/, when we have transformed the back difference list (from which we used to write) into the new front list (from which we will now read), we have to create a new back difference list, that is, a new ampar. However, we don't have a token at hand. It would be quite unpractical for a function like \mintinline{haskellc}/dequeue/ to ask for a linear token to be passed; it's better if tokens are only requested when spawning a new data structure, not when operating on one. That's why \mintinline{haskellc}/tokenBesides/ comes handy: we can instead reuse the linearity requirement of the existing UAmpar:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
tokenBesides ⩴ ∀ s t. UAmpar s t ⊸ (UAmpar s t, Token)

dequeue ⩴ EffQueue t ⊸ Maybe (t, EffQueue t)
dequeue (¤EffQueue front back) = case front of
  [] → case tokenBesides back of (back, tok) → case (toList back) of
    Ur [] → drop tok ; Nothing
    Ur (x : xs) → Just (x, (¤EffQueue xs (newDList tok)))
  (x : xs) → Just (x, (¤EffQueue xs back))
\end{minted}
}
\end{unbreakable}

This new version is not perfect either though. We have to create a new token before even knowing if we will need it. Indeed, we can only know if a new token is really needed when we have read the content of the old UAmpar, and we can only read it when it is no longer a UAmpar... so at that point we cannot spawn a new token any longer. So the solution is to always spawn a new token before consuming the old UAmpar, taking the risk that if the queue is really empty, we will have to discard the fresh token immediately (with \mintinline{haskellc}/drop/) before returning \mintinline{haskellc}/Nothing/. Implicit token passing, as proposed by Linear Constraints~\cite{spiwack_linearly_2022,spiwack_linear_prop_2023}, seems to really be the way forward to get rid of this kind of issues altogether.

%------------------------------

\paragraph{Getting out of an UAmpar}

Same as in \destcalculus{}, the structure encapsulated within a UAmpar can be released in two ways: with \mintinline{haskellc}/fromUAmpar'/, the value on the \mintinline{haskellc}/t/ side must be unit (\mintinline{haskellc}/()/), and just the complete \mintinline{haskellc}/s/ is returned, wrapped in \mintinline{haskellc}/Ur/. With \mintinline{haskellc}/fromUAmpar/, the type on the \mintinline{haskellc}/t/ side must be of the form \mintinline{haskellc}/Ur t'/, and then a pair \mintinline{haskellc}/Ur (s, t')/ is returned. We recall that in \destcalculus{}, we had $\ottkw{from}_{\ottkw{\ltimes} }' : [[S⧔① ¹ν → S]]$ and $\ottkw{from}_{\ottkw{\ltimes} } : [[S⧔(! ¹∞ T) ¹ν → (S⨂!¹∞ T)]]$, where the left side, when extracted from the ampar, is not wrapped in any modality.

Here in DPS Haskell, it is actually safe to wrap the structure that has been built in \mintinline{haskellc}/Ur/ because, as we've said previously, its leaves either come from non-linear sources (as \mintinline{haskellc}/fillLeaf ⩴ t → UDest t ⊸ ()/ consumes its first argument non-linearly) or are made of 0-ary constructors added with \mintinline{haskellc}/fill/, both of which can be used in an unrestricted fashion safely; and its spine, made of hollow constructors, can be duplicated at will.

% \mintinline{haskellc}/UAmpar s t/ has a linear functor instance to map on the \mintinline{haskellc}/t/ side (the one carrying destinations) which forces the continuation to be linear:

% {\figtextsize
% \begin{minted}[escapeinside=°°]{haskellc}
% instance Functor (UAmpar a) where
%   fmap ⩴ ∀ b c. (b ⊸ c) ⊸ UAmpar a b ⊸ UAmpar b c
%   fmap f (¤UAmpar (x, d)) = ¤UAmpar (x, f d)
% \end{minted}
% }

% And \mintinline{haskellc}/newUAmpar ⩴ ∀ s. Token ⊸ UAmpar s (UDest s)/` is the only function in which a \mintinline{haskellc}/UDest/ appears in positive position, but locked by an \mintinline{haskellc}/UAmpar/ (which is an opaque wrapper for the user). So destinations can only ever be accessed by mapping over an \mintinline{haskellc}/UAmpar/ with \mintinline{haskellc}/fmap//\mintinline{haskellc}/`updWith`/, and cannot leak to the outside. It isn't possible either for a \mintinline{haskellc}/UDest t/ to be linearly consumed by filling another \mintinline{haskellc}/UDest (UDest t)/ with \mintinline{haskellc}/fillLeaf/, as the first argument of the \mintinline{haskellc}/fillLeaf/ function isn't used linearly.\footnote{It would actually be desirable to have \mintinline{haskellc}/UDest (UDest t)/ work. But it turns out that doing so naively compromises the type safety properties related to linearity that we describe in this section. How to recover type safety in presence of destinations of destinations is still an open problem.}

% Morally, this linear \mintinline{haskellc}/Functor/ instance says that one can temporary forget about the root of the structure being built, and just manipulate the destinations as first-class objects that will produce remote building effects onto the structure that is invisible in the inner scope.

\subsection{Destination-filling functions}\label{ssec:fill-functions-v1}

The last part of the API is the one in charge of actually building the structures in a top-down fashion. To fill a hole represented by \mintinline{haskellc}/UDest t/, three functions are available:

\mintinline{haskellc}/fillLeaf ⩴ ∀ t. t → UDest t ⊸ ()/ uses a value of type \mintinline{haskellc}/t/ to fill the hole represented by the destination, as $[[◀]]$ from \destcalculus{}. However, if the destination is consumed linearly, as in \destcalculus{}, the value to fill the hole isn't (as indicated by the first non-linear arrow). This is key to the fact that UAmpars only host unrestricted data. Memory-wise, when \mintinline{haskellc}/fillLeaf/ is used, the address of the object of type \mintinline{haskellc}/t/ is written into the memory cell pointed to by the destination of type \mintinline{haskellc}/UDest t/ (see \cref{fig:schema-fillLeaf}).

\mintinline{haskellc}/fillComp ⩴ ∀ s t. UAmpar s t ⊸ UDest s ⊸ t/ is used to plug two \mintinline{haskellc}/UAmpar/ objects together. The parent \mintinline{haskellc}/UAmpar/ isn't represented in the signature of the function, as with the similar operator $\mathop{\triangleleft\mycirc}$ from \destcalculus{}. Instead, only the hole of the parent UAmpar that will receive the address of the child UAmpar is represented in the signature of the function by \mintinline{haskellc}/UDest s/; while \mintinline{haskellc}/UAmpar s t/ in the signature refers to the child UAmpar. A call to \mintinline{haskellc}/fillComp/ always takes place in the scope of \mintinline{haskellc}/`updWith`/ over the parent one:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
parent ⩴ UAmpar BigStruct (UDest SmallStruct, UDest OtherStruct)
child ⩴ UAmpar SmallStruct (UDest Int)
comp = parent `updWith` \(ds, do) → (ds &fillComp child, do)
       ⩴ UAmpar BigStruct (UDest Int, UDest OtherStruct)
\end{minted}
}
\end{unbreakable}

The resulting structure \mintinline{haskellc}/comp/ is morally a \mintinline{haskellc}/BigStruct/ like \mintinline{haskellc}/parent/, that inherited the hole from the child structure (\mintinline{haskellc}/UDest Int/) and still has its other hole (\mintinline{haskellc}/UDest OtherStruct/) waiting to be filled. An example of memory behavior of \mintinline{haskellc}/fillComp/ in action can be seen in \cref{fig:schema-dlist-concat}.

Finally, \mintinline{haskellc}/fill ⩴ ∀ lCtor t. UDest t ⊸ UDestsOf lCtor t/ is the generic function to fill a destination with hollow constructors. It takes a data constructor as a type parameter (\mintinline{haskellc}/lCtor/) and allocates a corresponding hollow constructor, that is, a heap object that has the same header as the specified constructor but unspecified fields. The address of the allocated hollow constructor is written in the destination that is passed to \mintinline{haskellc}/fill/. An example of the memory behavior of \mintinline{haskellc}/fill @'(:) ⩴ UDest [t] ⊸ (UDest t, UDest [t])/ is given in \cref{fig:schema-fillCons} and the one for \mintinline{haskellc}/fill @'[] ⩴ UDest [t] ⊸ ()/ is given in \cref{fig:schema-fillNil}.

The \mintinline{haskellc}/fill/ function also returns one destination of matching type for each of the fields of the constructor; in Haskell this is represented by \mintinline{haskellc}/UDestsOf lCtor t/. \mintinline{haskellc}/UDestsOf/ is a type family (i.e. a function from types to types) whose role is to map a constructor (lifted as a type) to the type of destinations for its fields. For example, \mintinline{haskellc}/UDestsOf '[] [t] = ()/ and \mintinline{haskellc}/UDestsOf '(:) [t] = (UDest t, UDest [t])/. More generally, the \mintinline{haskellc}/UDestsOf/ typeclass reflects the duality between the types of fields of a constructor and the ones of destinations for a hollow constructor, as first evoked in \cref{ssec:build-up-vocab}.

\begin{figure}\centering
  \scalebox{\figscale}{\tikzfig{schemas/fillCons}}
  \caption{Memory behavior of \texttt{fill @'(:) ⩴ UDest [t] ⊸ (UDest t, UDest [t])}}
  \label{fig:schema-fillCons}

  \scalebox{\figscale}{\tikzfig{schemas/fillNil}}
  \caption{Memory behavior of \texttt{fill @'[] ⩴ UDest [t] ⊸ ()}}
  \label{fig:schema-fillNil}

  \scalebox{\figscale}{\tikzfig{schemas/fillLeaf}}
  \caption{Memory behavior of \texttt{fillLeaf ⩴ t → UDest [t] ⊸ ()}}
  \label{fig:schema-fillLeaf}
\end{figure}

\section{Compact regions: a safe space for implementing DPS Haskell}\label{sec:implementation}

Having incomplete structures in the memory inherently introduces a lot of tension with both the garbage collector and the compiler. Indeed, the garbage collector of GHC assumes that every heap object it traverses is well-formed, whereas UAmpar structures are absolutely ill-formed: they contain uninitialized pointers, which the GC should absolutely not follow. Also, the compiler and GC can make some optimizations because they assume that every object is immutable, while DPS programming breaks that guarantee by mutating constructors after they have been allocated (albeit only one update can happen). Consequently, we looked for an alternative memory management scheme, mostly independent from the garbage collector, that would let us implement the DPS Haskell API without having to change the garbage collector internals or the memory representation used by GHC.

\subsection{Compact regions}\label{ssec:impl-compact-regions}

\emph{Compact regions} from~\citet{yang_efficient_2015} are special memory \emph{arenas} for Haskell. A compact region represents a memory area in the Haskell heap that is almost fully independent from the GC and the rest of the garbage-collected heap. For the GC, each compact region is seen as a single heap object with a single lifetime. The GC can efficiently check whether there is at least one pointer in the garbage-collected heap that points into the region, and while this is the case, the region is kept alive. When this condition is no longer matched, the whole region is discarded. The result is that the GC won't traverse any node from the region: it is treated as one opaque block (even though it is actually implemented as a chain of blocks of the same size, this doesn't change the principle). Also, compact regions are immobile in memory; the GC won't move them, so a destination to a hole in the compact region can just be implemented as a raw pointer (type \mintinline{haskellc}/Addr#/ in Haskell): \mintinline{haskellc}/data UDest r t = ¤UDest Addr#/, as we have the guarantee that the UAmpar containing the pointed hole won't move.

By using compact regions to implement DPS programming, we completely elude the concerns of tension between the garbage collector and UAmpar structures that we just mentioned above. Conversely, our destination-passing API also provides a new and powerful way to use compact regions.

In exchange for this implementation decision, we get two extra restrictions. First, every structure in a region must be in a fully-evaluated form. This is contrasting with the usual Haskell evaluation scheme, where everything is lazy by default. Consequently, as regions are strict, any heap object that is copied to a region is first forced into normal form (i.e. it is being fully evaluated). This might not always be a win, sometimes laziness is preferable for better performance.

Secondly, data in a region cannot contain pointers to the garbage-collected heap, or pointers to other regions: it must be self-contained. This forces us to slightly modify the API, to add a phantom type parameter \mintinline{haskellc}/r/ that tags each object with the identifier of the region it belongs to so that our API remains memory-safe, without runtime checks. There are two related consequences: first, when a value from the garbage-collected heap is used as an argument to \mintinline{haskellc}/fillLeaf/, it has to be fully evaluated and copied into the region instead of making just a pointer update; and secondly, \mintinline{haskellc}/fillComp/ can only plug together two \mintinline{haskellc}/UAmpar/s that come from the same region.

A typeclass \mintinline{haskellc}/Region r/ is also needed to carry around the details about a region that are required for the implementation. This typeclass has a single method \mintinline{haskellc}/reflect/, not available to the user, that returns the \mintinline{haskellc}/RegionInfo/ structure associated to identifier \mintinline{haskellc}/r/.

The \mintinline{haskellc}/inRegion/ function is the new addition to the modified API presented in \cref{table:destination-api-regions}. It receives an expression of arbitrary type in which \mintinline{haskellc}/r/ must be a free type variable. Internally, when used, it spawns a new compact region and a rigid type variable \mintinline[escapeinside=°°]{haskellc}/°\muline{r}°/ (behaving like a fresh, concrete, opaque type) and uses the \mintinline{text}/reflection/ library to provide an instance of \mintinline[escapeinside=°°]{haskellc}/Region °\muline{r}°/ on-the-fly that links \mintinline[escapeinside=°°]{haskellc}/°\muline{r}°/ and the \mintinline{haskellc}/RegionInfo/ for the new region (through the \mintinline{haskellc}/reflect/ function), and then substitute \mintinline{haskellc}/r/ with \mintinline[escapeinside=°°]{haskellc}/°\muline{r}°/ when evaluating the body of \mintinline{haskellc}/inRegion/, so that any call to \mintinline{haskellc}/reflect @r/ becomes \mintinline[escapeinside=°°]{haskellc}/reflect @°\muline{r}°/ and returns the right \mintinline{haskellc}/RegionInfo/. Actually, \mintinline{haskellc}/inRegion/ is a form of scope function (although this time, it isn't linear).

\begin{listing}[t]
\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
data Token
dup ⩴ Token ⊸ (Token, Token)
drop ⩴ Token ⊸ ()
withToken ⩴ ∀ t. (Token ⊸ Ur t) ⊸ Ur t

type Region r ⩴ Constraint
°\mnew{inRegion ⩴ ∀ t. (∀ r. Region r ⇒ t) ⊸ t}°

data UAmpar r s t
newUAmpar ⩴ ∀ r s. Region r ⇒ Token ⊸ UAmpar s (UDest r s)
tokenBesides ⩴ ∀ r s t. Region r ⇒ UAmpar r s t ⊸ (UAmpar r s t, Token)
toUAmpar ⩴ ∀ r s. Region r ⇒ Token ⊸ s → UAmpar r s ()
fromUAmpar  ⩴ ∀ r s t. Region r ⇒ UAmpar r s (Ur t) ⊸ Ur (s, t)
fromUAmpar' ⩴ ∀ r s. Region r ⇒ UAmpar r s () ⊸ Ur s
updWith ⩴ ∀ r s t u. Region r ⇒ UAmpar r s t ⊸ (t ⊸ u) ⊸ UAmpar r s u

data UDest r t
type family UDestsOf lCtor r t  -- returns dests associated to fields of constructor
fill ⩴ ∀ lCtor r t. Region r ⇒ UDest r t ⊸ UDestsOf lCtor r t
fillComp ⩴ ∀ r s t. Region r ⇒ UAmpar r s t ⊸ UDest r s ⊸ t
fillLeaf ⩴ ∀ r t. Region r ⇒ t → UDest r t ⊸ ()
\end{minted}
\caption{DPS Haskell API for compact regions}
\label{table:destination-api-regions}
\end{listing}

\subsection{Practical DPS programming in compact regions}\label{ssec:parser-sexpr}

We will now study an example, related to implementation concerns, for which DPS programming really shines (and for which compact regions are a really nice fit).

In client-server applications, the following pattern is very frequent: the server receives a request from a client with a serialized payload, the server then deserializes the payload, runs some code, and responds to the request. Most often, the deserialized payload is kept alive for the entirety of the request handling. In a garbage collected language, there's a real cost to this: the garbage collector (GC) will traverse the deserialized payload again and again, although we know that all its internal pointers are alive for the duration of the request.

Instead, we'd rather consider the deserialized payload as a single heap object, which doesn't need to be traversed, and can be freed as a block. Compact regions are, in fact, a very suitable tool for this job, as the GC never follows pointers into a compact region and consider each region as a having the same lifetime for all its contents.

If we use compact regions as-is, with the API provided with GHC, we would first deserialize the payload normally, in the GC heap, then copy it into a compact region and then only keep a reference to the copy. This way, internal pointers of the region copy will never be followed by the GC, and that copy will be collected as a whole later on, whereas the original one in the GC heap will be collected immediately.

However, we are still allocating two copies of the deserialized payload. This is wasteful, it would be much better to allocate directly in the region. Fortunately, with our implementation of DPS Haskell with compact regions, we now have a much better way to allocate and build structures directly into these compact regions!

Let's see how using destinations and compact regions for a parser of S-expressions (representing our request payload) can lead to greater performance. S-expressions are parenthesized lists whose elements are separated by spaces. These elements can be of several types: int, string, symbol (a textual token with no quotes around it), or a list of other S-expressions.

Parsing an S-expression can be done naively with mutually recursive functions:
\begin{itemize}
  \item \mintinline{haskellc}/parseSExpr/ scans the next character, and either dispatches to \mintinline{haskellc}/parseSList/ if it encounters an opening parenthesis, or to \mintinline{haskellc}/parseSString/ if it encounters an opening quote, or eventually parses the string into a number or symbol;
  \item \mintinline{haskellc}/parseSList/ calls \mintinline{haskellc}/parseSExpr/ to parse the next token, and then calls itself again until reaching a closing parenthesis, accumulating the parsed elements along the way.
\end{itemize}

Only the implementation of \mintinline{haskellc}/parseSList/ will be presented here as it is enough for our purpose, but the full implementation of both the naive and destination-based versions of the whole parser can be found in \texttt{src/Compact/Pure/SExpr.hs} of~\cite{linear_dest}.

The implementation presented in \cref{table:impl-parser-naive} is quite standard: the accumulator \mintinline{haskellc}/acc/ collects the nodes that are returned by \mintinline{haskellc}/parseSExpr/ in the reverse order (because it's the natural building order for a linked list without destinations). When the end of the list is reached (line 5), the accumulator is reversed, wrapped in the \mintinline{haskellc}/¤SList/ constructor, and returned. We also store, in the parsed result, the index \mintinline{haskellc}/i/ corresponding to the end position of the structure in the input string.

% \begin{listing}[t]
% \figtextsize
% \begin{minted}[linenos,escapeinside=°°]{haskellc}
% parseSExpr ⩴ ByteString → Int → Either Error SExpr
% parseSExpr bs i = case bs !? i of
%   ¤Nothing → ¤Left (¤UnexpectedEOFSExpr i)
%   ¤Just x → case x of
%     ')' → ¤Left (¤UnexpectedClosingParen i)
%     '(' → parseSList bs (i + 1) []
%     '"' → parseSString bs (i + 1) ¤False []
%     _ → let tok = extractNextToken bs i -- take chars until delimiter/space
%          in if null tok then parseSExpr bs (i + 1) else case parseInt tok of
%               ¤Just int → ¤Right (¤SInteger (i + length tok - 1) int)
%               ¤Nothing → ¤Right (¤SSymbol (i + length tok - 1) (toString tok))
% parseSList ⩴ ByteString → Int → [SExpr] → Either Error SExpr
% parseSList bs i acc = case bs !? i of
%   ¤Nothing → ¤Left (¤UnexpectedEOFSList i)
%   ¤Just x → if
%     | x == ')' → ¤Right (¤SList i (reverse acc))
%     | isSpace x → parseSList bs (i + 1) acc
%     | otherwise → case parseSExpr bs i of
%         ¤Left err → ¤Left err
%         ¤Right child → parseSList bs (endPos child + 1) (child : acc)
% parseSString ⩴ ByteString → Int → Bool → [Char] → Either Error SExpr
% parseSString bs i escape acc = case bs !? i of
%   ¤Nothing → ¤Left (¤UnexpectedEOFSString i)
%   ¤Just x → case x of
%     '"'  | not escape → ¤Right (SString i (reverse acc))
%     '\\' | not escape → parseSString bs (i + 1) ¤True acc
%     'n'  | escape → parseSString bs (i + 1) ¤False ('\n' : acc)
%     _ → parseSString bs (i + 1) ¤False (x : acc)
% \end{minted}
% \caption{Implementation of the S-expression parser without destinations}
% \label{table:impl-sexpr-parser-without-dest}
% \end{listing}

% \begin{listing}[t]
% \figtextsize
% \begin{minted}[linenos,escapeinside=°°]{haskellc}
% parseSExprDps ⩴ ByteString → Int → UDest SExpr ⊸ Either Error Int
% parseSExprDps bs i d = case bs !? i of
%   ¤Nothing → °\mnew{fillLeaf defaultSExpr d}° ; ¤Left (¤UnexpectedEOFSExpr i)
%   ¤Just x → case x of
%     ')' → °\mnew{fillLeaf defaultSExpr d}° ; ¤Left (¤UnexpectedClosingParen i)
%     '(' → parseSListDPS bs (i + 1) °\mnew{(fill @'SList d)}°
%     '"' → parseSStringDPS bs (i + 1) ¤False °\mnew{(fill @'SString d)}°
%     _ → let tok = extractNextToken bs i -- take chars until delimiter/space
%         in if null tok then parseSExprDps bs (i + 1) d else case parseInt tok of
%               ¤Just int → let °\mnew{!dint = fill @'SInteger d}°
%                           in °\mnew{fillLeaf int dint}° ; ¤Right (i + length tok - 1)
%               _ → let °\mnew{!dsym = fill @'SSymbol d}°
%                    in °\mnew{fillLeaf (toString tok) dsym}° ; ¤Right (i + length tok - 1)
% parseSListDPS ⩴ ByteString → Int → UDest [SExpr] ⊸ Either Error Int
% parseSListDPS bs i d = case bs !? i of
%   ¤Nothing → °\mnew{fill @'[] d}° ; ¤Left (¤UnexpectedEOFSList i)
%   ¤Just x → if
%     | x == ')' → °\mnew{fill @'[] d}° ; ¤Right i
%     | isSpace x → parseSListDPS bs (i + 1) d
%     | otherwise → let !(dh, dt) = °\mnew{fill @'(:) d}°
%                    in case parseSExprDps bs i °\mnew{dh}° of
%                         ¤Left err → fill @'[] dt ; ¤Left err
%                         ¤Right endPos → parseSListDPS bs (endPos + 1) °\mnew{dt}°
% parseSStringDPS ⩴ ByteString → Int → Bool → UDest [Char] ⊸ Either Error Int
% parseSStringDPS bs i escape d = case bs !? i of
%   ¤Nothing → °\mnew{fill @'[] d}° ; ¤Left (¤UnexpectedEOFSString i)
%   ¤Just x → case x of
%     '"'  | not escape → °\mnew{fill @'[] d}° ; ¤Right i
%     '\\' | not escape → parseSStringDPS bs (i + 1) ¤True d
%     'n'  | escape → let °\mnew{!(dh, dt) = fill @'(:) d}°
%                      in °\mnew{fillLeaf '\textbackslash{}n' dh}° ; parseSStringDPS bs (i + 1) ¤False °\mnew{dt}°
%     _ → let °\mnew{!(dh, dt) = fill @'(:) d}°
%          in °\mnew{fillLeaf x dh}° ; parseSStringDPS bs (i + 1) ¤False °\mnew{dt}°
% \end{minted}
% \caption{Implementation of the S-expression parser with destinations}
% \label{table:impl-sexpr-parser-with-dest}
% \end{listing}

\begin{listing}[t]
\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
parseSList ⩴ ByteString → Int → [SExpr] → Either Error SExpr
parseSList bs i acc = case bs !? i of
  ¤Nothing → ¤Left (¤UnexpectedEOFSList i)
  ¤Just x → if
    | x == ')' → ¤Right (¤SList i (reverse acc))
    | isSpace x → parseSList bs (i + 1) acc
    | otherwise → case parseSExpr bs i of
        ¤Left err → ¤Left err
        ¤Right child → parseSList bs (endPos child + 1) (child : acc)
\end{minted}
\caption{Implementation of the S-expression parser in Haskell (no destinations)}
\label{table:impl-parser-naive}
\end{listing}

\begin{listing}[b]
\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
parseSListDPS ⩴ ByteString → Int → UDest [SExpr] ⊸ Either Error Int
parseSListDPS bs i d = case bs !? i of
  ¤Nothing → °\mnew{d &fill @'[]}° ; ¤Left (¤UnexpectedEOFSList i)
  ¤Just x → if
    | x == ')' → °\mnew{d &fill @'[]}° ; ¤Right i
    | isSpace x → parseSListDPS bs (i + 1) d
    | otherwise →
        case °\mnew{d &fill @'(:)}° of
          (dh, dt) → case parseSExprDps bs i °\mnew{dh}° of
              ¤Left err → dt &fill @'[] ; ¤Left err
              ¤Right endPos → parseSListDPS bs (endPos + 1) °\mnew{dt}°
\end{minted}
\caption{Implementation of the S-expression parser in DPS Haskell}
\label{table:impl-parser-dps}
\end{listing}

We will see that destinations can bring very significative performance gains with only very little stylistic changes in the code. Accumulators of tail-recursive functions just have to be changed into destinations. Instead of writing elements into a list that will be reversed at the end as we did before, the program in the destination style will directly write the elements into their final location and in the right order!

Code for \mintinline{haskellc}/parseSListDPS/ is presented in \cref{table:impl-parser-dps}. Let's see what changed compared to the naive implementation:

\begin{itemize}
  \item even for error cases, we are forced to consume the destination that we receive as an argument (to stay linear), hence we write some sensible default data to it (see line 3);
  \item the \mintinline{haskellc}/SExpr/ value resulting from \mintinline{haskellc}/parseSExprDps/ is not collected by \mintinline{haskellc}/parseSListDPS/ but instead written directly into its final location by \mintinline{haskellc}/parseSExprDps/ through the passing and filling of destination \mintinline{haskellc}/dh/ (see line 9);
  \item adding an element of type \mintinline{haskellc}/SExpr/ to the accumulator \mintinline{haskellc}/[SExpr]/ is replaced with writing a new cons cell with \mintinline{haskellc}/fill @'(:)/ into the hole represented by the \mintinline{haskellc}/UDest [SExpr]/, writing an element to the new \emph{head} destination, and then doing a recursive call with the new \emph{tail} destination passed as an argument (which has type \mintinline{haskellc}/UDest [SExpr]/ again);
  \item instead of reversing and returning the accumulator at the end of the processing, it is enough to complete the list by writing a nil element to the tail destination (with \mintinline{haskellc}/fill @'[]/, see line 5), as the list has been built in a top-down approach;
  \item ultimately, DPS functions just return the offset of the next character to read instead of returning a parsed value (as the parsed value is written directly into the destination received as a parameter).
\end{itemize}

Thanks to that new implementation which is barely longer (in terms of lines of code) than the naive one, the program runs almost twice as fast, mostly because garbage-collection time goes to almost zero. The detailed benchmark is available in \cref{sec:benchmark}.

We see here that compact regions and destination-passing style are quite symbiotic; compact regions makes the DPS Haskell API easy to implement, and destination-passing style makes compact regions more efficient and flexible to use.

\subsection{Memory representation of \texttt{UAmpar}s in Compact regions}\label{ssec:repr-ampar}

As we detailed in \cref{ssec:runtime-values,ssec:dpshaskell-dlist}, we want \mintinline{haskellc}/UAmpar r s t/ to contains a value of type \mintinline{haskellc}/s/ and one of type \mintinline{haskellc}/t/, and let the value of type \mintinline{haskellc}/s/ free when the one of type \mintinline{haskellc}/t/ has been fully consumed (or linearly transformed into \mintinline{haskellc}/Ur t'/).% So the most straightforward implementation for \mintinline{haskellc}/UAmpar/ would be a pair \mintinline{haskellc}/(s, t)/, where \mintinline{haskellc}/s/ in the pair is only partially complete.

We also know that \mintinline{haskellc}/newUAmpar/ returns an \mintinline{haskellc}/UAmpar r s (UDest s)/: there is nothing more here than an empty memory cell that will host the root of the future structure of type \mintinline{haskellc}/s/, which the associated destination of type \mintinline{haskellc}/UDest s/ points to, as presented in \cref{fig:schema-alloc}. As we said earlier, whatever goes in the destination is exactly what will be retrieved in the \mintinline{haskellc}/s/ side.

The naive and most direct idea is to represent \mintinline{haskellc}/UAmpar r s t/ in memory as a pair-like constructor with one field of type \mintinline{haskellc}/s/ and one of type \mintinline{haskellc}/t/, with the first field being there to host the root of the structure being built (as in \cref{fig:schema-alloc}). However, the root hole, that will later host the structure of type \mintinline{haskellc}/s/, might contain garbage data, so the GC must be prevented to read it and follow uninitialized pointers (otherwise we risk a segmentation fault). In our compact region implementation, we can prevent this issue by having the root hole live inside the compact region (so that the GC doesn't look into it). But we also want the \mintinline{haskellc}/¤UAmpar/ constructor to live in the garbage-collected heap so that it can sometimes be optimized away by the compiler, and always deallocated as soon as possible. These two requirements are, apparently, incompatible, as the root hole corresponds to the first field of the \mintinline{haskellc}/¤UAmpar/ constructor in this representation.

Given that UAmpars hold unrestricted data, one potential workaround is to represent \mintinline{haskellc}/UAmpar r s t/ in memory as a constructor with one field of type \mintinline{haskellc}/Ur s/ and one of type \mintinline{haskellc}/t/ (instead of one of type \mintinline{haskellc}/s/ and one of type \mintinline{haskellc}/t/ previously). It means we need an intermediary \mintinline{haskellc}/¤Ur/ constructor between the UAmpar wrapper and the structure of type \mintinline{haskellc}/s/ being built, and we can decide to allocate this \mintinline{haskellc}/¤Ur/ constructor inside the region, with its field of type \mintinline{haskellc}/s/, also inside the region, serving as the root hole. With this approach, illustrated in \cref{fig:schema-alloc-region-notchosen}, we have a root hole that won't be read by the GC, while the \mintinline{haskellc}/UAmpar/ wrapper can live in the GC heap without issues and be discarded as soon as possible, as we wanted! We also get a very efficient implementation of \mintinline{haskellc}/fromUAmpar'/, as the first field of a completed UAmpar is directly what \mintinline{haskellc}/fromUAmpar'/ should return (it doesn't help for \mintinline{haskellc}/fromUAmpar/ though). The downside is that every UAmpar will now allocate a few words in the region (to host the \mintinline{haskellc}/¤Ur/ constructor\footnote{One might ask why \mintinline{haskellc}/Ur/ can't be a zero-cost \mintinline{haskellc}/newtype/ wrapper. First, if \mintinline{haskellc}/Ur/ were a \mintinline{haskellc}/newtype/, it wouldn't work for what we are trying to achieve here, as we are relying on the fact that it creates an indirection in memory. Secondly, and more importantly, there are semantics motivations, detailed in~\cite{spiwack_ur_2024}.}) that won't be collected by the GC for a long time even if the parent \mintinline{haskellc}/UAmpar/ is collected. In particular, this makes \mintinline{haskellc}/toUAmpar/ quite inefficient memory-wise, as it will have to allocate a \mintinline{haskellc}/Ur/ wrapper in the region that is useless for already complete structures (because already complete structures don't need to have a root hole living in the compact region, as they don't have holes at all).

\begin{figure}
\centering
\scalebox{\figscale}{\tikzfig{schemas/alloc-region-notchosen}}
\caption{Representation of \texttt{UAmpar r} using \texttt{Ur} (not chosen)}\label{fig:schema-alloc-region-notchosen}
\end{figure}

Another option, that we decided to go for in our real implementation, is to allocate an object inside the compact region to host the root hole of the structure \emph{only} for truly incomplete structures created with \mintinline{haskellc}/newUAmpar/. For already complete structures that are turned into an \mintinline{haskellc}/UAmpar/ with \mintinline{haskellc}/toUAmpar/, we skip this allocation; but we preserve the same underlying types for the fields of the UAmpar in both cases (\mintinline{haskellc}/newUAmpar/ and \mintinline{haskellc}/toUAmpar/). This is made possible by the use of the special indirection object (\mintinline{haskellc}/stg_IND/ in GHC codebase) in the UAmpar returned by \mintinline{haskellc}/newUAmpar/. A \mintinline{haskellc}/stg_IND/ object is a kind of one-field constructor that is considered to have type \mintinline{haskellc}/s/ although its field also has type \mintinline{haskellc}/s/. We illustrate this in \cref{fig:schema-alloc-region}:

\begin{figure}
\centering
\scalebox{\figscale}{\tikzfig{schemas/alloc-region}}
\caption{Representation of \texttt{UAmpar r} using indirections}\label{fig:schema-alloc-region}
\end{figure}

\begin{itemize}
  \item in the pair-like structure returned by \mintinline{haskellc}/newUAmpar/, the \mintinline{haskellc}/s/ side points to an indirection object, which is allocated in the region, and serves as the root hole for the incomplete structure (because it is in the compact region, the GC won't follow garbage pointers that it might contain at the moment);
  \item in the pair-like structure returned by \mintinline{haskellc}/toUAmpar/, the \mintinline{haskellc}/s/ side directly points to the object of type \mintinline{haskellc}/s/ that has been copied to the region, without indirection.
\end{itemize}
Compared to the previous solution, this one is more efficient when using \mintinline{haskellc}/toUAmpar/ (as no long-lived garbage is produced), but does no longer give efficiency benefits for \mintinline{haskellc}/fromUAmpar'/ (as we no longer produce an \mintinline{haskellc}/Ur/ object). This is the solution we with with in the original artifact~\cite{linear_dest}.

In~\cite{linear_dest}, we thus have the following private definitions\footnote{In the sense that they are opaque for the consumer of the API.} for our API types:
\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
data Token = ¤Token  -- same representation as ()
data UAmpar r s t = ¤UAmpar s t  -- same representation as (s, t),
                                        -- with sometimes an indirection on the s field
data UDest r t = ¤UDest Addr#  -- boxed Addr#, same representation as Ptr t
\end{minted}
}
\end{unbreakable}

\subsection{Deriving \texttt{fill} for all constructors with \texttt{Generics}}\label{ssec:impl-generics}

In \destcalculus, it was quite easy to have a dedicated destination-filling function for each corresponding data constructor. Indeed, we only had to account for a finite number of them (one for unit, two for sums, one for pair, and one for exponential; with the one for function being quite optional).

In Haskell however, we want to be able to do destination passing with arbitrary data types, even user-defined ones, so it wouldn't be feasible to implement, manually, one filling function per possible data constructor. So instead, we have to generalize all filling functions into a polymorphic \mintinline{haskellc}/fill/ function that is able to work for any data constructor.

To this extent, we create a typeclass \mintinline{haskellc}/Fill lCtor t/, and we define \mintinline{haskellc}/fill/ as the only method of this class. Having such a typeclass lets us tie the behaviour of \mintinline{haskellc}/fill/ to the type parameter \mintinline{haskellc}/lCtor/. The idea is that \mintinline{haskellc}/lCtor/ represents the type-level name of the data constructor we want to use \mintinline{haskellc}/fill/ with. The syntax at call site is of the form \mintinline{haskellc}/fill @'Ctor/: we lift the constructor \mintinline{haskellc}/¤Ctor/ into a type-level name with the \mintinline{haskellc}/'/ operator, then make a type application with \mintinline{haskellc}/@/. We recall that the \mintinline{haskellc}/fill/ function should plug a new hollow constructor of the specified variant into the hole of a structure whose corresponding destination is received as an argument.

Let's now see how we can use the type-level name of the constructor \mintinline{haskellc}/lCtor/ and its base type \mintinline{haskellc}/t/ to obtain all the information we need to write the concrete implementation of the corresponding \mintinline{haskellc}/fill/ function. The first thing we need to know is the shape of the constructor and more precisely the number and types of its fields, as every data constructor for a linked data type can be seen as an $n$-ary product constructor. So we will leverage \mintinline{haskellc}/GHC.Generics/ to find the required information.

\mintinline{haskellc}/GHC.Generics/ is a built-in Haskell library that provides compile-time inspection of a type metadata through the \mintinline{haskellc}/Generic/ typeclass: list of constructors, their fields, memory representation, etc. And that typeclass can be derived automatically for (almost) any type! Here's, for example, the \mintinline{haskellc}/Generic/ representation of \mintinline{haskellc}/Maybe t/:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
repl> :k! Rep (Maybe a) () -- display the Generic representation of Maybe a
M1 D (MetaData "Maybe" "GHC.Maybe" "base" False) (
  M1 C (MetaCons "¤Nothing" PrefixI False) U1
  :+: M1 C (MetaCons "¤Just" PrefixI False) (M1 S ¤[...¤] (K1 R t)))
\end{minted}
}
\end{unbreakable}

We see that there are two different constructors (indicated by \mintinline{haskellc}/M1 C .../ lines): \mintinline{haskellc}/¤Nothing/ has zero fields (indicated by \mintinline{haskellc}/U1/) and \mintinline{haskellc}/¤Just/ has one field of type \mintinline{haskellc}/t/ (indicated by \mintinline{haskellc}/K1 R t/).

As illustrated with the example above, the generic representation of a type contains most of what we want to know about its constructors. So, with type-level programming techniques\footnote{see \texttt{src/Compact/Pure/Internal.hs:418} in~\cite{linear_dest}}, in particular with type families, we can extract the parts of this generic representation that are relevant to us (as type-level expressions), and refer to them in the head of our typeclass' instances, to have several specialized implementations of \mintinline{haskellc}/fill/. For instance, in~\cite{linear_dest}, we have one specialized implementation of \mintinline{haskellc}/fill/ for every possible number of fields in \mintinline{haskellc}/lCtor/, from 0 to 7\footnote{We could go higher, 7 is just an arbitrary limit for the number of fields that is rather common in standard Haskell libraries.}. The main reason is that \mintinline{haskellc}/fill/ should return a tuple containing as many new destinations as there are fields in the the chosen constructor, and at the moment, there is no way to abstract over the length of a tuple in Haskell, so we need to harcode each possible case.

The resolution of the \mintinline{haskellc}/UDestsOf lCtor t/ type family into a concrete tuple type of destinations is made similarly, by type-level expressions based on the generic representation of the base type \mintinline{haskellc}/t/ which \mintinline{haskellc}/lCtor/ belongs to.

At this point we still miss a major ingredient though: the internal Haskell machinery to allocate the proper hollow constructor object in the compact region.

\subsection{Changes to GHC internals and its RTS}\label{ssec:impl-ghc}

%STOPPED HERE%

We will see here how to allocate a hollow constructor, that is, a hollow heap object for a given constructor, but let's first take a detour to give more context about the internals of the compiler.

Haskell's runtime system (RTS) is written in a mix of C and C-\,-~\cite{ramsey_cminus_2005}. The RTS has many roles, among which managing threads, organizing garbage collection or managing compact regions. It also defines various primitive operations, named \emph{external primops}, that expose the RTS capabilities as normal functions that can be used in Haskell code. Despite all its responsibilities, the RTS is surprisingly not responsible for the allocation of normal constructors (built in the garbage-collected heap). One reason is that it doesn't have all the information needed to build a constructor heap object, namely, the info table associated to the constructor.

In Haskell, a heap object is a piece of data in memory, that corresponds to a given instance of a data constructor, or a (partially-applied) function. The info table is what defines both the layout and behavior of a heap object. All heap objects representing a same data constructor (let's say \mintinline{haskellc}/¤Just/) have the same info table, which acts as the identity card for this constructor, even when the associated types are different (e.g. \mintinline{haskellc}/Just x ⩴ Maybe Int/ and \mintinline{haskellc}/Just y ⩴ Maybe Bool/ share the same info table). In the compilation process, during code generation, all instances of the \mintinline{haskellc}/Just/ constructor will hold the label \mintinline{text}/Just_con_info/, that is later resolved by the linker into an actual pointer to the shared info table, that all corresponding heap objects for this constructor will carry.

On the other hand, the RTS is a static piece of code that is compiled once when GHC is built, and is then linked into every executable built by GHC. Only when a program is launched does the RTS run (to oversee the program execution), so the RTS has no direct way to access the information that had been emitted during the compilation of the program. In particular, it has no way to inspect the info table labels have long been replaced by actual pointers (which are indistinguishable in memory from any other data). But on the other hand, the RTS is the one which knows how to allocate space inside a compact region for our new hollow constructor.

As a result, we need a way to pass information, or more precisely, the info table pointer, from compile time to the runtime. Consequently, we manage the creation of a new hollow constructor in a compact region through two new primitives:

\begin{itemize}
\item one \emph{internal primop} which resolves, at compile time, a data constructor into the (static) value representing its info table pointer.
\item one \emph{external primop}, in the RTS, which allocates space inside a compact region for a hollow constructor, and which sets the info table pointer of the hollow constructor to a value received as an argument, that the developer can provide using the internal primop.
\end{itemize}

All the alterations to GHC that will be showed here are available in full form in~\cite{custom_ghc}.

\paragraph{Internal primop: obtain the info table pointer of a constructor as a static value}

Internal primops are macros that are transformed at compile time into C-\,- code. So, for our new internal primop, in charge of obtaining the info table label (later resolved into a pointer) of a constructor, we must indicate our choice of constructor as a type-level argument so that it can be inspected at compile-time (as, of course, term-level values cannot be inspected easily at compile time). For that, we use the \emph{tick} operator $\ottstype{\mathrm{'}}$ in front of the constructor name, which promotes an object from the term-level namespace (in which data constructors live) to type-level, to be used where a type would normally be expected\footnote{We refer the reader to \cite{yorgey_haskellprom_2012} for more information about promoting term-level objects to type-level ones in Haskell.}.

Now, we need the internal primop to inspect the type parameter representing our choice of constructor. Usually, in Haskell, we use a parameter of type \mintinline{haskellc}/Proxy t/ (the unit type with a phantom type parameter \mintinline{haskellc}/t/) as a carrier for a type-level input that we want to pass to a function. Unfortunately, primops are special functions, and due to a quirk of the compiler, they no longer have access to the type of their arguments at the stage in which they are resolved, so designing our primop with a \mintinline{haskellc}/Proxy lCtor/ parameter wouldn't let it read the supplied \mintinline{haskellc}/lCtor/. However primops can---surprisingly---access their return type. Consequently, we design a custom return type \mintinline{haskellc}/InfoPtrPlaceholder#/ for our primop that carries the output value of the primop (that is to say, an info table pointer); but we also add a phantom type parameter \mintinline{haskellc}/lCtor/ to \mintinline{haskellc}/InfoPtrPlaceholder#/ that has no relation to the underlying value, but just serves as a carrier for our type-level input representing the chosen constructor!

The concrete implementation is presented brielfy in \cref{table:impl-reifyInfoPtr} (we omit the roughly one hundred lines of boilerplate code scattered across other parts of the GHC codebase that handle primop registration, interface declaration, etc.). This implementation goes a bit further than what we just described above, as \mintinline{haskellc}/reifyInfoPtr#/ can obtain the info table pointer of data constructors (as announced), but also the info table pointer of special compiler objects like the indirection object we used in \cref{ssec:repr-ampar}. The primop pattern-matches on the type \mintinline{haskellc}/resTy/ of its return value (which should be of the form \mintinline{haskellc}/InfoPtrPlaceholder# lCtorOrSym/). In the case it reads a lifted data constructor (which is the most common case), it resolves the primop call into the label \mintinline{text}/<ctor>_con_info/ which corresponds to the info table pointer of that constructor. In the case \mintinline{haskellc}/lCtorOrSym/ is a symbol (that is, a type-level string literal), it resolves the primop call into the label \mintinline{text}/stg_<sym>/ (e.g. to allocate a \mintinline{haskellc}/stg_IND/ object, as we just mentionned). The returned \mintinline{haskellc}/InfoPtrPlaceholder# lCtorOrSym/ can later be converted back to an \mintinline{haskellc}/Addr#/ using the \mintinline{haskellc}/unsafeCoerceAddr/ function, which despite the name, is not a very unsafe coercion as it can only be used on two types sharing the same underlying representation.

\begin{listing}
\figtextsize
\begin{minted}[linenos]{haskellc}
case primop of
  [...]
  ¤ReifyInfoPtrOp → \_ →  -- we don't care about the function argument (# #)
    opIntoRegsTy $ \[res] resTy → emitAssign (¤CmmLocal res) $ case resTy of
      -- when the phantom type parameter is a lifted data constructor, extracts it as a DataCon
      ¤TyConApp _infoPtrPlaceholderTyCon [_typeParamKind, ¤TyConApp tyCon _]
        | ¤Just dataCon ← isPromotedDataCon_maybe tyCon →
          ¤CmmLit (¤CmmLabel (
            mkConInfoTableLabel (dataConName dataCon) ¤DefinitionSite))
      -- when the phantom type parameter is a Symbol, we extracts the symbol value in 'sym'
      ¤TyConApp _infoPtrPlaceholderTyCon [_typeParamKind, ¤LitTy (¤StrTyLit sym)] →
          ¤CmmLit (¤CmmLabel (
            mkCmmInfoLabel rtsUnitId (fsLit "stg_" `appendFS` sym)))
      _ → [...] -- error when no pattern matches
\end{minted}
\caption{Implementation of \texttt{reifyInfoPtr\#} in \texttt{compiler/GHC/StgToCmm/Prim.hs}}
\label{table:impl-reifyInfoPtr}
\end{listing}

\paragraph{External primop: allocate a hollow constructor in a region}

Our external primop, as part of the RTS, is directly written in C-\,-. Its implementation is presented in \cref{table:impl-compactAddHollow}, under the name \mintinline{c}/stg_compactAddHollowzh/; but in Haskell code we access it through the name \mintinline{haskellc}/compactAddHollow#/. The primop consists mostly in a glorified call to the \mintinline{haskellc}/ALLOCATE/ macro defined in the \mintinline{text}/Compact.cmm/ file, which tries to do a pointer-bumping allocation in the current block of the compact region if there is enough space, and otherwise adds a new block to the region. Then, it takes the info table pointer of the constructor to allocate, received as its second parameter \mintinline{c}/W_ info/ (as it cannot obtain this information itself), and write it to the first word of the heap object using \mintinline{c}/SET_HDR/. Then the pointer to the newly allocated hollow constructor is set as the return value.

\begin{listing}
\figtextsize
\begin{minted}[linenos]{c}
// compactAddHollow#
//   ⩴ Compact# → Addr# → State# RealWorld → (# State# RealWorld, a #)
stg_compactAddHollowzh(P_ compact, W_ info) {
    W_ pp, ptrs, nptrs, size, tag, hp;
    P_ to, p; p = NULL;  // p isn't actually used by ALLOCATE macro
    again: MAYBE_GC(again); STK_CHK_GEN();

    pp = compact + SIZEOF_StgHeader + OFFSET_StgCompactNFData_result;
    ptrs  = TO_W_(%INFO_PTRS(%STD_INFO(info)));
    nptrs  = TO_W_(%INFO_NPTRS(%STD_INFO(info)));
    size = BYTES_TO_WDS(SIZEOF_StgHeader) + ptrs + nptrs;
    
    ALLOCATE(compact, size, p, to, tag);
    P_[pp] = to;
    SET_HDR(to, info, CCS_SYSTEM);
  #if defined(DEBUG)
    ccall verifyCompact(compact);
  #endif
    return (P_[pp]);
}
\end{minted}
\caption{Implementation of \texttt{compactAddHollow\#} in \texttt{rts/Compact.cmm}}
\label{table:impl-compactAddHollow}
\end{listing}

\paragraph{Combining both primops to obtain a new hollow constructor}

It's time to show the two primops in action. Here is how we could allocate, for example, a hollow \mintinline{haskellc}/¤Just/ in a compact region:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos]{haskellc}
hollowJust ⩴ Maybe a = compactAddHollow#
  region
  (unsafeCoerceAddr (reifyInfoPtr# (# #) ⩴ InfoPtrPlaceholder# 'Just ))
\end{minted}
}
\end{unbreakable}

It's important to note that the expression \mintinline{haskellc}/reifyInfoPtr# (# #)/ will be replaced at compile-time by a static value representing the info table pointer of \mintinline{haskellc}/¤Just/; while the call to \mintinline{haskellc}/compactAddHollow#/ behaves as a normal function call, and will only be executed when the program runs.

\paragraph{Built-in type family to go from a lifted constructor to the associated symbol}

The internal primop \mintinline{haskellc}/reifyInfoPtr#/ that we just introduced uses a constructor lifted at type level as an input. So the \mintinline{haskellc}/fill/ function, which uses \mintinline{haskellc}/reifyInfoPtr#/ and \mintinline{haskellc}/compactAddHollow#/ under the hood, should also use a constructor lifted at type level as a way of selecting the hollow constructor that will be added to a structure with holes.

However, \mintinline{haskellc}/fill/ relies on \mintinline{haskellc}/GHC.Generics/ to obtain various informations on the chosen constructor (see \cref{ssec:impl-generics}), and in the \mintinline{haskellc}/Generic/ representation of a type, constructors are identified by their name (as type-level strings), which are distinct type-level entities than the constructors lifted at type level like we used in \mintinline{haskellc}/reifyInfoPtr#/.

Consequently, we need a way to translate a constructor lifted at type level into the type-level string representing the name of this constructor\footnote{We perform the translation in this direction because a type-level string alone does not uniquely identify a constructor. For example, \mintinline{haskellc}/reifyInfoPtr# @"Just"/ is ambiguous and provides the compiler with insufficient information to resolve it to the correct \mintinline{haskellc}/¤Just/ constructor. In contrast, using the promoted constructor, as in \mintinline{haskellc}/reifyInfoPtr# @'Just/, enables proper resolution.}. This is the role of the type family \mintinline{haskellc}/LCtorToSymbol/ that we also added to GHC. It just inspects its (type-level) parameter representing a constructor, fetches its associated \mintinline{haskellc}/DataCon/ structure, and returns a type-level string (kind \mintinline{haskellc}/Symbol/) carrying the constructor name, as presented in \cref{table:impl-LCtorToSymbol}.

\begin{listing}
\figtextsize
\begin{minted}[linenos]{haskellc}
matchFamLCtorToSymbol ⩴ [Type] → Maybe (CoAxiomRule, [Type], Type)
matchFamLCtorToSymbol [kind, ty]
  | ¤TyConApp tyCon _ ← ty, ¤Just dataCon ← isPromotedDataCon_maybe tyCon =
      let symbolLit = (mkStrLitTy . occNameFS . occName . getName $ dataCon)
       in ¤Just (axLCtorToSymbolDef, [kind, ty], symbolLit)
matchFamLCtorToSymbol tys = ¤Nothing

axLCtorToSymbolDef =
  mkBinAxiom "LCtorToSymbolDef" typeLCtorToSymbolTyCon ¤Just
    (\case { ¤TyConApp tyCon _ → isPromotedDataCon_maybe tyCon ; _ → ¤Nothing })
    (\_ dataCon → ¤Just (mkStrLitTy . occNameFS . occName . getName $ dataCon))
\end{minted}
\caption{Implementation of \texttt{LCtorToSymbol} in \texttt{compiler/GHC/Builtin/Types/Literal.hs}}
\label{table:impl-LCtorToSymbol}
\end{listing}

\paragraph{Putting it all together, and further evolutions}

\begin{figure}\centering
  \scalebox{1.5}{\tikzfig{schemas/implem-recap}}
  \caption{Schematic overview of the implementation architecture for \texttt{fill}}
  \label{fig:implem-recap}
\end{figure}

All these elements combined provide the low-level backbone needed to implement our \mintinline{haskellc}/fill/ function. They are summarized in \cref{fig:implem-recap}. The implementations presented above were designed to minimize changes to GHC itself, based on the idea that the fewer modifications required, the more realistic it would be for this feature to work well and be accepted into the main version of GHC, especially since destination-passing remains a relatively niche concern for such an industrial-grade compiler.

However, since this initial experimentation~\cite{custom_ghc}, a pull request has been opened~\cite{bagrel_primitives_2024} to gather community input on the topic, and we now appear to be moving toward a slightly more complete set of primitives than the three previously mentioned. In particular, primitive support for \mintinline{haskellc}/fill/ will likely be included in GHC itself (rather than existing solely in library code, as it does currently). At the time of writing, the interface design is still under discussion, and much of the implementation remains to be done (or be adapted from earlier experiments).

\section{Evaluating the performance of DPS Haskell}\label{sec:benchmark}

It's now time to see if our claims of performance are verified with our prototype destination-passing implementation~\cite{linear_dest}.

\paragraph{Benchmarking methodology}

We will compare programs in both usual functional style and DPS style, and see if we can observe differences in terms of speed but also memory efficiency. But first we must be careful in the way we measure time and space usage, especially since Haskell defaults to a lazy evaluation strategy. In particular we need a way to force Haskell to fully evaluate the results of the implementations that we benchmark.

In all DPS implementations, the program output is stored in a compact region (as required). As noted in \cref{ssec:impl-compact-regions}, compact regions enforce strictness; so the structure is automatically fully evaluated. While this may be suboptimal in some real use-cases, it simplifies benchmarking of these implementations at least.

For naive implementations, we must choose a way to force full evaluation. One option is to keep the result in the GC heap and use \mintinline{haskellc}/Control.DeepSeq.force/ to recursively evaluate each thunk. Alternatively, we can copy the result to a compact region using \mintinline{haskellc}/Data.Compact.compact/, which enforces strictness by default.

Copying into a compact region is only really desirable for a handful of programs, for which the result is held in memory for a long time and used in full (as the unused parts cannot be collected if it is in a compact region). Still, \mintinline{haskellc}/compact/ is sometimes faster than \mintinline{haskellc}/force/. We therefore benchmark naive implementations primarily with \mintinline{haskellc}/force/, but also with \mintinline{haskellc}/compact/ (noted with a \mintinline{haskellc}/.copyCR/ suffix) when this appears beneficial---either because storing the result in a compact region is desirable (e.g., in the parser example), or when it yields significantly better performance.

All charts have a relative scale on the Y axis (for both time and memory). The baseline is the time taken and the peak memory used by the method of reference, which is plotted in navy blue. Thus, the absolute scale differs for each input size, with 1.0 corresponding to the reference method’s value at that size. For memory charts, all series (\emph{peak}, \emph{allocated}, and \emph{copied}) use the peak memory value of the method of reference as a baseline, so that values across all series at a given size are directly comparable.

\paragraph{About memory reporting}

Accurate memory reporting in Haskell is challenging. When benchmarks are run via \mintinline{haskellc}/Tasty.Bench/, each is executed repeatedly until time results fall within a target deviation. \mintinline{haskellc}/Tasty.Bench/ also reports three memory metrics: allocated memory, copied volume during garbage collection, and peak memory. Unfortunately, allocated memory does not include allocations in compact regions, making it unsuitable as a baseline. We therefore use peak memory as a baseline, which approximates how RAM hungry a given implementation is.

However, if we launch a full suite of benchmarks with \mintinline{haskellc}/Tasty.Bench/, then the peak memory value reported can only ever increase across the whole run, and isn't reset when benchmarking a different implementation of the suite. We tried to solve this by running the benchmark of each implementation+size separately (instead of launching a full suite), so that each implementation can have a proper peak memory reporting. Yet even then, \mintinline{haskellc}/Tasty.Bench/ always report at least 69~MB of peak memory use, even if the given case uses far less (as we can deduce from the associated allocated and copied values).

Consequently, in addition to the \mintinline{haskellc}/Tasty/ benchmarks, we ran each implementation+size separately, 10 times, with RTS options \mintinline{text}/+T +s/, to get a more accurate reading of peak memory use. In this benchmarking mode, we observed a minimum of 6~MB of peak memory---accounting for the Haskell runtime and other boilerplate---which is quite better than the 69~MB reported by \mintinline{haskellc}/Tasty.Bench/.

Still, many programs report exactly 6~MB of peak memory, while allocating between 0 and 3~MB (on average 0.5~MB). This shows that the reported peak memory does not accurately reflect the actual memory used by the implementation itself---it consistently overestimates usage by 3 to 6~MB, typically 5.5~MB. Based on this, we decided to subtract 5.5~MB from all peak memory readings equal to or above 10.5~MB to obtain an adjusted peak memory value. We do not show the data points that have a peak reading below 10.5~MB, as the margin of error, up to 2.5~MB, would represent more than half of the adjusted value.

When we don't have a trusted data point for the peak memory value of the reference method, we instead use an arbitrary origin for that input size (that is computed to preserve the shape of the copied and allocated curves). That way, we can still easily compare the other values with one another.

\paragraph{Execution environment}

We launch the benchmark of each implementation at a given size in isolation, either manually or through \mintinline{haskellc}/Tasty.Bench/, with RTS options \mintinline{text}/+T +s/. We do not set any other runtime option; in particuler, we do \emph{not} use multithreading or configure any particular garbage collection option.

All programs are compiled with optimization \mintinline{text}/-O2/, with a version of GHC 9.12 modified to incorpore the changes initially presented in \cite{custom_ghc} (which were initially designed for GHC 9.8).

Benchmarks have been conducted on a laptop under Kubuntu 22.04 with Intel i7-1165G7\,@\,2.80~GHz CPU, 32~GB RAM, with power supply connected.

\subsection{Concatenating lists and difference lists}\label{ssec:benchmark-dlist}

\begin{figure}\centering
  \hspace*{-1.75cm}\includegraphics[width=19cm]{graphics/plot-concat.pdf}
  \caption{Benchmark of iterated concatenations on lists and difference lists}
  \label{fig:bench-concat}
\end{figure}

We compared three implementations for list concatenation.

The first one, \mintinline{haskellc}/concatListRight.force/, uses usual linked lists, and has calls to \mintinline{haskellc}/(++)/ nested to the right, giving the most optimal context for list concatenation: it should run in $\mathcal{O}(n)$ time. We only include it as a point of reference, since it isn't the most common use-case for concatenation in practice. In real-world programs, list concatenation is often nested to the left, for example when appending lines to a log before eventually flushing it to disk. We chose not to include the version with calls to \mintinline{haskellc}/(++)/ nested to the left, as its time and memory consumption grow so rapidly that it renders the rest of the graph unreadable.

For the two other implementations, the calls to \mintinline{haskellc}/concat/ are nested to the left. The implementation \mintinline[escapeinside=°°]{haskellc}/concatDListFunLeft.force/ uses function-backed difference lists, and \mintinline{haskellc}/concatDListDpsLeft/ uses destination-backed ones (from \cref{ssec:dpshaskell-dlist}). They should still run in $\mathcal{O}(n)$ however, thanks to difference list magic.

We see in \cref{fig:bench-concat} that the destination-backed difference lists have a slightly reduced memory consumption and lower runtime (about 10 \%) compared to the two other implementations, for very large datasets only, in exchange of being quite slower and less memory efficient for all the smaller datasets. It's a trend that we will see for the rest of the benchmarks: for very large datasets, garbage collection becomes very costly, and the destination-passing implementations shine not really by being faster to create or process data, but rather by avoiding garbage collection on large data trees that are still live. It's much cheaper to collect garbage (e.g. the \mintinline{haskellc}/Ampar/ or \mintinline{haskellc}/Dest/ wrappers in the GC heap) than to preserve non-garbage.

Compared to the benchmarks initially presented in \cite{bagrel_destination-passing_2024}, it seems that the performance of destination-based difference lists has improved, through more efficient handling of linear programs by the compiler, albeit the difference with naive implementations still isn't substancial enough compared to what we could expect theoretically, especially when other simpler tweaks exist (notably, setting custom GC parameters at program's launch).

\subsection{Breadth-first tree relabeling}\label{par:benchmark-bf-tree-traversal}

\begin{figure}\centering
  \hspace*{-1.75cm}\includegraphics[width=19cm]{graphics/plot-bft.pdf}
  \caption{Benchmark of breadth-first tree relabeling}
  \label{fig:bench-bft}
\end{figure}

For breadth-first tree traversal, we benchmark the destination-passing implementation of \cref{ssec:bf-tree-traversal} against an implementation based on \emph{Phases} applicatives, presented in~\cite{gibbons_phases_2023}---a similarly concise approach to breadth-first tree traversal in a functional setting:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos]{haskellc}
import Control.Monad.State.Lazy (runState, state)

data Phases m a where
  Pure :: a → Phases m a
  Link :: (a → b → c) → m a → Phases m b → Phases m c

instance Functor (Phases m) where ...
instance (Applicative m) ⇒ Applicative (Phases m) ...

now :: (Applicative m) ⇒ m a → Phases m a
now xs = Link (curry fst) xs (Pure ())

later :: (Applicative m) ⇒ Phases m a → Phases m a
later xs = Link (curry snd) (pure ()) xs

runPhases :: (Applicative m) ⇒ Phases m a → m a
runPhases (Pure x) = pure x
runPhases (Link f xs ys) = pure f <*> xs <*> runPhases ys

traverseBFS :: (Applicative m) ⇒ (a → m b) → BinTree a → Phases m (BinTree b)
traverseBFS _ Nil = pure Nil
traverseBFS f (Node x tl tr) =
  pure Node <*> now (f x) <*> later ((traverseBFS f) tl) <*> later ((traverseBFS f) tr)

relabelPh :: BinTree () → (BinTree Int, Int)
relabelPh tree = runState (runPhases $ traverseBFS (\_ → state (\s → (s, s + 1))) tree) 0
\end{minted}
}
\end{unbreakable}

We see in part \cref{fig:bench-bft} that the destination-based tree traversal is about five times more efficient, both time-wise and memory-wise, than the implementation from~\citet{gibbons_phases_2023}. We must admit though that we didn't particularly try to optimize the performance of the \emph{Phases} implementation as much as we did for our DPS framework in general.

This is a good example of a program for which the destination-passing version is almost as natural to write as the usual functional-style one, but performs significantly better even at smaller sizes.

\subsection{Parsing S-expressions}

\begin{figure}\centering
  \hspace*{-1.75cm}\includegraphics[width=19cm]{graphics/plot-parser.pdf}
  \caption{Benchmark of S-expression parser}
  \label{fig:bench-parser}
\end{figure}

In \cref{fig:bench-parser}, we compare the naive and DPS implementations of the S-expression parser presented in \cref{ssec:parser-sexpr}. For this particular program, using compact regions is interesting even without destination-passing considerations, as it will reduce the GC load of the application as long as the data is held in memory (which is quite usual with parsed documents). Thus, we benchmark the naive version twice: once in which the results stays in the GC heap and is evaluated with \mintinline{haskellc}/force/ (its name is suffixed with a star), and once to produce a result that we copy in a compact region (\mintinline{haskellc}/copyCR/).

The DPS implementation starts by being less efficient than the naive versions for small inputs, but gets significantly better than the two other implementations tested as soon as garbage collection kicks in (we see it by the copied memory value rising very fast compared to the allocated and peak memory values, around $2^{16}$).

On the dataset of size $2^{19}$ to $2^{25}$, the DPS implementation uses 35-55\% less time and 10-45\% less memory at its peak than the two naive implementation, despite showing more allocations than \mintinline{haskellc}/parseSExpr.force/. This indicates that most of the data allocated in the GC heap by the DPS version is wrappers such as \mintinline{haskellc}/Ampar/s and \mintinline{haskellc}/Dest/s that just last one generation and thus can be discarded very early by the GC (as intended), without needing to be copied into the next generation, unlike the data nodes allocated by the naive versions that have to be persisted by the GC (by copying them).

It's slightly surprising to see that for this particular example, the \mintinline{haskellc}/parseSExpr.copyCR/ version performs worse than \mintinline{haskellc}/parseSExpr.force/ even for the largest datasets. In a real-world scenario though---where data is processed for some time after having been parsed instead of being discarded immediately---we expect the \mintinline{haskellc}/copyCR/ implementation to get an edge on garbage collection time (and thus, runtime) over the \mintinline{haskellc}/force/ version, but it would probably still be behind the DPS implementation which saves the copy of the result from the GC heap to the compact region.

\subsection{Mapping a function over a list}\label{ssec:benchmark-map}

\begin{figure}\centering
  \hspace*{-1.75cm}\includegraphics[width=19cm]{graphics/plot-map.pdf}
  \caption{Benchmark of map function on list}
  \label{fig:bench-map}
\end{figure}

In a strict functional language such as OCaml, the choice of implementation for the \emph{map} function on lists is crucial as the naive, non tail-recursive one makes the stack grow linearly with the size of the processed list. To alleviate the issue, a strict tail-recursive implementation is possible (\mintinline{haskellc}/mapTRS/), but it requires an extra $\mathcal{O}(n)$ operation at the end of the processing to reverse the accumulator. In Haskell, the issue is of a lesser importance perhaps, as the lazy-by-default evaluation strategy makes the naive \emph{map} (either \mintinline{haskellc}/mapL/ for full lazy behavior, or \mintinline{haskellc}/mapSH/ for strict head but lazy tail) run efficiently in constant space. Still, let see how our DPS versions compare.

With destinations, \emph{map} can be implemented in a tail-recursive fashion too (\mintinline{haskellc}/mapDpsTRS/), as explained in \cref{ssec:map-tr}, without requiring the reverse operation that \mintinline{haskellc}/mapTRS/ needs, as the list is built directly in a top-down approach. Interestingly, thanks to destinations, \emph{map} can alternatively be implemented as a left fold (\mintinline{haskellc}/mapDpsFoldS/), as we also show below! In the following code excerpt, the \mintinline{haskellc}/!/ in front of a variable name denotes a strict binding, so the expression assigned to the variable will be evaluated to weak-head normal form before the body of the \mintinline{haskellc}/let/ is evaluated:
{\figtextsize
\begin{minted}[linenos]{haskellc}
append ⩴ UDest [t] ⊸ t → UDest [t]
d `append` x = case d &fill @'(:) of (dh, dt) → dh &fillLeaf x ; dt

mapDpsTRS _ [] d = d &fill @'[]
mapDpsTRS f (x : xs) d = let !y = f x ; !d' = d `append` y in mapDpsTRS f xs d'

mapDpsFoldS f l dl = fill @'[] (foldl' (\d x → let !y = f x in d `append` y) d l)
\end{minted}
}

\cref{fig:bench-map} presents the results of the different implementations. First, we confirm that the most naive \mintinline{haskellc}/mapL.force/ is performing really well given its simplicity. We also see that the strict functional tail-recursive implementation \mintinline{haskellc}/mapTRS/ is not particularly interesting for a language like Haskell (even in its strict head, lazy tail variant that isn't shown in the chart, but got similar results), as it performs worse than all the other implementations both time and memory-wise.

For \mintinline{haskellc}/mapSH/, the situation is a bit more complex. With the \mintinline{haskellc}/force/ evaluation method, it performs partially better, partially worse than the reference implementation \mintinline{haskellc}/mapL.force/ depending on the input size; but with copy to compact region in guise of deep evaluation, it achieves the smallest possible time for $2^{16}$ and $2^{22}$ sizes, surpassing the destination-based implementations by a few percents, with equally good peak memory use too. We can't really explain why we get so better results with copy to compact region for this particular implementation though; but it deserved to be included for being the top-performer for a few sizes.

Finally, the destination-based implementations \mintinline{haskellc}/mapDpsTRS/ and \mintinline{haskellc}/mapDpsFoldS/ present both very honorable performance, with 20-45\% less time and up to 25\% peak memory saving compared to the reference implementation for most datasests starting from $2^{16}$ size. However, as we said before, the performance of DPS Haskell on small lists is still fairly disappointing. Reducing the overhead of destination-passing should probably be a focus of future refinements of the DPS Haskell implementation.

As a side note, \citet{bour_tmc_2021} showed that in OCaml, the equivalent of \mintinline{haskellc}/mapDpsTRS/ is always more performant time-wise (15-50\% less time) than the equivalent of \mintinline{haskellc}/mapTRS/. However, their destination-passing system is used only at the compiler’s intermediate language level, which perhaps allow for more aggressive optimizations than what we can achieve at the source level.

\section{Conclusion}

Programming with destinations definitely has a place in practical functional programming, as shown by the recent adoption of \emph{Tail Modulo Cons}~\cite{bour_tmc_2021} in the OCaml compiler. In this chapter, we have shown how we can take most ideas from \destcalculus{} (\cref{chap:dest-calculus}) and adapt them to allow safe destination passing in Haskell, without modifying the core principles of this language. We also showed that most of the examples presented in chapter \cref{chap:dest-calculus} can be implemented in DPS Haskell with little to no adaptations required, and we also emphasized how our particular implementation of destination passing for Haskell can help to improve the performance of programs in which garbage collection is very costly. To that extent, we rely on the \emph{compact regions}~\cite{yang_efficient_2015} memory management scheme for Haskell; but our work also extend the way in which compact regions can be used efficiently.

Finally, we also demonstrated practical gains brought by destination-passing for some of the algorithms we presented all along this manuscript. Still, our DPS Haskell implementation is a bit heavyweight and thus doesn't always let the destination-based algorithms match the theoretical performance expectations.

In this chapter, we could only use DPS Haskell for structures carrying unrestricted data. This was decided to keep a (relatively) simple API while avoiding the issues of scope escape (\cref{sec:scope-escape-dests}). In the next chapter, we'll revisit this limitation and find ways to modify DPS Haskell so that we can safely store linear resources in structures with holes.
