\chapter{Introduction}\label{chap:global-intro}

\section{Memory management: a core design axis for programming languages}

Over the last fifty years, programming languages have evolved into indispensable tools, profoundly shaping how we interact with computers and process information across almost every domain. Much like human languages, the vast variety of programming languages reflects the diversity of computing needs, design philosophies, and objectives that developers and researchers have pursued. This diversity is a response to specialized applications, user preferences, and the continuous search for improvements in speed, efficiency, and expressiveness. Today, hundreds of programming languages exist, each with unique features and tailored strengths, making language selection a nuanced process for developers.

At a high level, programming languages differ in how they handle core aspects of data and program execution, and they can be classified along several key dimensions. These dimensions often reveal the underlying principles of the language and its suitability for different types of applications. Some of the main characteristics that distinguish programming languages include:
\begin{itemize}
    \item how the user communicates intent to the computer --- whether through explicit step-by-step instructions in procedural languages or through a more functional/declarative style that emphasizes what to compute rather than how;
    \item the organization and manipulation of data --- whether through object-oriented paradigms that encapsulate domain data within objects than can interact with each other or through simpler data structures that can be operated on and dissected by functions and procedures;
    \item the level of abstraction --- a higher-level language abstracts technical details to enable more complex functionality in fewer lines of code, while lower-level languages provide more control over the environment and execution of a task;
    \item the management of memory --- whether memory allocation and deallocation are automatic, handled by the language itself, or require explicit intervention from the programmer;
    \item side effects and exceptions --- whether they can in represented by the language, caught, and/or manipulated.
\end{itemize}

Among these, memory management is one of the most critical, yet often less visible, aspects of programming language design. Every program requires memory to store data and instructions, and this memory must be managed efficiently to prevent errors and maintain performance. Typically, programs operate with input sizes and data structures that can vary greatly, requiring dynamic memory allocation to ensure smooth execution. Therefore, memory management in a programming language must balance the need for flexibility with performance and reliability. The way a language handles memory management impacts not only how programmers write and structure their code but also the scope of features the language can support.

High-level languages, such as Python or Java, often manage memory automatically, through garbage collection. With automatic garbage collection (thanks to a tracing or reference-counting garbage collector), memory allocation and deallocation happen behind the scenes, freeing developers from the complexities of manual memory control. This automatic memory management simplifies programming, allowing developers to focus on functionality and making the language more accessible for rapid development. Although garbage collection is decently fast overall, it can be slow for some specific use-cases, and is also dreaded for its unpredictable overhead or pauses in the program execution.

In contrast, low-level languages like C or Zig tend to provide developers with direct control over memory allocation and deallocation. It indeed allows for greater optimization and efficient resource usage, particularly useful in systems programming or performance-sensitive applications. This control, however, comes with increased risk; errors like memory leaks, buffer overflows, and dangling pointers can lead to instability and security vulnerabilities if not carefully detected and addressed.

Interestingly, some languages defy these typical categorizations by combining high-level features with rigorous memory management. For instance, Rust offers advanced abstractions without sacrificing strict control over memory. Through its unique ownership model, Rust provides safety guarantees comparable to those of garbage-collected languages but without the overhead. This makes it suitable for both high-level application programming and low-level systems development, bridging a gap that was traditionally divided by memory management style.

Ultimately, memory management is more than a technical detail; it is a foundational characteristic that influences not only the efficiency and reliability of the language but also the range of possible features and applications. A language's approach to memory shapes everything from performance and security to developer experience.

\section{Destination passing style: taking roots in the old imperative world}

In programming languages where fine control over memory is essential, one approach is explicit manual memory allocation and deallocation, as seen in languages like C. Here, developers bear full responsibility for allocating memory when needed and freeing it once it's no longer in use. This allows for precise memory optimization but is highly error-prone, with memory errors such as use-after-free, double free, or memory leaks being notoriously difficult to debug.

A more robust way to control memory, with fewer opportunities for error, involves managing resource lifetimes explicitly while allowing the language to handle for us allocation and deallocation at the start and end of each resource's lifetime. This technique draws on principles like smart pointers in C++ and the ownership model in Rust to manage resource lifetimes, as well as Scope-Bound Resource Management (SBRM) and Resource Acquisition is Initialization (RAII) to handle memory implicitly for these resources. Initially applicable only to stack-allocated objects in early C++, SBRM evolved with the introduction of smart pointers for heap-allocated objects in the early 2000s. These tools have since become fundamental to modern C++ and Rust, significantly improving memory safety and are now on the path to becoming the standard for memory management in systems programming.

However, much system software was written before smart pointers and SBRM were available, leading to the establishment of a few best practices for managing resources in manual memory management. One key principle is that the scope that allocates a resource should also be responsible for deallocating it, simplifying memory management. This has a major implication though: functions that produce non-primitive results (i.e. data more complex than simple integers) should not themselves allocate the memory for those results. Since the results must often outlive the function's scope, they should be allocated by the caller's scope, not within the function itself.

In practice, this means that the parent scope manages not only the allocation and deallocation of a function's inputs, but also those of \emph{the function's outputs}. Output slots are passed to functions as pointers (or mutable references in higher-level languages), allowing the function to write directly into memory managed by the caller. These pointers, called out parameters or \emph{destinations}, specify the exact memory location where the function should store its output.

This idea forms the foundation of destination-passing style programming: instead of allowing functions to allocate memory for their outputs, the caller provides them with write access to a pre-allocated memory space that can hold the function's results.

\paragraph{More control over memory}

Assigning memory allocation responsibility to the caller, rather than the callee, makes functions more flexible to use. Indeed, in DPS, memory allocation gets decoupled from the actual function logic. This holds true even in the functional interpretation of destination-passing style (DPS) that will follow: destination-passing style is strictly more general than immediate initialization of data structures.

In practice, DPS offers additional performance benefits. For example, a large memory block can be allocated in advance and divided into smaller segments, each designated as an output slot for a specific function call. This minimizes the number of separate allocations required, which is often a performance bottleneck, especially in contexts where memory allocation is intensive, both in manual memory management and garbage-collected environments.

\section{Functional programming languages}

Functional programming languages are often seen as the opposite end of the spectrum from system languages.

Functional programming lacks a single definition, but most agree that a functional language:
\begin{itemize}
    \item supports lambda abstractions, aka. anonymous functions, as first-class values, allowing functions to be stored, passed as parameters, and to capture parts of their parent environment (closures);
    \item emphasizes expressions over statements, where each instruction produces a value of a specific type;
    \item builds complex expressions by applying and composing functions rather than chaining statements.
\end{itemize}

From these principles, functional languages tend to favor immutable data structures and a declarative style, where new data structures are defined by specifying their differences from existing ones, often relying on structural sharing and persistent data structures instead of direct mutation.

Since “everything is an expression” in functional languages, they are particularly amenable to mathematical formalization. This is no accident: many core concepts in functional programming originate in mathematics, such as lambda calculus and its extensions. Lambda calculus, a minimal yet fully expressive model of computation, is as powerful as a Turing machine (which is a rather empirical model of computation) but also connects closely with formal proofs through the Curry-Howard isomorphism.

Despite this, many functional languages still permit expressions with side effects. Side effects encompass all the observable actions a program can take on its environment, e.g. writing to a file, or to the console, or altering memory. Side effects are hard to reason about, especially if any expression of the language is likely to emit one, but they are in fact a very crucial piece of any programming language: without them, there is no way for a program to communicate its results to the "outside".

\paragraph{Pure functional languages}

In response, some languages enforce a stricter approach by disallowing side effects until the boundaries of the main program, a concept known as \emph{pure functional programming}. Here, all programmer-defined expressions compute and return values without side effects. Instead, the \emph{intention} of performing side effects (like printing to the console) can be represented as a value, which only the language runtime evaluates to trigger the intended side effect.

This restriction provides substantial benefits. Programs can be wholly modeled as mathematical functions, making reasoning about behavior easier and allowing for more powerful analysis of a program’s behavior.

A key property of pure functional languages is \emph{referential transparency}: any function call can be substituted with its result without altering the program’s behavior. This property is obviously absent in languages like C; for example, replacing \mintinline{C}`write(myfile, string, 12)` with the value \mintinline{C}`12` (its return if the write is successful) would not produce the intended behavior: the latter would not produce any write effect at all.

This ability to reason about programs as pure functions greatly improves the predictability of programs, especially across library boundaries, and overall improves software safety. Pure functional languages also support easier and more efficient application of formal methods, enabling stronger guarantees with less effort.

\paragraph{Memory management in functional languages}

Explicit memory management is at its core very \emph{impure}. Indeed, changing the state of memory in a way that it can be later inspected \emph{is} a side effect. As a result, most functional languages, even those which aren't fully \emph{pure}, like OCaml or Scala, tend to use a garbage collector to hide memory management and access from the user.

However, it doesn't mean that functional languages have to completely give up with control over memory. It just means that memory control cannot be done using instructions like \mintinline{C}`malloc` or \mintinline{C}`free`; instead memory management have to be orthogonal to the ``business logic'' expressions.

In practice, that means that memory management should not impact the value of an expression of the being computed in the language. One way to do so is to use annotations that attach to a given function or block of code how it should be managed/where it should be allocated. Another way is to use a type system with \emph{modes} to indicate how a value should be managed memory-wise. Yet another would be to use magic, functions like \mintinline{haskellc}`oneShot` in Haskell that have the type of the identity function \mintinline{haskellc}`a -> a`, so they doesn't change the value being computed, but have special meaning for the compiler.

There is still another solution though. We said earlier that side effects --- which are forbidden in pure functional language --- are modification of environment or memory that is observable in a way by the user. What if we can allow explicit memory management expressions but prevent the user from observing these (temporary) modification of the environment or memory?

In fact, this is the path we'll follow to give more fine-grained control over memory while respecting the purity principle.

\section{Functional structures with holes: pioneered by Minamide and quite active since}

Let's start by saying that in most contexts, not having pay attention to memory management at all in functional languages, thanks to the presence of a garbage collector, is actually quite efficient. Often it performs better than programs in which memory has to be managed manually at all time.

The biggest limitation comes in fact from the \emph{immutability} aspect of most functional languages. It only let us build data structures in their final form immediately. Any update that is not just expanding the original structure has to make a (partial) copy of that original structure.

As a result, algorithms that creates large structures, either by reading an external source, or by transforming an existing structure, will have to silently allocate tons of intermediate structures that each represent an intermediary state of the processing. As said before, we have good reasons to favor immutability, but in that present case, it can gets in the way of performance optimization.

One idea, pioneered by Minamide\cite{minamide_functional_1998}, is to extend the functional language with a specific type for structures that are yet incomplete. Incomplete structures are not allowed to be read until they are completed. This is enforced by the type system. That way, the actual mutation that take place, behind the hood, to update the structure from its incomplete state to the completed one cannot be observed by the user: the latter can only observe the final result, as in a normal immutable functional language. Using the type system to make sure intrinsically imperative/impure computations are safe and that their effects cannot be observed by the user is the key idea I will build upon in this document.

In Minamide's work, incomplete structures, aka. \emph{structures with a hole} are formally represented by hole abstractions: a sort of pure function that takes a value having the type of the missing bit of the structure as an argument, and returns the structure completed using the supplied piece. In other terms it represents the pending construction of a structure, than can only be carried out when all its pieces have been supplied. However, hole abstractions can be composed, like LEGO bricks, and the composition can be computed immediately, with no need to wait for the missing piece of the resulting structure: $([[ˢλ x ¹ν ⟼ ˢ1 ˢ:: x]]) \circ ([[ˢλ y ¹ν ⟼ ˢ2 ˢ:: y]]) \leadsto [[ˢλ y ¹ν ⟼ ˢ1 ˢ:: ༼ˢ2 ˢ:: y༽]]$. Memory wise, each hole abstraction is not represented by a function object, but rather by a pair of pointers, one to the data structure that the hole abstraction describes, and one to the yet unspecified field in the data structure. Composition of two holes abstractions results, memory-wise, in the mutation of the previously missing field of the first one, to point to the root of the second one.

In Minamide's system, the (pointer to the) hole and the structure that has the hole are inseparable. That's a reason why an incomplete structure is only allowed to have one hole. As a result, I would argue that with Minamide's system, we're not yet doing destination-passing style programming. Still, it is one of the first instances of a pure functional language that allows for data structures to be passed around with write permissions without compromising the guarantees of the language (memory safety, purity, etc).

This idea has been refreshed and extended more recently with \cite{leijen_trmc_2023} and \cite{lorenzen_searchtree_2024}.

To reach true destination-passing style programming however, we need a way to refer to the hole(s) of the incomplete structure without having to pass around the structure that has the hole(s). This in fact hasn't been explored much in functional settings so far. \cite{bour_tmc_2021} is probably the closest existing work on this matter, but destination-passing style is only used at the intermediary language level, in the compiler, to do optimizations, so the user can't make use of DPS in their code.

\section{Unifying and formalizing Functional DPS frameworks}

What I'll develop on in this thesis is a functional language in which \emph{structure with holes} are a first-class type in the language, as in~\cite{minamide_functional_1998}, but that also integrate first-class \emph{pointer to these holes}, aka. \emph{destinations}, as part of the user-facing language.

We'll see that combining these two features give us performance improvement of some functional algorithms like in~\cite{bour_tmc_2021} or \cite{leijen_trmc_2023}, but we also get some extra expressiveness that is usually a privilege of imperative languages only. It should be indeed the first instance of a functional language that supports user-facing write pointers to make flexible and efficient data structure building!

In fact, I'll design a formal language that is based at its core on these two principles, breaking from the traditional way of building structures in functional languages; and I'll demonstrate how this can be implemented in a practical functional language and that predicted benefits of the approach are mostly preserved through this implementation.

The formal language will not only be the basis for the implementation, but will aim at providing a framework that encompass most existing work on functional DPS, and can be used to prove that earlier work is indeed sound. Of course, immutability and referential transparency principles have to be part of this formal language.

\section{Linear types: reconcile write pointers and immutability}

We cannot just do imperative memory mutations in a functional setting and just hope for the best.

We'll need tools to ensure only controlled mutations are allowed. Basically we want to allow for a structure to be partially uninitialized for a while, and have write pointers pointing to the uninitialized bits of the structure. But once the structure has been completely defined, with a value for each field, then it shouldn't change anymore. Otherwise we break immutability. In other terms, we want to ensure a write-once discipline for fields of a structure (which are mutated through the associated \emph{destination}s, aka. write pointers). Controlling the number of uses of destinations at runtime would be very inconvenient; instead, we need static guarantees that each destination is only used once.

In PLT, when facing a need for new static guarantees, people design a type system that ensures that the desired guarantee holds if the program is well typed. This is exactly what I'm going to do here. It's made easier by the existence of a well-studied type system, \emph{linear types}, that can be used to track the number of uses of resources. 



- Linear types are a refinement of usual typing theory where variable uses can be counted

- Functions have to announce in their signature how many times they use their arguments

- This allows to track write pointers and ensure they are used exactly once

- Using a write pointer exactly once before the structure is ever read is no different than giving a value to a field when the structure is defined, in terms of observability
