\chapter{Introduction}\label{chap:global-intro}

\section{Memory management: a core design axis for programming languages}

Over the last fifty years, programming languages have evolved into indispensable tools, profoundly shaping how we interact with computers and process information across almost every domain. Much like human languages, the vast variety of programming languages reflects the diversity of computing needs, design philosophies, and objectives that developers and researchers have pursued. This diversity is a response to specialized applications, user preferences, and the continuous search for improvements in speed, efficiency, and expressiveness. Today, hundreds of programming languages exist, each with unique features and tailored strengths, making language selection a nuanced process for developers.

At a high level, programming languages differ in how they handle core aspects of data and program execution, and they can be classified along several key dimensions. These dimensions often reveal the underlying principles of the language and its suitability for different types of applications. Some of the main characteristics that distinguish programming languages include:
\begin{itemize}
    \item how the user communicates intent to the computer --- whether through explicit step-by-step instructions in procedural languages or through a more functional/declarative style that emphasizes what to compute rather than how;
    \item the organization and manipulation of data --- whether through object-oriented paradigms that encapsulate domain data within objects than can interact with each other or through simpler data structures that can be operated on and dissected by functions and procedures;
    \item the level of abstraction --- a higher-level language abstracts technical details to enable more complex functionality in fewer lines of code, while lower-level languages provide more control over the environment and execution of a task;
    \item the management of memory --- whether memory allocation and deallocation are automatic, handled by the language itself, or require explicit intervention from the programmer;
    \item side effects and exceptions --- whether they can in represented by the language, caught, and/or manipulated.
\end{itemize}

Among these, memory management is one of the most critical, yet often less visible, aspects of programming language design. Every program requires memory to store data and instructions, and this memory must be managed efficiently to prevent errors and maintain performance. Typically, programs operate with input sizes and data structures that can vary greatly, requiring dynamic memory allocation to ensure smooth execution. Therefore, memory management in a programming language must balance the need for flexibility with performance and reliability. The way a language handles memory management impacts not only how programmers write and structure their code but also the scope of features the language can support.

High-level languages, such as Python or Java, often manage memory automatically, through garbage collection. With automatic garbage collection (thanks to a tracing or reference-counting garbage collector), memory allocation and deallocation happen behind the scenes, freeing developers from the complexities of manual memory control. This automatic memory management simplifies programming, allowing developers to focus on functionality and making the language more accessible for rapid development. Although garbage collection is decently fast overall, it can be slow for some specific use-cases, and is also dreaded for its unpredictable overhead or pauses in the program execution.

In contrast, low-level languages like C or Zig tend to provide developers with direct control over memory allocation and deallocation. It indeed allows for greater optimization and efficient resource usage, particularly useful in systems programming or performance-sensitive applications. This control, however, comes with increased risk; errors like memory leaks, buffer overflows, and dangling pointers can lead to instability and security vulnerabilities if not carefully detected and addressed.

Interestingly, some languages defy these typical categorizations by combining high-level features with rigorous memory management. For instance, Rust offers advanced abstractions without sacrificing strict control over memory. Through its unique ownership model, Rust provides safety guarantees comparable to those of garbage-collected languages but without the overhead. This makes it suitable for both high-level application programming and low-level systems development, bridging a gap that was traditionally divided by memory management style.

Ultimately, memory management is more than a technical detail; it is a foundational characteristic that influences not only the efficiency and reliability of the language but also the range of possible features and applications. A language's approach to memory shapes everything from performance and security to developer experience.

\section{Destination passing style: taking roots in the old imperative world}

In programming languages where fine control over memory is essential, one approach is explicit manual memory allocation and deallocation, as seen in languages like C. Here, developers bear full responsibility for allocating memory when needed and freeing it once it's no longer in use. This allows for precise memory optimization but is highly error-prone, with memory errors such as use-after-free, double free, or memory leaks being notoriously difficult to debug.

A more robust way to control memory, with fewer opportunities for error, involves managing resource lifetimes explicitly while allowing the language to handle for us allocation and deallocation at the start and end of each resource's lifetime. This technique draws on principles like smart pointers in C++ and the ownership model in Rust to manage resource lifetimes, as well as Scope-Bound Resource Management (SBRM) and Resource Acquisition is Initialization (RAII) to handle memory implicitly for these resources. Initially applicable only to stack-allocated objects in early C++, SBRM evolved with the introduction of smart pointers for heap-allocated objects in the early 2000s. These tools have since become fundamental to modern C++ and Rust, significantly improving memory safety and are now on the path to becoming the standard for memory management in systems programming.

However, much system software was written before smart pointers and SBRM were available, leading to the establishment of a few best practices for managing resources in manual memory management. One key principle is that the scope that allocates a resource should also be responsible for deallocating it, simplifying memory management. This has a major implication though: functions that produce non-primitive results (i.e. data more complex than simple integers) should not themselves allocate the memory for those results. Since the results must often outlive the function's scope, they should be allocated by the caller's scope, not within the function itself.

In practice, this means that the parent scope manages not only the allocation and deallocation of a function's inputs, but also those of \emph{the function's outputs}. Output slots are passed to functions as pointers (or mutable references in higher-level languages), allowing the function to write directly into memory managed by the caller. These pointers, called out parameters or \emph{destinations}, specify the exact memory location where the function should store its output.

This idea forms the foundation of destination-passing style programming: instead of allowing functions to allocate memory for their outputs, the caller provides them with write access to a pre-allocated memory space that can hold the function's results.

\paragraph{More control over memory}

Assigning memory allocation responsibility to the caller, rather than the callee, makes functions more flexible to use. Indeed, in DPS, memory allocation gets decoupled from the actual function logic. This holds true even in the functional interpretation of destination-passing style (DPS) that will follow: destination-passing style is strictly more general than immediate initialization of data structures.

In practice, DPS offers additional performance benefits. For example, a large memory block can be allocated in advance and divided into smaller segments, each designated as an output slot for a specific function call. This minimizes the number of separate allocations required, which is often a performance bottleneck, especially in contexts where memory allocation is intensive, both in manual memory management and garbage-collected environments.

\section{Functional programming languages}

Functional programming languages are often seen as the opposite end of the spectrum from system languages.

Functional programming lacks a single definition, but most agree that a functional language:
\begin{itemize}
    \item supports lambda abstractions, aka. anonymous functions, as first-class values, allowing functions to be stored, passed as parameters, and to capture parts of their parent environment (closures);
    \item emphasizes expressions over statements, where each instruction produces a value of a specific type;
    \item builds complex expressions by applying and composing functions rather than chaining statements.
\end{itemize}

From these principles, functional languages tend to favor immutable data structures and a declarative style, where new data structures are defined by specifying their differences from existing ones, often relying on structural sharing and persistent data structures instead of direct mutation.

Since “everything is an expression” in functional languages, they are particularly amenable to mathematical formalization. This is no accident: many core concepts in functional programming originate in mathematics, such as lambda calculus and its extensions. Lambda calculus, a minimal yet fully expressive model of computation, is as powerful as a Turing machine (which is a rather empirical model of computation) but also connects closely with formal proofs through the Curry-Howard isomorphism.

Despite this, many functional languages still permit expressions with side effects. Side effects encompass all the observable actions a program can take on its environment, e.g. writing to a file, or to the console, or altering memory. Side effects are hard to reason about, especially if any expression of the language is likely to emit one, but they are in fact a very crucial piece of any programming language: without them, there is no way for a program to communicate its results to the "outside".

\paragraph{Pure functional languages}

In response, some languages enforce a stricter approach by disallowing side effects until the boundaries of the main program, a concept known as \emph{pure functional programming}. Here, all programmer-defined expressions compute and return values without side effects. Instead, the \emph{intention} of performing side effects (like printing to the console) can be represented as a value, which only the language runtime evaluates to trigger the intended side effect.

This restriction provides substantial benefits. Programs can be wholly modeled as mathematical functions, making reasoning about behavior easier and allowing for more powerful analysis of a program’s behavior.

A key property of pure functional languages is \emph{referential transparency}: any function call can be substituted with its result without altering the program’s behavior. This property is obviously absent in languages like C; for example, replacing \mintinline{C}`write(myfile, string, 12)` with the value \mintinline{C}`12` (its return if the write is successful) would not produce the intended behavior: the latter would not produce any write effect at all.

This ability to reason about programs as pure functions greatly improves the predictability of programs, especially across library boundaries, and overall improves software safety. Pure functional languages also support easier and more efficient application of formal methods, enabling stronger guarantees with less effort.

\paragraph{Memory management in functional languages}

Explicit memory management is inherently \emph{impure}, as modifying memory in a way that can later be inspected \emph{is} a side effect. Consequently, most functional languages, even those that aren’t fully \emph{pure} like OCaml or Scala, tend to rely on a garbage collector to abstract memory management and access away from the user.

However, this doesn’t mean that functional languages must entirely forgo memory control; it simply means that memory control cannot involve explicit instructions like \mintinline{C}`malloc` or \mintinline{C}`free`. Instead, memory management must remain orthogonal to the “business logic”, in other terms, expressions that carries the real meaning of the program.

In practice, it requires that memory management should not affect the value of an expression being computed within the language. One way to achieve this is by using annotations attached to specific functions or blocks of code, indicating how or where memory should be allocated. Another approach is to employ a type system with \emph{modes} that specify how values are managed in memory (more on that later). Additionally, some languages use special functions, like \mintinline{haskellc}`oneShot` in Haskell, which have the type of the identity function \mintinline{haskellc}`a -> a` but carry special significance for the compiler in managing memory.

There is also another possible solution. We previously mentioned that side effects --- prohibited in pure functional languages --- are any modifications of the environment or memory that are observable by the user. What if we allow explicit memory management expressions while ensuring that these (temporary) changes to memory or the environment remain unobservable?

This is the approach we will adopt, allowing for finer-grained memory control while upholding the principles of purity.

\section{Functional structures with holes: pioneered by Minamide and quite active since}

In most contexts, functional languages with garbage collection efficiently manage memory without requiring programmer intervention. In fact, they often perform better than systems where memory must be managed manually at all times.

However, a major limitation arises from the \emph{immutability} characteristic common to most functional languages, which restricts us to constructing data structures directly in their final form. That means the programmer has to give an initial value to every field of the structure, even if no meaningful value for them has been computed yet. And later, any update beyond simply expanding the original structure requires creating a (partial) copy of that structure.

As a result, algorithms that generate large structures --- whether by reading from an external source or transforming an existing structure --- might create many intermediate structures, each representing a temporary processing state. While immutability has advantages, in this case, it can become an obstacle to optimizing performance.

Minamide’s work\cite{minamide_functional_1998} introduces an approach that extends functional languages with a specific type to represent \emph{yet incomplete} data structures. Incomplete structures are not allowed to be read until they are completed. This is enforced by the type system. This way, the underlying mutations that occur while updating the structure from its incomplete state to completion are hidden from the user, who can only observe the final result, preserving the feel of an immutable functional language. This method of making imperative or impure computations opaque to the user by making sure that their effects remain unobservable to the user thanks to type-level guarantees, is central to the approach developed in this document.

In Minamide's work, incomplete structures, or \emph{structures with a hole}, are represented by hole abstractions --- essentially pure functions that take the missing component of the structure as an argument and return the completed structure. In other terms, it represents the pending construction of a structure, than can theoretically only be carried out when all its missing pieces have been supplied. Hole abstractions can also be composed, like functions: $([[ˢλ x ¹ν ⟼ ˢ1 ˢ:: x]]) \circ ([[ˢλ y ¹ν ⟼ ˢ2 ˢ:: y]]) \leadsto [[ˢλ y ¹ν ⟼ ˢ1 ˢ:: ༼ˢ2 ˢ:: y༽]]$. Behind the scenes however, each hole abstraction is not represented in memory by a function object, but rather by a pair of pointers, one to a data structure that the hole abstraction describes, and one to the yet unspecified field in the data structure. Composition of two holes abstractions can be carried out immediately, with no need to wait for the missing value of the second hole abstraction, and results, memory-wise, in the mutation of the previously missing field of the first one, to point to the root of the second one.

In Minamide’s system, the (pointer to the) hole and the structure containing it are inseparable, which limits an incomplete structure to having only a single hole. For these reasons, I would argue that Minamide’s approach does not yet qualify as destination-passing style programming. Nonetheless, it is one of the earliest examples of a pure functional language allowing data structures to be passed with write permissions while preserving key language guarantees such as memory safety and purity.

This idea has been refreshed and extended more recently with \cite{leijen_trmc_2023} and \cite{lorenzen_searchtree_2024}.

To reach true destination-passing style programming however, we need a way to refer to the hole(s) of the incomplete structure without having to pass around the structure that has the hole(s). This in fact hasn't been explored much in functional settings so far. \cite{bour_tmc_2021} is probably the closest existing work on this matter, but destination-passing style is only used at the intermediary language level, in the compiler, to do optimizations, so the user can't make use of DPS in their code.

\section{Unifying and formalizing Functional DPS frameworks}

What I'll develop on in this thesis is a functional language in which \emph{structure with holes} are a first-class type in the language, as in~\cite{minamide_functional_1998}, but that also integrate first-class \emph{pointer to these holes}, aka. \emph{destinations}, as part of the user-facing language.

We'll see that combining these two features give us performance improvement of some functional algorithms like in~\cite{bour_tmc_2021} or \cite{leijen_trmc_2023}, but we also get some extra expressiveness that is usually a privilege of imperative languages only. It should be indeed the first instance of a functional language that supports user-facing write pointers to make flexible and efficient data structure building!

In fact, I'll design a formal language that is based at its core on these two principles, breaking from the traditional way of building structures in functional languages; and I'll demonstrate how this can be implemented in a practical functional language and that predicted benefits of the approach are mostly preserved through this implementation.

The formal language will not only be the basis for the implementation, but will aim at providing a framework that encompass most existing work on functional DPS, and can be used to prove that earlier work is indeed sound. Of course, immutability and referential transparency principles have to be part of this formal language.

\section{Linear types: reconcile write pointers and immutability}

We cannot simply introduce imperative memory mutations into a functional setting and hope for the best. Instead, we need tools to ensure that only controlled mutations occur. The goal is to allow a structure to remain partially uninitialized temporarily, with write pointers referencing these uninitialized portions. However, once the structure has been fully populated, with a value assigned to each field, it should become immutable to maintain the integrity of the functional paradigm. Put simply, we need a write-once discipline for fields within a structure, which can be mutated through their associated \emph{destinations}, aka. write pointers. While enforcing a single-use discipline for destinations at runtime would be unwieldy, we can instead deploy static guarantees through the type system to ensure that every destination will be used exactly once.

In programming language theory, when new guarantees are required, it’s common to design a type system to enforce them, ensuring the program remains well-typed. This is precisely the approach I’ll take here, facilitated by an established type system --- \emph{linear types} --- which can monitor resource usage, especially the number of times a variable is used.

Linear Logic, introduced by Jean-Yves Girard in the 1980s, refines classical and intuitionistic logic by restricting certain rules of deduction, specifically \emph{weakening} and \emph{contraction}. In classical logic, these rules allow assumptions to be duplicated or discarded freely. Linear Logic, on the other hand, eliminates these rules to enforce a stricter form of logical reasoning in which each assumption must be used exactly once.

Through the Curry-Howard isomorphism, intuitionistic linear logic becomes the linear simply typed $\lambda$-calculus. It inherits the same property has linear logic: there is no weakening and contraction allowed for normal types, in other terms, by default, each variable in a typing context must be used exactly once to form a valid program.

Only a variable with a type prefixed by the new modality $\ottstype{!}$, pronounced \emph{bang} or \emph{of course}, can be used multiple times. That way, we retain the expressive power of the full intuitionistic logic. A new function arrow $\ottstype{\multimap}$ is introduced to denote that a function uses its argument exactly once. The usual function type $[[T]] \ottstype{\to} [[U]]$, with no restrictions on how many times the argument of type $[[T]]$ is used, can be encoded as $\ottstype{(![[T]])\!\!\multimap}[[U]]$.

Leveraging a linear type system, we can design a language in which structures with holes are first-class citizens, and write pointers to these holes are only usable once. This ensures that each hole is filled exactly once, preserving immutability and making sure that structure are indeed completed before being read.

