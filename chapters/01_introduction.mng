\chapter{Introduction}\label{chap:global-intro}

\section{Destination passing style: taking roots in imperative world}

The way memory management is done in a programming language is is a very important aspect of the language design, as it has to integrate tightly to the core of the language and often restricts which kind of features the language can later host.

Garbage collection (with tracing GC or reference-counting one) is a very convenient solution for memory management, that happens to be decently fast in most contexts, and let the user forget about memory management completely. It also doesn't hamper the addition of very high-level features to the language.

In system programming however, and in many contexts with real-time constraints, garbage collection is dreaded has it incurs unpredictable overhead, often with actual pauses in the program execution to free some memory. In these contexts, programmers tend to rely on two memory managing schemes:

\begin{itemize}
    \item manual allocation and deallocation of memory, with explicit mutation
    \item \emph{smart pointers}, which covers memory allocation and initialization of said memory, and automatic deallocation when the resource goes out of scope
\end{itemize}

The smart pointer scheme is, in practice, way safer than manual memory management, but it requires a more subtle \emph{move} semantics for the language, and also requires a clear notion of scope (Scope-bound-resource-management, SBRM, aka RAII). It also needs a borrow-checking system --- that can reveal to be quite complex --- if smart-pointer-allocated resources are allowed to be shared (instead of moved).

It is a relatively recent development in PL: SBRM was already present in the first release of C++, but only for stack-allocated objects; heap-allocated objects had to be allocated and released manually. It's during the early 00's that actual smart pointers for heap objects were born, and progressively got adopted in C++, and then later, have been incorporated at the root of the Rust programming language.

So, when smart pointers were not available, people developed good practices or guidelines to avoid the most common pitfalls of manual memory management. One of the most important principle is that the scope in which a resource is allocated must be the scope in charge of deallocation of the said resource. It makes tracking of resource management much easier. This has an immediate, major implication though: a function that produces a non-primitive result (i.e. not just an integer) should \emph{not} allocate that result itself. Because the result has to outlive the current scope, it should not, of course, be deallocated here. So it mustn't be allocated in the scope of the function, but rather, in the caller scope.

In other words, the parent scope is responsible both for allocation and deallocation of inputs, but also \emph{outputs} that will be used by a function call. And outputs slots will be communicated to the function using pointers, or mutable references in higher level languages. We call such pointers \emph{out parameters} or \emph{destinations}: they represent the \emph{destinations} in which the function will write its outputs when it is called.

This is where the idea of destination-passing style programming takes its root: instead of letting functions allocate for their results, we can instead provide them with a write pointer, or write capability for a memory area that is large enough to hold their results.

\emph{More control over memory}

Making allocation a responsibility of the caller instead of the callee makes functions actually more flexible to use. This is an idea that will stay true even in the functional interpretation of DPS: destination-passing style is strictly more general than allocation with immediate initialization of data structures.

For example, in DPS, one can allocate a large chunk of memory at once, and divide it into smaller parts that will each be used as a result slot for a function call. Reducing the number of allocations almost always results in performance improvements, be it in manual memory management or in a GC-controlled context.

\section{Functional programming languages}

Functional programming languages are often considered to be at the totally opposite side of the spectrum compared to system languages that we just wrote about.

Functional programming doesn't have a unanimous definition, but most people agree that a functional language:
- has lambda abstractions --- that's to say anonymous functions --- as first-class values of the language. In other terms, functions can be stored and passed around as parameters like normal values, and those functions are also able to capture part of their parents environments (aka. closures).
- focuses on expressions rather than statements: every instruction returns a value of a given type.
- build more complex expressions by applying and composing functions, instead of chaining statements

From these principle follows that in functional languages, we are often more interested in building immutable data structures in declarative style, by stating how the new one should differ from the old one (structural sharing and persistent data structures), rather than mutating existing ones.

As \emph{everything is expression} in a functional language, such languages are well-suited to be mathematically formalized. In fact, that's not a pure coincidence: a lot of principles from functional programming languages are derived from mathematical principles, for instance the lambda calculus and its many extensions. Lambda calculus is a very minimal yet sufficient model of computation, that has as much expressive power as the empirical Turing machine, but has a very strong connection with Theory of Demonstration (formal proofs) thanks to the Curry-Howard isomorphism.

Still, many mainstream languages that are considered functional allow their expressions to have side effects. Side effects encompass all the observable actions a program can take on its environment. E.g. writing to a file, or to the console, or altering memory. Side effects are hard to reason about, especially if any expression of the language is likely to emit one, but they are in fact a very crucial piece of any programming language: without them, there is no way for a program to communicate its results to the "outside".

\paragraph{Pure functional languages}

In reaction, some languages take it one step further, and forbid side effects until the absolute boundaries of the main program. In other terms, everything the programmer writes are expressions that computes and return values with no side effects. The \emph{intention} of emitting a side effect (e.g. printing to the console) can be represented as a value, and only the runtime of the language will be responsible of evaluating this dedicated value into the proper side effect.

This blunt limitation to the way programs can interact with the outside actually brings many benefits. Programs can be totally modeled by mathematical values, and get much easier to reason about. We also get more inspection and analysis capabilities about the effects that a program will emit.

Finally, we get a very important property called \emph{referential transparency}: we can always substitute a function call for its result value, with no observable difference for the user. This is very not true in C for example: \mintinline{C}`write(myfile, string, 12)` cannot be substituted with just the value \mintinline{C}`12` that the function would return if everything went well. The latter would not produce the effect that the former do.

Having a program that we can reason about is a key ingredient to reducing the amount of bugs in practical software, and improving safety in general. Formal methods can be applied much more efficiently, with reduced efforts and give more ambitious guarantees, on such languages.

\paragraph{Memory management in functional languages}

- usually garbage-collected or reference-counted, as manual memory management usually breaks referential transparency

- can be made to work in a GC-free fashion, Ã  la Rust, for a linear subset of the language (see Oxidizing Ocaml), but still experimental

- conversely, not always efficient to build large data-structures because of immutability -> forces to build structures in full right of the bat, from the leaves up to the root

- How to give more flexibility on data structure building while keeping the benefits of functional programming?

\section{Functional structures with holes: pioneered by Minamide and quite active since}

- Allow structures with holes to be a first-class type of the language

- Structures no longer have to be built in full. Can be built incrementally, with holes to be filled later

- When DPS is exposed to the user, usually write pointers are not exposed to the user; the latter can only compose/merge structures with holes. They work like linear functions (Minamide, TMRC, Imperative Nature of BF Search trees) so no safety proof is needed

- When DPS has exposed write pointers, it is used in intermediary language; not in the user-facing one, in a framework where its usage is thought to be sound (TMC Ocaml)

- Still, both of these approaches show promising results in terms of performance and expressiveness

* Enable tail-recursion in usually non-tail-recursive contexts, e.g. map

* Enable efficient and more flexible data structure building (e.g. when reading from a serialized source)

- Proof of that with bench / graphs

\section{Unifying and formalizing Functional DPS frameworks}

- Can we go a step further, and design a unified framework for functional DPS?

- This framework has to support first-class write-pointers, with safeguards to not break immutability and referential transparency

- This framework has to be able to express the two previous approaches, and more

- This framework has to be able to be used in a user-facing language, with a proof of soundness

\section{Linear types: reconcile write pointers and immutability}

- Linear types are a refinement of usual typing theory where variable uses can be counted

- Functions have to announce in their signature how many times they use their arguments

- This allows to track write pointers and ensure they are used exactly once

- Using a write pointer exactly once before the structure is ever read is no different than giving a value to a field when the structure is defined, in terms of observability
