\DontNumberThisInToc
\SpecialSection{Introduction}\label{chap:global-intro}

\section{Memory management: a core design axis for programming languages}

Over the last fifty years, programming languages have evolved into indispensable tools, profoundly shaping how we interact with computers and process information across almost every domain. Much like human languages, the vast variety of programming languages reflects the diversity of computing needs, design philosophies, and objectives that developers and researchers have pursued. This diversity is a response to specialized applications, user preferences, and the continuous search for improvements in either speed or expressiveness. Today, hundreds of programming languages exist, each with unique features and tailored strengths, making language selection a nuanced process for developers.

At a high level, programming languages differ in how they handle core aspects of data representation and program execution, and they can be classified along several key dimensions. These dimensions often reveal the underlying principles of the language and its suitability for different types of applications. Some of the main characteristics that distinguish programming languages include:
\begin{itemize}
    \item how the user communicates intent to the computer --- whether through explicit step-by-step instructions in procedural languages or through a more functional/declarative style that emphasizes what to compute rather than how;
    \item the organization and manipulation of data --- whether through object-oriented paradigms that encapsulate domain data within objects than can interact with each other or through simpler data structures that can be operated on and dissected by functions and procedures;
    \item the level of abstraction --- a higher-level language abstracts technical details to enable more complex functionality in fewer lines of code, while lower-level languages provide more control over the environment and execution of a task;
    \item the management of memory --- whether memory allocation and deallocation are automatic, handled by the language itself, or require explicit intervention from the programmer;
    \item side effects and exceptions --- whether they can be represented, caught, and/or manipulated with the language.
\end{itemize}

Among these, memory management is one of the most critical, yet often less visible, aspects of programming language design. Every program requires memory to store data and instructions, and this memory must be managed judiciously to prevent errors and maintain performance. Typically, programs operate with input sizes and data structures that can vary greatly, requiring dynamic memory allocation to ensure smooth execution. Therefore, the way a language handles memory management impacts not only how programmers write and structure their code but also which scope of features the language can support.

High-level languages, such as Python or Java, often manage memory automatically, through garbage collection. With automatic garbage collection (thanks to a tracing or reference-counting garbage collector), memory allocation and deallocation happen behind the scenes, freeing developers from the complexities of manual memory control. This automatic memory management simplifies programming, allowing developers to focus on functionality and making the language more accessible for rapid development. Although garbage collection is decently fast overall, it can be slow for some specific use-cases, and is also dreaded for its unpredictable overhead or pauses in the program execution.

In contrast, low-level languages like C or Zig tend to provide developers with direct control over memory allocation and deallocation. It indeed allows for greater optimization and efficient resource usage, particularly useful in systems programming or performance-sensitive applications. This control, however, comes with increased risk; errors like memory leaks, buffer overflows, and dangling pointers can lead to instability and security vulnerabilities if not carefully detected and addressed.

Interestingly, some languages defy these typical categorizations by combining high-level features with rigorous memory management. Such languages let the user manage resource lifetimes explicitly while taking the responsibility of allocating and deallocating memory at the start and end of each resource's lifetime. The most well-known examples are smart pointers in C++ and the ownership model in Rust, whose founding principles are also known as \emph{Scope-Bound Resource Management} (SBRM) or \emph{Resource Acquisition is Initialization} (RAII). Initially applicable only to stack-allocated objects in early C++, SBRM evolved with the introduction of smart pointers for heap-allocated objects in the early 2000s. These tools have since become fundamental to modern C++ and Rust, significantly improving memory safety. In particular, Rust provides safety guarantees comparable to those of garbage-collected languages but without the garbage collection overhead, at the cost of a steep learning curve. This makes Rust suitable for both high-level application programming and low-level systems development, bridging a gap that was traditionally divided by memory management style.

Ultimately, as we've seen, memory management is more than a technical detail; it is a foundational characteristic that influences the possible features and use-cases for the language.

\section{Destination passing style: taking roots in the old imperative world}

In systems programming, where fine control over memory is essential, the same memory buffer is often (re)used several times, in order to save up memory space and decrease the number of costly calls to the memory allocator. As a result, we don't want intermediary functions to allocate memory for the result of their computations. Instead, we want to provide them with a memory address in which to write their result, so that we can decide to reuse an already allocated memory slot or maybe write to a memory-mapped file, or RDMA buffer\ldots{}

In practice, this means that the parent scope manages not only the allocation and deallocation of the function inputs, but also those of \emph{the function outputs}. Output slots are passed to functions as pointers (or mutable references in higher-level languages), allowing the function to write directly into memory managed by the caller. These pointers, called out parameters or \emph{destinations}, specify the exact memory location where the function should store its output.

This idea forms the foundation of destination-passing style programming (DPS): instead of letting a function allocate memory for its outputs, the caller provides it with write access to a pre-allocated memory space that can hold the function's results.

Assigning memory responsibility to the caller, rather than the callee, makes functions more flexible to use: this is the core advantage of destination-passing style programming (DPS).

In a low-level, system-programming setting, DPS offers direct performance benefits. For example, a large memory block can be allocated in advance and divided into smaller segments, each designated as an output slot for a specific function call. This minimizes the number of separate allocations required, which is often a performance bottleneck, especially in contexts where memory allocation is intensive.

In the functional interpretation of DPS that will follow, the memory saving will be more subtle, but we will still leverage the fact that destination-passing style is strictly more general than when functions handle memory for their results themselves.

\section{Functional programming languages}

Functional programming languages are often seen as the opposite end of the spectrum from system languages.

Functional programming lacks a single definition, but most agree that a functional language:
\begin{itemize}
    \item supports lambda abstractions, a.k.a. functions as first-class values, that can an capture parts of their parent environment (closures), can be stored, and can passed as parameters;
    \item emphasizes expressions over statements, where each instruction always produces a value;
    \item builds complex expressions by applying and composing functions rather than chaining statements.
\end{itemize}

From these principles, functional languages tend to favor immutable data structures and a declarative style, where new data structures are defined by specifying their differences from existing ones, often relying on structural sharing and persistent data structures instead of direct mutation.

Since “everything is an expression” in functional languages, they are particularly amenable to mathematical formalization. This is no accident: many core concepts in functional programming originate in mathematics, such as lambda calculus --- a minimal yet fully expressive model of computation, that also connects closely with formal proofs through the Curry-Howard isomorphism.

Despite this, many functional languages still permit expressions with side effects. Side effects encompass all the observable actions a program can take on its environment, e.g. writing to a file, or to the console, or altering memory. Side effects are hard to reason about, especially if any expression of the language is likely to emit one, but they are in fact a very crucial piece of any programming language: without them, there is no way for a program to communicate its results to the "outside".

\paragraph{Pure functional languages}

In response, some languages enforce a stricter approach by disallowing side effects except at the boundaries of the main program, a concept known as \emph{pure functional programming}. Here, all programmer-defined expressions compute and return values without side effects. Instead, the \emph{intention} of performing side effects (like printing to the console) can be represented as a value, which only the language runtime evaluates to trigger the intended side effect.

This restriction provides substantial benefits. Programs can be wholly modeled as mathematical functions, making reasoning about behavior easier and allowing for more powerful analysis of a program’s behavior.

A key property of pure functional languages is \emph{referential transparency}: any function call can be substituted with its result without altering the program’s behavior. This property is obviously absent in languages like C; for example, replacing \mintinline{C}/write(myfile, string, 12)/ with the value \mintinline{C}/12/ (its return if the write is successful) would not produce the intended behavior: the latter would not produce any write effect at all.

This ability to reason about programs as pure functions greatly improves the predictability of programs, especially across library boundaries, and overall improves software safety.

\paragraph{Memory management in functional languages}

Explicit memory management is very often \emph{impure}, as modifying memory in a way that can later be inspected \emph{is} a side effect. Consequently, most functional languages, even those that aren’t fully \emph{pure} like OCaml or Scala, tend to rely on a garbage collector to abstract away memory management and memory access for the user.

However, this doesn’t mean that functional languages must entirely give up any aspect of memory control; it simply means that memory control cannot involve explicit effectful statements like \mintinline{C}/malloc/ or \mintinline{C}/free/. In practice, it requires that memory management should not affect the value of an expression being computed within the language. One way to achieve this is by using annotations attached to specific functions or blocks of code, indicating how or where memory should be allocated. Another approach is to employ a type system with \emph{modes} that specify how values are managed in memory (as in the recent modal development for Ocaml~\cite{lorenzen_oxidizing_2024}). Additionally, some languages use special functions\footnote{For example, \mintinline{haskellc}/oneShot/ in Haskell.} which do not affect the result of the expression they wrap around, but carry special significance for the compiler in managing memory.

There is also another possible solution. We previously mentioned that side effects --- prohibited in pure functional languages --- are any modifications of the environment or memory that are \emph{observable} by the user. What if we allow explicit memory management expressions while ensuring that these (temporary) changes to memory or the environment remain unobservable?

This is the approach we will adopt, allowing for finer-grained memory control while upholding the principles of purity.

\section{Functional structures with holes: pioneered by Minamide and quite active since}

In most contexts, functional languages with garbage collectors efficiently manage memory without requiring programmer intervention. However, a major limitation arises from the \emph{immutability} characteristic common to most functional languages, which restricts us to constructing data structures directly in their final form. That means that the programmer has to give an initial value to every field of the structure, even if no meaningful value for them has been computed yet. And later, any update beyond simply expanding the original structure requires creating a (partial) copy of that structure. This incurs load on the garbage collector.

As a result, algorithms that generate large structures --- whether by reading from an external source or transforming an existing structure --- might need many intermediate allocations, each representing a temporary processing state. While immutability has advantages, in this case, it can become an obstacle to optimizing performance.

To lift this limitation, Minamide’s work\cite{minamide_functional_1998} introduces an extension for functional languages that allows to represent \emph{yet incomplete} data structures, that can be extended and completed later. Incomplete structures are not allowed to be read until they are completed --- this is enforced by the type system --- so that the underlying mutations that occur while updating the structure from its incomplete state to completion are hidden from the user, who can only observe the final result. This way, it preserves the feel of an immutable functional language and doesn't break purity per se. This method of making imperative or impure computations opaque to the user by making sure that their effects remain unobservable to the user (thanks to type-level guarantees) is central to the approach developed in this document.

In Minamide's work, incomplete structures, or \emph{structures with a hole}, are represented by hole abstractions --- essentially pure functions that take the missing component of the structure as an argument and return the completed structure. In other terms, it represents the pending construction of a structure, than can theoretically only be carried out when all its missing pieces have been supplied. Hole abstractions can also be composed, like functions: $([[ˢλ x ¹ν ⟼ ˢ1 ˢ:: x]]) \circ ([[ˢλ y ¹ν ⟼ ˢ2 ˢ:: y]]) \leadsto [[ˢλ y ¹ν ⟼ ˢ1 ˢ:: ༼ˢ2 ˢ:: y༽]]$. Behind the scenes however, each hole abstraction is not represented in memory by a function object, but rather by a pair of pointers, one read pointer to the root of the data structure that the hole abstraction describes, and one write pointer to the yet unspecified field in the data structure. Composition of two holes abstractions can be carried out immediately, with no need to wait for the missing value of the second hole abstraction, and results, memory-wise, in the mutation of the previously missing field of the first one, to point to the root of the second one.

In Minamide’s system, the (pointer to the) hole and the structure containing it are inseparable, which limits an incomplete structure to having only a single hole. For these reasons, I would argue that Minamide’s approach does not yet qualify as destination-passing style programming. Nonetheless, it is one of the earliest examples of a pure functional language allowing data structures to be passed with write permissions while preserving key language guarantees such as memory safety and purity.

This idea has been refreshed and extended more recently with \cite{leijen_trmc_2023} and \cite{lorenzen_searchtree_2024}, where structures with holes are referred to as \emph{first-class contexts}.

To reach true destination-passing style programming however, we need a way to refer to the hole(s) of the incomplete structure without having to pass around the structure that has the hole(s). This in fact hasn't been explored much in functional settings so far. \cite{bour_tmc_2021} is probably the closest existing work on this matter, but destination-passing style is only used at the intermediary language level, in the compiler, to do optimizations, so the user can't make use of DPS in their code.

\section{Unifying and formalizing Functional DPS frameworks}

What I'll develop on in this thesis is a functional language in which \emph{structures with holes} are a first-class type in the language, as in~\cite{minamide_functional_1998}, but that also integrate first-class \emph{pointers to these holes}, aka. \emph{destinations}, as part of the user-facing language.

We'll see that combining these two features gives us performance improvement for some functional algorithms like in~\cite{bour_tmc_2021} or \cite{leijen_trmc_2023}, but we also get some extra expressiveness that is usually a privilege of imperative languages only. It should be indeed the first instance of a functional language that supports write pointers in the frontend (and not only in intermediate representation like \cite{bour_tmc_2021,leijen_trmc_2023}) to make flexible and efficient data structure building!

In fact, I'll design a formal language in which \emph{structures with holes} are the core component for building any data structure, breaking from the traditional way of building structures in functional languages using data constructor. I'll also demonstrate how these ideas can be implemented in a practical functional language and that predicted benefits of the approach are mostly preserved in this implementation.

The formal language will also aim at providing a framework that encompasses most existing work on functional DPS, and can be used to prove that earlier work is indeed sound.

\section{Reconciling write pointers and immutability}

We cannot simply introduce imperative write pointers into a functional setting and hope for the best. Instead, we need tools to ensure that only controlled mutations occur. The goal is to allow a structure to remain partially uninitialized temporarily, with write pointers referencing these uninitialized portions. However, once the structure has been fully populated, with a value assigned to each field, it should become immutable to maintain the integrity of the functional paradigm. Put simply, we need a write-once discipline for fields within a structure, which can be mutated through their associated write pointers aka. \emph{destinations}. While enforcing a single-use discipline for destinations at runtime would be unwieldy, we can instead deploy static guarantees through the type system to ensure that every destination will be used exactly once.

In programming language theory, when new guarantees are required, it’s common to design a type system to enforce them, ensuring the program remains well-typed. This is precisely the approach we’ll take here, facilitated by an established type system --- \emph{linear types} --- which can monitor resource usage, especially the number of times a resource is used. Leveraging a linear type system, we can design a language in which structures with holes are first-class citizens, and write pointers to these holes are only usable once. This ensures that each hole is filled exactly once, preserving immutability and making sure that structure are indeed completed before being read.
