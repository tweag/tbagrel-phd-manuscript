 
\chapter{Linear $\lambda$-calculus and Linear Haskell}\label{chap:preli}

\begin{quote}
  In the following we assume that the reader is familiar with the simply-typed lambda calculus, its typing rules, and usual semantics. We also assume some degree of familiarity with the intuitionistic fragment of natural deduction. We kindly refer the reader to~\cite{pierce_tapl_2002} and~\cite{sorensen_natural_curry_howard_2006} for a proper introduction to these notions.
\end{quote}

\section{From $\lambda$-calculus to linear $\lambda$-calculus}

At the end of the 30s, Alonzo Church introduced the \emph{untyped $\lambda$-calculus} as a formal mathematical model of computation. Untyped $\lambda$-calculus is based on the concept of function abstraction and application, and is Turing-complete: in other terms, it has the same expressive power as the rather imperative model of \emph{Turing machines}, introduced in the same decade. The only available objects in the untyped $\lambda$-calculus are pure functions; but they can be used smartly to encode many other kind of data, and as a result they are enough to form a solid model of computation.

In 1940, Church defined a typed variant of its original calculus, denoted \emph{simply-typed $\lambda$-calculus}, or STLC, that gives up Turing-completeness but becomes strongly-normalizing: every well-typed term eventually reduces to a normal form. STLC assign types to terms, and restricts the application of functions to terms of the right type. This restriction is enforced by the typing rules of the calculus.

It has been observed by Howard in 1969~\cite{howard_formulae-as-types_1969} that the simply-typed $\lambda$-calculus is actually isomorphic to the logical framework named \emph{natural deduction}. This observation relates to prior work by Curry where the former also observed the same sort of symmetries between systems on the computational side and on the logical side. These observations have been formalized into the \emph{Curry-Howard isomorphism} theorem, which states that types in a typed $\lambda$-calculus correspond to formulae in a proof system, and that terms correspond to proofs of these formulae. 

The Curry-Howard isomorphism has been later extended to other logics and calculi, and has been a fruitful source of inspiration for research on both the logical and computational side. In that sense, Jean-Yves Girard first introduced Linear Logic in 1987~\cite{girard_original_1987}, and only later studied the corresponding calculus, denoted \emph{linear $\lambda$-calculus}. Linear logic follows from the observation that in some logical presentations, notably the sequent calculus\footnote{Sequent calculus is a very popular logic framework that is an alternative to natural deduction and that has also been introduced by Gentzen in the 30s}, hypotheses are duplicated or discarded explicitly using \emph{structural} rules of the system, named respectively \emph{contraction} and \emph{weakening} (in contrast to natural deduction where all that happens implicitly, as it is part of the meta-theory). As a result, it is possible to track the number of times a formula is used by counting the use of these structural rules. Linear logic take this idea further, and deliberately restricts contraction and weakening, so by default, every hypothesis must be used exactly once. Consequently, logical implication $[[T → U]]$ is not part of linear logic, but is replaced by linear implication $[[T ⊸ U]]$, where $[[T]]$ must be used exactly once to prove $[[U]]$. Linear logic also introduces a modality $\ottstype{!}$, pronounced \emph{of course} or \emph{bang}, to allow weakening and contraction on specific formulae: $[[!T]]$ denotes that $[[T]]$ can be used an arbitrary number of times (we say it is \emph{unrestricted}). We say that linear logic is a \emph{substructural} logic because it restricts the use of structural rules of usual logic.

We present in \cref{fig:linlog} the natural deduction formulation of intuitionistic linear logic (\CILL), as it lends itself well to a computational interpretation as a linear $\lambda$-calculus with usual syntax. We borrow the \emph{sequent style} notation of sequent calculus for easier transition into typing rules of terms later. However, as we are in an intuitionistic setting, rules only derive a single conclusion from a multiset of formulae.

\bgroup
\renewcommand\ottaltinferrule[4]{
  \inferrule*[narrower=0.3,right=#1,#2]
    {#3}
    {#4}
}

All the rules of \CILL, except the ones related to the $\ottstype{!}$ modality, are directly taken (and slightly adapted) from natural deduction. $\ottstype{1}$ denotes the truth constant, $\ottstype{\oplus}$ denotes (additive) disjunction, and $\ottstype{\otimes}$ denotes (multiplicative) conjunction. Hypotheses are represented by multisets $[[P]]$. These multisets keep track of how many times each formula appear in them. The comma operator in $[[P1,P2]]$ is multiset union, so it sums the number of occurrences of each formula in $[[P1]]$ and $[[P2]]$. We denote the empty multiset by $[[{}]]$.

Let's focus on the four rules for the $\ottstype{!}$ modality now. The promotion rule \CILL\CSep\CBangProm{} states that a formula $[[T]]$ can become an unrestricted formula $[[!T]]$ if it only depends on formulae that are themselves unrestricted. This is denoted by the (potentially empty) multiset $[[!P]]$. The dereliction rule \CILL\CSep\CBangDerel{} states that an unrestricted formula $[[!T]]$ can be used in a place expecting a linear formula $[[T]]$. The contraction rule \CILL\CSep\CBangContr{} and weakening rule \CILL\CSep\CBangWeak{} state respectively that an unrestricted formula $[[!T]]$ can be cloned or discarded at any time.

The linear logic system might appear very restrictive, but we can always simulate the usual non-linear natural deduction in \CILL, through the $\ottstype{!}$ modality. Girard gives precise rules for such a translation in Section 2.2.6 of~\cite{girard_linear_1995}, whose main idea is to prefix most formulae with $\ottstype{!}$ and encode the non-linear implication $[[T → U]]$ as $[[!T ⊸ U]]$.

\begin{ottfig}{\caption{Intuitionistic linear logic in natural deduction presentation, sequent-style (\rref*{\CILL})}\label{fig:linlog}}
\bgroup\renewcommand{\ottdrulename}[1]{}
\ottdefnLinMonXXLog{}\egroup
\end{ottfig}

To get more familiar with this system, let's see how we can derive the implication $[[!T ⊸ (T ⨂ T)]]$. In other terms, how we can get two (linear) copies of $[[T]]$s out of an unrestricted $[[T]]$:

\[
\inferrule*[right=\CFun]{
  \inferrule*[right=\CBangContr]{
    \inferrule*[right=\CId]{ }{[[_!T ╟ !T]]} \quad
    \inferrule*[right=\CProd]{
      \inferrule*[right=\CBangDerel]{
        \inferrule*[right=\CId]{ }{[[_!T ╟ !T]]}
      }{[[_!T ╟ T]]} \quad
      \inferrule*[right=\CBangDerel]{
        \inferrule*[right=\CId]{ }{[[_!T ╟ !T]]}
      }{[[_!T ╟ T]]}
    }{[[_!T, _!T ╟ T ⨂ T]]}
  }{[[_!T ╟ T ⨂ T]]}
}{[[{} ╟ !T ⊸ (T ⨂ T)]]}
\]

We use dereliction and identity rules in both branches of the pair, to transform the unrestricted $[[T]]$ into a regular one. That way, the product $[[T ⨂ T]]$ requires the context $[[_!T, _!T]]$ which is exactly of the right form to apply the contraction rule \rref*{\CILL\CSep\CBangContr}. We will see in next section how this derivation translates into a well-typed program.

% mapping a function on a proto-list

% d : 1 + (T x T)
% f : !(T -o U)

% case d of
%   Inl u -> drop f in Inl u
%   Inr x -> case x of (x1, x2) -> dup f as f1, f2 in ((derelict f) x1, (derelict f) x2)

% case f of Many f' ->
% case d of
%   Inl u -> Inl u
%   Inr x -> case x of (x1, x2) -> (f' x1, f' x2)

\section{Linear $\lambda$-calculus: a computational interpretation of linear logic}

There exists several possible interpretations of linear logic as a linear $\lambda$-calculus. The first one, named \emph{monadic} presentation of linear $\lambda$-calculus in~\cite{andreoli_linlog_1992}, and denoted \CLOne{} in this document, is a direct term assignment of the natural deduction rules of \CILL{} given in \cref{fig:linlog}. The syntax and typing rules of this presentation, inspired greatly from the work of~\cite{bierman_linlog_phd_1994}, are given in \cref{fig:linmon-grammar,fig:linmon-ty-term}.

\begin{ottfig}{\caption{Grammar of linear $\lambda$-calculus in monadic presentation (\CLOne)}\label{fig:linmon-grammar}}\[{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
       [[v]] &::=& [[λ x ⟼ u]] \grammsep [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᴇ v]] \\
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[ˢInl t]] \grammsep [[ˢInr t]] \grammsep [[ˢ( t1 , t2 )]] \grammsep [[ᴇ t]] \grammsep [[t t']] \grammsep [[t ; t']] \\
&|\,& [[ t ►case ¹ν { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case ¹ν ( x1 , x2 ) ⟼ u]] \\ &|\,& [[dup t as x1, x2 in u]] \grammsep [[drop t in u]] \grammsep [[derelict t]] \\
&&\\
[[T]], [[U]] &::=& [[T ⊸ U]] \grammsep [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! T]] \\
&&\\
[[P]] &::=& [[{ }]] \grammsep [[{ x ⫶ T }]] \grammsep [[P1 , P2]] \\
\end{array}
}\]\end{ottfig}

\begin{ottfig}{\caption{Typing rules for linear $\lambda$-calculus in monadic presentation (\rref*{\CLOne\CTy})}\label{fig:linmon-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{}
\ottdefnLinMonXXTyXXterm{}\egroup
\end{ottfig}

In \CLOne, $[[P]]$ is now a finite map from variables to types that can be represented as a set of variable bindings $[[{x ⫶ T}]]$. As usual, duplicated variable names are not allowed in a context $[[P]]$. The comma operator now denotes disjoint union for finite maps.

Term grammar for \CLOne{} borrows most of simply-typed lambda calculus grammar. Elimination of unit type $[[①]]$ is made with the $\patu$ operator. Pattern-matching on sum and product types is made with the $\ottkw{case}$ keyword. Finally, we have new operators $\ottkw{dup}$, $\ottkw{drop}$ and $\ottkw{derelict}$ for, respectively, contraction, weakening and dereliction of unrestricted terms with type $[[!T]]$. In addition, the language includes a data constructor for unrestricted terms and values, denoted by $[[ᴇ t]]$ and $[[ᴇ v]]$. Promotion of a term to an unrestricted form is made by direct application of constructor $\ottsctor{Many}$. For easier exposition, we allow ourselves to use syntactic sugar $[[let x ≔ t in u]]$ and encode it as $[[(λ x ⟼ u)(t)]]$.

The derivation of the linear implication $[[!T ⊸ (T ⨂ T)]]$ from previous section now translates into a typing derivation for a well-typed term of type $[[!T ⊸ (T ⨂ T)]]$. Let's see how it goes:

\[
\inferrule*[right=\CFun]{
  \inferrule*[right=\CBangContr]{
    \inferrule*[right=\CId]{ }{[[{x ⫶ !T } ⊪ x : !T]]} \quad
    \inferrule*[right=\CProd]{
      \inferrule*[right=\CBangDerel]{
        \inferrule*[right=\CId]{ }{[[{x1 ⫶ !T } ⊪ x1 : !T]]}
      }{[[{x1 ⫶ !T } ⊪ derelict x1 : T]]} \quad
      \inferrule*[right=\CBangDerel]{
        \inferrule*[right=\CId]{ }{[[{x2 ⫶ !T } ⊪ x2 : !T]]}
      }{[[{x2 ⫶ !T } ⊪ derelict x2 : T]]}
    }{[[{x1 ⫶ !T}, {x2 ⫶ !T} ⊪ ˢ(derelict x1, derelict x2) : T ⨂ T]]}
  }{[[{x ⫶ !T} ⊪ dup x as x1 , x2 in ˢ(derelict x1, derelict x2) : T ⨂ T]]}
}{[[{} ⊪ λ x ⟼ dup x as x1 , x2 in ˢ(derelict x1, derelict x2) : !T ⊸ (T ⨂ T)]]}
\]

This translation is very direct. The only change is that we have to give distinct variable names to hypotheses of the same type inside the typing context, while there was no concept of named copies of a same formula in \CILL. We also see that dealing with unrestricted hypotheses can be rather heavyweight because of the need of manual duplication and dereliction. Next section present a way to circumvent this issue.

\section{Implicit structural rules for unrestricted resources}

% Gentzen, avec le calcul des séquents, on met en évidence le fait que la virgule et le and ne sont pas exactement pareil, et que la virgule à droite est une internalisation du and
% Avec la présentation dyadique, en quelque sorte, on internalise le bang sous la forme du 2e contexte à gauche.

% Les variables sont là pour gérer les règles structurelles, et si les variables ne gèrent plus les règles structurelles, elles ne servent plus à grand chose.

In the monadic presentation \CLOne, the use of unrestricted terms can become very verbose and unhandy because of the need for explicit contraction, weakening and dereliction. A second and equivalent presentation of the linear $\lambda$-calculus, named \emph{dyadic} presentation or \CLTwo, tends to alleviate this burden by using two typing contexts on each judgment, one for linear variables and one for unrestricted variables. The syntax and typing rules of \CLTwo{} are given in \cref{fig:lindya-grammar,fig:lindya-ty-term}.

\begin{ottfig}{\caption{Grammar of linear $\lambda$-calculus in dyadic presentation (\CLTwo)}\label{fig:lindya-grammar}}\[{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
       [[v]] &::=& [[λ x ⟼ u]] \grammsep [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᴇ v]] \\
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[ˢInl t]] \grammsep [[ˢInr t]] \grammsep [[ˢ( t1 , t2 )]] \grammsep [[ᴇ t]] \grammsep [[t t']] \grammsep [[t ; t']] \\
&|\,& [[ t ►case ¹ν { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case ¹ν ( x1 , x2 ) ⟼ u]] \grammsep [[t ►case ¹ν ᴇ x ⟼ u]] \\
&&\\
[[T]], [[U]] &::=& [[T ⊸ U]] \grammsep [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! T]] \\
&&\\
[[P]] &::=& [[{ }]] \grammsep [[{ x ⫶ T }]] \grammsep [[P1 , P2]] \\
[[Ur]] &::=& [[{ }]] \grammsep [[{ x ⫶ T }]] \grammsep [[Ur1 , Ur2]]
\end{array}
}\]\end{ottfig}

\begin{ottfig}{\caption{Typing rules for linear $\lambda$-calculus in dyadic presentation (\rref*{\CLTwo\CTy})}\label{fig:lindya-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{}
\ottdefnLinDyaXXTyXXterm{}\egroup
\end{ottfig}

In \CLTwo{} there are no longer rules for contraction, weakening, and dereliction of unrestricted resources. Instead, each judgment is equipped with a second context $[[Ur]]$ that holds variable bindings that can be used in an unrestricted fashion. The $[[t ►case ¹ν ᴇ x ⟼ u]]$ construct let us access a term of type $[[!T]]$ as a variable $[[x]]$ of type $[[T]]$ that lives in the unrestricted context $[[Ur]]$. It's important to note that $[[t ►case ¹ν ᴇ x ⟼ u]]$ is not dereliction: one can still use $[[x]]$ several times within body $[[u]]$, or recreate $[[t]]$ by wrapping $[[x]]$ back as $[[ᴇ x]]$; while that wouldn't be possible with $[[let x ≔ derelict t in u]]$ of \CLOne. Morally, we can view the pair of contexts $[[P]] ; [[Ur]]$ of \CLTwo{} as a single context $[[P,!Ur]]$ of \CLOne, where $[[!Ur]]$ is the context with the same variable bindings as $[[Ur]]$, except that all types are prefixed by $\ottstype{!}$.

In \CLTwo, contraction for unrestricted resources happens implicitly every time a rule has two subterms as premises. Indeed, the unrestricted context $[[Ur]]$ is duplicated in both premises, unlike the linear context $[[P]]$ that must be split into two disjoint parts. All unrestricted variable bindings are thus propagated to the leaves of the typing tree, that is, the rules with no premises \rref*{\CLTwo\CTy\CSep\CId\textsubscript{Lin}}, \rref*{\CLTwo\CTy\CSep\CId\textsubscript{Ur}}, and \rref*{\CLTwo\CTy\CSep\CUnit}. These three rules discard all bindings of the unrestricted context $[[Ur]]$ that aren't used, performing several implicit weakening steps. 

Finally, this system has two identity rules. The first one, \rref*{\CLTwo\CTy\CSep\CId\textsubscript{Lin}}, is the usual linear identity: it consumes the variable $[[x]]$ present in the linear typing context. Because the linear typing context must be split into disjoint parts between sibling subterms (when a rule has several premises, like the product rule), it means $[[x]]$ can only be used in one of them. The second identity rule, \rref*{\CLTwo\CTy\CSep\CId\textsubscript{Ur}}, is the unrestricted identity: it lets us use the variable $[[x]]$ from the unrestricted typing context in a place where a linear variable is expected, performing a sort of implicit dereliction. Because the unrestricted typing context is duplicated between sibling subterms (again, when a rule has several premises), an unrestricted variable can be used in several of them. For instance, the following derivation is \emph{not valid} because we don't respect the disjointness condition of the comma operator for the resulting typing context of the pair:

\[\inferrule*[right=\CProd]{
  \inferrule*[right=\CId\textsubscript{Lin}]{ }{[[{ x' ⫶ T } ; {} ⊫ x' : T]]} \quad
  \inferrule*[right=\CId\textsubscript{Lin}]{ }{[[{ x' ⫶ T } ; {} ⊫ x' : T]]}
}{
  [[{ x' ⫶ T }, { x' ⫶ T } ; {} ⊫ ˢ(x', x') : T⨂T]]
}\]

but the following derivation is valid, as the unrestricted typing context is duplicated between sibling subterms:

\[\inferrule*[right=\CProd]{
  \inferrule*[right=\CId\textsubscript{Ur}]{ }{[[{} ; { x' ⫶ T } ⊫ x' : T]]} \quad
  \inferrule*[right=\CId\textsubscript{Ur}]{ }{[[{} ; { x' ⫶ T } ⊫ x' : T]]}
}{
  [[{} ; { x' ⫶ T } ⊫ ˢ(x', x') : T⨂T]]
}\]

At this point, we are very close from recreating a term of type $[[!T ⊸ (T ⨂ T)]]$ as we did in previous section. Let's finish the example:

\[
\inferrule*[right=\CFun]{
  \inferrule*[right=\CPatE]{
    \inferrule*[right=\CId\textsubscript{Lin}]{ }{[[{x ⫶ !T} ; {} ⊫ x : !T]]} \quad
    \inferrule*[right=\CProd]{
      \inferrule*[right=\CId\textsubscript{Ur}]{ }{[[{} ; { x' ⫶ T } ⊫ x' : T]]} \quad
      \inferrule*[right=\CId\textsubscript{Ur}]{ }{[[{} ; { x' ⫶ T } ⊫ x' : T]]}
    }{[[{} ; { x' ⫶ T } ⊫ ˢ(x', x') : T⨂T]]}
  }{[[{x ⫶ !T} ; {} ⊫ x ►case ¹ν ᴇ x' ⟼ ˢ(x', x') : T⨂T]]}
}{[[{} ; {} ⊫ λ x ⟼ ༼x ►case ¹ν ᴇ x' ⟼ ˢ(x', x')༽ : !T ⊸ T⨂T]]}
\]

If we had any useless variable binding polluting our unrestricted typing context (let's say $[[Ur]] = [[{y ⫶ !U}]]$), the derivation would still hold without any change at term level; the useless bindings would just be carried over throughout the typing tree, and eliminated at the leaves (via implicit weakening happening in leaf rules):

\[
\inferrule*[right=\CFun]{
  \inferrule*[right=\CPatE]{
    \inferrule*[right=\CId\textsubscript{Lin}]{ }{[[{x ⫶ !T} ; {y ⫶ !U} ⊫ x : !T]]} \quad
    \inferrule*[right=\CProd]{
      \inferrule*[right=\CId\textsubscript{Ur}]{ }{[[{} ; {y ⫶ !U} , { x' ⫶ T } ⊫ x' : T]]} \quad
      \inferrule*[right=\CId\textsubscript{Ur}]{ }{[[{} ; {y ⫶ !U} , { x' ⫶ T } ⊫ x' : T]]}
    }{[[{} ; {y ⫶ !U} , { x' ⫶ T } ⊫ ˢ(x', x') : T⨂T]]}
  }{[[{x ⫶ !T} ; {y ⫶ !U} ⊫ x ►case ¹ν ᴇ x' ⟼ ˢ(x', x') : T⨂T]]}
}{[[{} ; {y ⫶ !U} ⊫ λ x ⟼ ༼x ►case ¹ν ᴇ x' ⟼ ˢ(x', x')༽ : !T ⊸ T⨂T]]}
\]

Managing unrestricted hypotheses is, as we just demonstrated, much easier in the dyadic system \CLTwo{}. Typing trees are shorter, and probably easier to read too.

Also, we don't loose anything by going from \CLOne{} presentation to \CLTwo{}:~\citet{andreoli_linlog_1992} has a detailed proof that \rref*{\CLOne\CTy{}} and \rref*{\CLTwo\CTy{}} are equivalent, in other terms, that a program $[[t]]$ types in the pair of contexts $[[P]] ; [[Ur]]$ in \rref*{\CLTwo\CTy{}} if and only if it types in context $[[P,!Ur]]$ in \rref*{\CLOne\CTy}.

\section{Back to a single context with the graded modal approach}\label{sec:modal-lin}

So far, we only considered type systems that are linear, but that are otherwise fairly standard with respect to usual simply-typed $\lambda$-calculus. Anticipating on our future needs, further in this document we'll need to carry more information and restrictions throughout the type system than just linearity alone.

Naively, we could just multiply the typing contexts, for each new modality that we need. The problem is, that approach is not really scalable: at the end we need one context per possible combination of modalities (if modalities represent orthogonal principles), so it grows really quickly.

In fact, without thinking too far away, we already have a problem if we want a finer control over linearity. What if we want to allow variables to be used a fixed number of times that isn't just one or unlimited? In~\cite{girard_bounded_1992}, Girard considers the \emph{bounded} extension of linear logic, where the number of uses of hypotheses can be restricted to any value $[[m]]$ ---named \emph{multiplicity}--- instead of being just linear or unrestricted. For that he extends the $\ottstype{!}$ modality with an index $[[m]]$ that specifies how many times an hypothesis should be used. We say that $\ottstype{!}_{[[m]]}$ is a \emph{graded modality}.

Having a family of modalities $\left(\ottstype{!}_{[[m]]}\right)_{[[m]] \in \ottsmode{M}}$ with an arbitrary number of elements instead of the single $\ottstype{!}$ modality of original linear logic means that we cannot really have a distinct context for each of them as in the dyadic presentation. One solution is to go back to the monadic presentation, where variables carry their modality on their type until the very end when they get used. We would also need a new operator to go from $\ottstype{!}_{[[m]]} [[T]]$ to a pair $\ottstype{!}_{[[m]]\ottsmode{-1}} [[T]], [[T]]$ so that we can extract a single use from a value wrapped in a modality allowing several uses. Actually, it gets hairy and unpractical very fast.

Fortunately, there's a way out of this. Instead of having $[[m]]$ be part of the modality and thus of the type of terms, we can bake it in as an annotation on variable bindings. In place of $[[x]]:[[S]],[[y]]:[[! m T]],[[z]]:[[! n U]] \pmb{\vdash} \ldots$, we can have $[[x]]:_{\ottsmode{1}} [[S]], [[y]]:_{[[m]]} [[T]], [[z]]:_{[[n]]} [[U]] \pmb{\vdash}$. We'll call the new annotations on bindings \emph{modes}. Note that every binding is equipped with a mode $[[m]]$, even linear bindings that previously didn't have modalities on their types\footnote{This uniform approach where every binding receives a mode seems to originate from~\cite{ghica_bounded_2014} and~\cite{petricek_coeffects_2014}.}. Now, we can completely encode the restrictions and rules of a linear type system by defining operations on modes and by extension, on typing contexts, and use these operations in typing rules, instead of needing extra operators such as $\ottkw{derelict}$ or $\ottkw{dup}$ that need their own typing rules and have to be used explicitly by the user. With the modal approach, we recover a system, like the dyadic one, in which contraction, weakening, and dereliction can be made conveniently implicit, without loosing any control power over resource use, and with no explosion of the number of contexts!

Moreover, we build on the key insight, which seem to originate with~\cite{ghica_bounded_2014}, that equipping the set of modes with a semiring structure is sufficient to express, algebraically, all the typing context manipulation that we need. The idea is to have two operations on modes: \emph{times} $\ottsmode{\cdot}$ that represents what happens to modes when there is composition (e.g. for linearity, if function $[[f]]$ uses its argument $\ottsmode{2}$ times and $[[g]]$ too, then $[[f (g x)]]$ uses $[[x]]$ $\ottsmode{2\cdot 2}$ times), while \emph{plus} $\ottsmode{+}$ describes what happens to modes when a same variable is used in two (or more) subterms (if subterm $[[t]]$ uses $[[x]]$ $\ottsmode{2}$ times and $[[u]]$ uses $[[x]]$ $\ottsmode{3}$ times, then $[[t ; u]]$ uses $[[x]]$ $\ottsmode{2+3}$ times). By abstracting away the structural restrictions of the type system as a mode system with a semiring structure, we make the type system much more scalable: if we were to enrich the type system, and if we are able to express the extension as a new semiring, then we almost don't have to modify the typing rules; just the underlying mode semiring.

We then lift the mode operators to variable bindings and typing contexts in the following way:

{\figtextsize
\bgroup
\renewcommand\tabcolsep{2pt}

\begin{tabular}[c]{rclcc@{\qquad}l}
  $[[n]]$ &$\cdot$& $[[{}]]$ & $\btriangleq$ & $[[{}]]$\\
  $[[n]]$ &$\cdot$& $[[({ x : m T },P)]]$ & $\btriangleq$ & $[[({ x : n · m T }),n·P]]$\\
\end{tabular}

\begin{tabular}[c]{rclcc@{\qquad}l}
  $[[{}]]$ &$+$& $[[P]]$ & $\btriangleq$ & $[[P]]$\\
  $[[({ x : m T }, P1)]]$ &$+$& $[[P2]]$ & $\btriangleq$ & $[[{ x : m T },(P1+P2)]]$ & \textrm{if $[[x]]\notin[[P2]]$}\\
  $[[({ x : m T }, P1)]]$ &$+$& $[[({ x : m' T }, P2)]]$ & $\btriangleq$ & $[[{x : m+m' T}, (P1+P2)]]$
\end{tabular}

\egroup
}

We'll now show what the concrete \emph{modal} presentation for intuitionistic $\lambda$-calculus or \CLm{} looks like. We use a very simple ringoid\footnote{We still get all the algebraic benefits of~\cite{ghica_bounded_2014} even without a zero for our structure.} for modes, that just models linearity equivalently as the previous presentations \CLOne and \CLTwo{}: $[[˥]]$ for variables that must be managed in a linear fashion, and $[[ɷ]]$ for unrestricted ones. We give the following operation tables:

\begin{center}
\begin{tabular}{|c||c|c|}\hline
$\ottsmode{+}$ & $[[¹]]$ & $[[ω]]$ \\\hhline{|=#=|=|}
$[[¹]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hspace{1cm}
\begin{tabular}{|c||c|c|}\hline
$[[·]]$        & $[[¹]]$ & $[[ω]]$ \\\hhline{|=#=|=|}
$[[¹]]$        & $[[¹]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\end{center}

Now that we have a completely defined the ringoid for modes, we can move to the grammar and typing rules of \CLm, given in \cref{fig:linmod-grammar,fig:linmod-ty-term}. The main change compared to \CLTwo{} is that the constructor for $[[! m T]]$ is now $[[ˢᴇ m t]]$ (instead of $[[ᴇ t]]$ for $[[! T]]$).

\begin{ottfig}{\caption{Grammar of linear $\lambda$-calculus in modal presentation (\CLm)}\label{fig:linmod-grammar}}\[{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
       [[v]] &::=& [[ˢλ x m ⟼ u]] \grammsep [[()]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[( v1 , v2 )]] \grammsep [[ᴇ n v]] \\
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[ˢInl t]] \grammsep [[ˢInr t]] \grammsep [[ˢ( t1 , t2 )]] \grammsep [[ˢᴇ n t]] \grammsep [[t t']] \grammsep [[t ; t']] \\
&|\,& [[ t ►case ¹ν { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t ►case ¹ν ( x1 , x2 ) ⟼ u]] \grammsep [[t ►case ¹ν ᴇ n x ⟼ u]] \\
&&\\
[[T]], [[U]] &::=& [[T m → U]] \grammsep [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! n T]] \\
[[m]], [[n]] &::=& [[˥]] \grammsep [[ɷ]] \\
&&\\
[[P]] &::=& [[{ }]] \grammsep [[{ x : m T }]] \grammsep [[P1 , P2]]\\
\end{array}
}\]\end{ottfig}

\begin{ottfig}{\caption{Typing rules for linear $\lambda$-calculus in modal presentation (\rref*{\CLm\CTy})}\label{fig:linmod-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{}
\ottdefnLinModXXTyXXterm{}\egroup
\end{ottfig}

In \CLm{} we're back to a single identity rule \rref*{\CLm\CTy\CSep\CId}, that asks for $[[x]]$ to be in the typing context with a mode $[[m]]$ that must be \emph{compatible with a single linear use} (we note $[[˥ ⥶ m]]$). In our ringoid, with only two elements, we have both $[[˥ ⥶ ˥]]$ and $[[˥ ⥶ ɷ]]$, so $[[x]]$ can actually have any mode (either linear or unrestricted, so encompassing both \rref*{\CLTwo\CTy\CSep\CId\textsubscript{Lin}} and \rref*{\CLTwo\CTy\CSep\CId\textsubscript{Ur}}), but in more complex modal systems, not all modes have to be compatible with the unit of the ringoid\footnote{See \cref{ssec:ty-term} for a case of a ringoid where not all modes are compatible with the unit.}. Rules \rref*{\CLm\CTy\CSep\CId} and \rref*{\CLm\CTy\CSep\CUnit} also allow to discard any context composed only of unrestricted bindings, denoted by $[[ɷ·P]]$ (equivalent to notation $[[!P]]$ in \CLOne), so they are performing implicit weakening as in \CLTwo.

Every rule of \CLm{} that mentions two subterms uses the newly defined $+$ operator on typing contexts in the conclusion of the rule. If a same variable $[[x]]$ is required in both $[[P1]]$ and $[[P2]]$ (either with mode $[[˥]]$ or $[[ɷ]]$), then $[[P1+P2]]$ will contain binding $[[{x : ɷ T}]]$. Said differently, the parent term will automatically deduce whether $[[x]]$ needs to be linear or unrestricted based on how (many) subterms use $[[x]]$, thanks to the $+$ operator. It's a vastly different approach than the linear context split for subterms in \CLOneOrTwo{} and unrestricted context duplication for subterms in \CLTwo. I would argue that, thanks to the algebraic properties of the mode semiring, it makes the presentation of the system smoother (at the theoretical level at least; implementing type checking for such a linear type system can be trickier).

Much as in \CLTwo, the exponential modality $\ottstype{!}_{[[m]]}$ is eliminated by a $[[t ►case ¹ν ᴇ m x ⟼ u]]$ expression, that binds the scrutinee to a new variable $[[x]]$ with mode $[[m]]$. The original boxed value can be recreated if needed with $[[ˢᴇ m x]]$; indeed, as in \CLTwo, elimination rule for $\ottstype{!}_{[[m]]}$ is \emph{not} dereliction.

The last specificity of this system is that the function arrow $\ottstype{\multimap}$ now has a mode $[[m]]$ to which it binds its argument. Previously, in \CLOneOrTwo{}, the argument of a function was always linear, in the sense that the corresponding variable binding in the body of the function was linear. Here, in \CLm{}, we can bind argument at any mode $[[m]]$ (and the rule \rref*{\CLm\CTy\CSep\CApp} for function application reflects that: the typing context $[[P1]]$ for the argument passed to the function is multiplied by $[[m]]$ in the conclusion of the rule). Actually that doesn't change the expressivity of the system compared to previous presentations; we would have been just fine with only a purely linear arrow, but because we have to assign a mode to any variable binding in this presentation anyway, then it makes sense to allow this mode $[[m]]$ to be whatever the programmer wants, and not just default to $[[˥]]$.

Let's update our running example. The term syntax doesn't change, we only see an evolution in typing contexts (as we now have only one, with mode annotations, instead of two):

\[
\inferrule*[right=\CFun]{
  \inferrule*[right=\CPatE]{
    \inferrule*[right=\CId]{[[˥]] ⥶ [[˥]]}{[[{y : ɷ U}, {x : ˥ !T} ⫢ x : !T]]} \quad
    \inferrule*[right=\CProd]{
      \inferrule*[right=\CId]{[[˥]] ⥶ [[˥]]}{[[{ x' : ˥ T } ⫢ x' : T]]} \quad
      \inferrule*[right=\CId]{[[˥]] ⥶ [[˥]]}{[[{ x' : ˥ T } ⫢ x' : T]]}
    }{[[{ x' : ɷ T } ⫢ ˢ(x', x') : T⨂T]]}
  }{[[{y : ɷ U}, {x : ˥ !T} ⫢ x ►case ¹ν ᴇ ɷ x' ⟼ ˢ(x', x') : T⨂T]]}
}{[[{y : ɷ U} ⫢ ˢλ x ˥ ⟼ ༼x ►case ¹ν ᴇ ɷ x' ⟼ ˢ(x', x')༽ : !T ˥ → T⨂T]]}
\]

Now the unrestricted, unused part of the typing context ($[[{y : ɷ U}]]$ in this case) doesn't have to be propagated towards all leaves, but only towards at least one of them (though we can still choose to propagate it towards all of them), thanks to the way the $+$ works on contexts. We also could have chosen to type $[[x']]$ at mode $[[ɷ]]$ in the two identical instances of the \rref*{\CLm\CTy\CSep\CId} rule at the top of the tree; it has no impact on the rest of the derivation as both $[[˥ + ˥]]$ and $[[ɷ + ɷ]]$ gives $[[ɷ]]$ as a result.

Finally, we can give an alternative and more concise version of this function, taking advantage of the \emph{mode on function arrow} feature that we just discussed above. This new version has type $[[T ɷ → T⨂T]]$ instead of $[[!T ˥ → T⨂T]]$:

\[
\inferrule*[right=\CFun]{
    \inferrule*[right=\CProd]{
      \inferrule*[right=\CId]{[[˥]] ⥶ [[˥]]}{[[{y : ɷ U},{ x' : ˥ T } ⫢ x' : T]]} \quad
      \inferrule*[right=\CId]{[[˥]] ⥶ [[˥]]}{[[{y : ɷ U},{ x' : ˥ T } ⫢ x' : T]]}
    }{[[{y : ɷ U}, { x' : ɷ T } ⫢ ˢ(x', x') : T⨂T]]}
}{[[{y : ɷ U} ⫢ ˢλ x' ɷ ⟼ ༼ˢ(x', x')༽ : T ɷ → T⨂T]]}
\]

% STOPPED THERE

\section{Deep modes: projecting modes through fields of data structures}

In original linear logic from Girard (see \CILL{} and presentation \CLOne{} above), there is only a one-way morphism between $[[!T ⨂ !U]]$ and $[[!(T⨂U)]]$; we cannot go from $[[!(T⨂U)]]$ to $[[!T ⨂ !U]]$. In other terms, an unrestricted pair $[[!(T⨂U)]]$ doesn't allow for unrestricted use of its components. The pair can be duplicated or discarded at will, but to be used, it needs to be derelicted first, to become $[[T⨂U]]$, that no longer allow to duplicate or discard any of $[[T]]$ or $[[U]]$. As a result, $[[T]]$ and $[[U]]$ will have to be used exactly the same number of times, even though they are part of an unrestricted pair!

Situation is no different in \CLTwo{} (resp. \CLm): although the pair of type $[[!(T⨂U)]]$ can be bound in an unrestricted binding $[[{x ⫶ T⨂U}]] \in [[Ur]]$ (resp. $[[{x : ɷ T⨂U}]]$ in \CLm) and be used as this without need for dereliction, when it will be pattern-matched on (through the rule \rref*{\CLTwoOrm\CTy\CSep\CPatP}), implicit dereliction will still happen, and linear-only bindings will be made for its components: $[[{x1 ⫶ T},{x2 ⫶ U}]]$ in the linear typing context (resp. $[[{x1 : ˥ T},{x2 : ˥ U}]]$ in \CLm).

Let's say we want to write the $\ottkw{fst}$ function, that extracts the first element of a pair. In a non-linear language, we would probably write it that way:

\codehere{
\newoperator
  {\ottkw{fst}_{\ottkw{nl}}}{[[T⨂U → T]]}
  {\ottkw{fst}_{\ottkw{nl}}}{[[λ x ⟼ x ►case ¹ν (x1, x2) ⟼ x1]]}
}

But in \CLm{}, or even \CLOneOrTwo{}, we are not allowed to do this. A naive idea would be to give the function the signature $[[T⨂U ɷ → T]]$, but that still doesn't help: as we just said, even with an unrestricted pair, we have to consume both elements of the pair the same number of times. The only valid solution is to indicate, in the signature, that the second element of the pair --that we discard-- will not be used linearly:

\codehere{
\newoperator
  {\ottkw{fst}_{\ottkw{m}}}{[[T⨂(!ɷ U) ˥ → T]]}
  {\ottkw{fst}_{\ottkw{m}}}{\!\!\!\begin{array}[t]{l}[[
ˢλ x ˥ ⟼ x ►case ¹ν (x1, ux2) ⟼⮒
‥‥༼ux2 ►case ¹ν ᴇ ɷ x2 ⟼ x1༽
]]\end{array}}
}

It's often desirable to lift this limitation in a real programming language, and let the $\ottstype{!}_{[[m]]}$ modality, and more generally, the modes of variables, distribute over the other connectives like $\ottstype{\otimes}$ or $\ottstype{\oplus}$. The main motivation for \emph{deep modes} (in the sense that they propagate throughout a data structure) is that they make it more convenient to write non-linear programs in a linear language.

Indeed, in a modal linear $\lambda$-calculus with deep modes (further denoted \CLdm), functions of type $[[T ɷ → U]]$ gain exactly the same expressive power as the ones with type $[[T → U]]$ in STLC. In other terms, any program that is valid in STLC but doesn't abide by linearity can still type in \CLdm{} without any restructuration needed, we just need to add the proper mode annotations! This was clearly not the case in a faithful interpretation of Girard's linear logic, like we just showed before with limitations of \CLm{} when implementing $\ottkw{fst}_{\ottkw{m}}$.

Revisiting this $\ottkw{fst}$ function, with deep modes in \CLdm{}, we are able to write:

\codehere{
\newoperator
  {\ottkw{fst}_{\ottkw{dm}}}{[[T⨂U ɷ → T]]}
  {\ottkw{fst}_{\ottkw{dm}}}{[[ˢλ x ɷ ⟼ x ►case ɷ (x1, x2) ⟼ x1]]}
}

This is precisely the same implementation as $\ottkw{fst}_{\ottkw{nl}}$, with just extra mode annotations!

Adding deep modes to a practical linear language is not a very original take; it has been done in Linear Haskell~\cite{bernardy_linear_2018} and in recent work from~\citet{lorenzen_oxidizing_2024}.
With deep modes, we get the following equivalences:

\codehere{\!\!\!\begin{array}[t]{rcl}
[[!m (T⨂U)]] &\simeq& [[(!m T) ⨂ (!m U)]] \\
[[!m (T⨁U)]] &\simeq& [[(!m T) ⨁ (!m U)]] \\
[[!m (!n T)]] &\simeq& [[!(m·n) T]]
\end{array}}

The only change needed on the grammar between \CLm{} and \CLdm{} is that the $\ottkw{case}$ constructs in \CLdm{} take a mode $[[m]]$ to which they consume the scrutinee , which is propagated to the variable bindings for the field(s) of the scrutinee in the body of the $\ottkw{case}$. The new typing rules for $\ottkw{case}$ are presented in \cref{fig:lindeepmod-ty-term}, all the other rules of linear $\lambda$-calculus with deep modes, \CLdm, are identical to those of \cref{fig:linmod-ty-term}.

\begin{ottfig}{\caption{Altered typing rules for deep modes in linear $\lambda$-calculus in modal presentation (\rref*{\CLdm\CTy})}\label{fig:lindeepmod-ty-term}}
\bgroup\renewcommand{\ottdrulename}[1]{}
\ottdefnLinDeepModXXTyXXterm{}\egroup
\end{ottfig}

For these new $\ottkw{case}$ rules, we observe that the typing context $[[P1]]$ in which the scrutinee types is scaled by $[[m]]$ in the conclusion of these rules. This is very similar to the application rule \rref*{\CLm\CTy\CSep\CApp} : it makes sure that the resources required to type $[[t]]$ are consumed $[[m]]$ times if we want to use $[[t]]$ at mode $[[m]]$. Given that $[[{ x2 : ˥ T2 } ⊢ ˢ((), x2) : ① ⨂ T2]]$ (the pair uses $[[x2]]$ exactly once), if we want to extract the pair components with mode $[[ɷ]]$ to drop $[[x2]]$ (as in $\ottkw{fst}_{\ottkw{dm}}$), then $[[ˢ((), x2) ►case ɷ (x1, x2) ⟼ x2]]$ will require context $[[ɷ·({ x2 : ˥ T2 })]]$ i.e. $[[{ x2 : ɷ T2 }]]$. In other terms, we cannot use parts of a structure having dependencies on linear variables in an unrestricted way (as that would break linearity).

The linear $\lambda$-calculus with deep modes, \CLdm, will be the basis for our core contribution that follows in next chapter: the destination calculus \destcalculus{}.

\section{Semantics of linear $\lambda$-calculus (with deep modes)}\label{sec:lincalc-sem}

Many semantics presentations exist for lambda calculus. In \cref{fig:linsem} we present a small-step reduction system for \CLdm, in \emph{reduction semantics} style inspired from~\cite{felleisen_calculi_1987} and subsequent~\cite{danvy_refocusing_2004,biernacka_syntactic_2007}, that is, a semantics in which the evaluation context $[[C]]$ is manipulated explicitly and syntactically as a stack.

We represent a running program by a pair $[[ C[t] ]]$ of an evaluation context $[[C]]$, and a term $[[t]]$ under focus. We call such a pair $[[ C[t] ]]$ a \emph{command}, borrowing the terminology from~\citet{herbelin_curien_2000}.

\begin{ottfig}{\caption{Small-step semantics for \CLdm{} (\CLdm\CSemSuff)}\label{fig:linsem}}
\begin{augmentwidth}{2cm}
\bgroup
\renewcommand{\ottdrulename}[1]{}
\renewcommand\arraystretch{1.5}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \setbox0=\hbox{\ensuremath{#3}\foreverunspace}\ifdim\wd0=0pt ~ \else$\star$\fi & \text{\textsc{#1}} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{lll}}
  {\endarray$}
\makeatother
\drules{$[[C [ t ] ⟶ C' [ t' ] ]]$}{Small-step evaluation}{%
LinFocus-Left,
LinUnfocus-Left,
LinFocus-Right,
LinUnfocus-Right,
LinFocus-ProdOne,
LinUnfocus-ProdOne,
LinFocus-ProdTwo,
LinUnfocus-ProdTwo,
LinFocus-Exp,
LinUnfocus-Exp,
LinFocus-AppOne,
LinUnfocus-AppOne,
LinFocus-AppTwo,
LinUnfocus-AppTwo,
LinRed-App,
LinFocus-PatU,
LinUnfocus-PatU,
LinRed-PatU,
LinFocus-PatS,
LinUnfocus-PatS,
LinRed-PatL,
LinRed-PatR,
LinFocus-PatP,
LinUnfocus-PatP,
LinRed-PatP,
LinFocus-PatE,
LinUnfocus-PatE,
LinRed-PatE}
\egroup

\vspace{-0.5cm}

\begin{center}
$\star$~:~only allowed if the term that would become the new focus is not already a value
\end{center}
\end{augmentwidth}
\end{ottfig}

\egroup

Small-step evaluation rules are of three kinds:
\begin{itemize}
\item focusing rules (F) that split the current term under focus in two parts: an evaluation context component that is pushed on the stack $[[C]]$ for later use, and a subterm that is put under focus;
\item unfocusing rules (U) that recreate a larger term once the term under focus is a value, by popping the most recent evaluation component from the stack and merging it with the value;
\item contraction rules (C) that do not operate on the stack $[[C]]$ but just transform the term under focus when it's a redex.
\end{itemize}

To have a fully deterministic and straightforward reduction system, focusing rules can only trigger when the subterm that would be focused is not already a value (denoted by $\star$ in \cref{fig:linsem}).

Data constructors only have focusing and unfocusing rules, as they do not describe computations that can be reduced. In that regard, $[[ˢᴇ m t]]$ is treated as an unary data constructor. Once a data constructor is focused, it is evaluated fully to a value form.

In most cases, unfocusing and contraction rules could be merged into a single step. For example, a contraction could be triggered as soon as we have $[[(C ∘ (⬜ ►case m (x1,x2) ⟼ u))[(v1, v2)] ]]$, without the need to recreate the term $[[(v1, v2) ►case m (x1,x2) ⟼ u]]$. However, I prefer the presentation in three distinct steps, that despite being more verbose, clearly shows that contraction rules do not modify the evaluation context.

The rest of the system is very standard. In particular, contraction rules of \CLdm{} --- that capture the essence of the calculus --- are very similar to those of lambda-calculi with sum and product types (and strict evaluation strategy).

For instance, let's see the reduction for function application. We reuse the same example of our function with signature $[[!T ⊸ (T ⨂ T)]]$, here applied to the argument $[[ᴇ ɷ (Inl ())]]$:

\[\bgroup\renewcommand{\arraystretch}{1.5}\begin{array}{clrl}
&                 [[ ⬜ [ (ˢλ x ˥ ⟼ ༼x ►case ¹ν ᴇ ɷ x' ⟼ ˢ(x', x')༽) (ᴇ ɷ (Inl ())) ] ]] & \\
\longrightarrow & [[ ⬜ [ (ᴇ ɷ (Inl ()) ►case ¹ν ᴇ ɷ x' ⟼ ˢ(x', x')) ] ]] & \rref*{\CApp\CRed} &\text{ with substitution } [[x]] \assigneq [[ᴇ ɷ (Inl ())]] \\
\longrightarrow & [[ ⬜ [ ˢ(Inl (), Inl ()) ] ]] & \rref*{\CPatE\CRed} &\text{ with substitution } [[x']] \assigneq [[Inl ()]]
\end{array}\egroup\]

Because we have a function in value form already, with simple body, applied to a value, we don't need to focus into a subterm and unfocus back later; we see only see a succession of contraction steps.

A command of the form $[[ ⬜ [ v ] ]]$ with $[[v]]$ a value is the stopping point of the reduction for all well-typed programs. If we have a non-value term $[[t]]$ in $[[⬜ [ t ] ]]$ then a focusing or contraction step should trigger; and if we have a non-empty evaluation context $[[C]]$ in $[[C [ v ] ]]$ then an unfocusing step should trigger. Only ill-shaped term or value can cause the reduction to be stuck, which is a symptom of a wrongly-typed program.

\section{Linear Types in Haskell}\label{sec:intro-linearity}

It's no surprise that we intend to implement prototypes in a industrial functional programming language along our journey in the destination-passing world. And Haskell is a suitable target for that, thanks to its support for linear types.

Linear Haskell~\cite{bernardy_linear_2018} is a language extension for the Glasgow Haskell Compiler (GHC) that introduces the linear function arrow, \mintinline{haskellc}/t ⊸ u/ and modifies the type checker of GHC so that it can enforce linearity requirements. A linear function of type \mintinline{haskellc}/t ⊸ u/ guarantees that the argument of the function will be consumed exactly once when the result of the function is consumed exactly once. The regular function arrow \mintinline{haskellc}/t → u/ is still available when Linear Haskell is enabled; this one doesn't guarantee how many times its argument will be consumed when its result is consumed once. Actually Linear Haskell is based on a modal formalism, like \CLdm{}, so there is also a multiplicity-polymorphic arrow, \mintinline{haskellc}/t %m → u/, much like $[[T m → U]]$; so \mintinline{haskellc}/t ⊸ u/ and \mintinline{haskellc}/t → u/ are sugar for \mintinline{haskellc}/t %1 → u/ and \mintinline{haskellc}/t %ω → u/ respectively.

A value is said to be \emph{consumed once} (or \emph{consumed linearly}) when it is pattern-matched on and its sub-components are consumed once; or when it is passed as an argument to a linear function whose result is consumed once. A function is said to be \emph{consumed once} when it is applied to an argument and when the result is consumed exactly once. We say that a variable \mintinline{haskellc}/x/ is \emph{used linearly} in an expression \mintinline{haskellc}/u/ when consuming \mintinline{haskellc}/u/ once implies consuming \mintinline{haskellc}/x/ exactly once. This is no more than a practical view of the typing rules given for \CLdm{}.

\paragraph{Unrestricted Values}

Linear Haskell introduces a wrapper named \mintinline{haskellc}/Ur/ which is used to indicate that a value in a linear context doesn't have to be used linearly. \mintinline{haskellc}/Ur t/ is the Haskell equivalent for $[[! T]]$ in \CLOneOrTwo{} or $[[! ɷ T]]$ in \CLdm{}, and there is an equivalence between \mintinline{haskellc}/Ur t ⊸ u/ and \mintinline{haskellc}/t → u/. As in \CLTwo{} and \CLdm{}, we can pattern-match on a term of type \mintinline{haskellc}/Ur t/ with \mintinline{haskellc}/case term of ¤Ur x ⊸ term'/ to obtain an unrestricted variable binding \mintinline{haskellc}/x/ of type \mintinline{haskellc}/t/.

As usual with linear type systems, only values already wrapped in \mintinline{haskellc}/Ur/ or coming from the left of a non-linear arrow can be put in another \mintinline{haskellc}/Ur/ without breaking linearity. This echoes rules \rref*{\CLOne\CTy\CSep\CBangProm} or \rref*{\CLTwoOrm\CTy\CSep\CExp} where a term wrapped in $\ottsctor{Many}$ or $\expcons{[[ɷ]]}$ must only depend on an unrestricted context. The only exceptions to this rule are terms of types that implement the \mintinline{haskellc}/Movable/ typeclass\footnote{The \mintinline{haskellc}/Movable/ typeclass is not part of the Linear Haskell language extension, but instead defined in the \texttt{linear-base} library.} such as \mintinline{haskellc}/Int/ or \mintinline{haskellc}/()/. \mintinline{haskellc}/Movable/ provides the function \mintinline{haskellc}/move ⩴ t ⊸ Ur t/, that is, unconditional promotion to \mintinline{haskellc}/Ur/ for types implementing this typeclass.

% Arnaud: this sentence sounds premature to me
% Destinations are represented by the type \mintinline{haskellc}/UDest t/, where \mintinline{haskellc}/t/ stands for the type of the associated hole. \mintinline{haskellc}/UDest/s are meant to be managed linearly, so don't implement \mintinline{haskellc}/Movable/.

\paragraph{Operators}

Some Haskell operators and notations are often used in the rest of this article:

\mintinline{haskellc}/(;) ⩴ () ⊸ u ⊸ u/ is used to chain a linear operation returning \mintinline{haskellc}/()/ with one returning a value of type \mintinline{haskellc}/u/ without breaking linearity. It's the Haskell equivalent of \rref*{\CLdm\CTy\CSep\CPatU}. It isn't part of the core Haskell syntax, but can be defined simply as \mintinline{haskellc}/case term1 of () -> term2/.

\mintinline{haskellc}/Class ⇒ .../ is notation for typeclass constraints (resolved implicitly by the compiler).

\mintinline{haskellc}/@t/ in \mintinline{haskellc}/f @t .../ is a type application; it allows to specify an explicit type for polymorphic functions.

In code excerpts, types are in blue, while terms, variables, and data constructors (sometimes having the same name as the type they belong to, like \mintinline{haskellc}/Ur/ (type) / \mintinline{haskellc}/¤Ur/ (data constructor)) are in black, to stay as consistent as possible with previous chapters. Unfortunately, while type variables $[[T]],[[U]]$ are in uppercase in our formalism, we have no choice than to use lowercase letters in Haskell \mintinline{haskellc}/t/, \mintinline{haskellc}/u/.

\section{Uniqueness and Linear Scopes}\label{sec:linear-scopes}

In the introduction we said that we will use linear type systems to control the number of times a resource --in particular a destination-- is used. This is indeed a promising idea in order to forgo monads entirely when doing resource management, for a less contaminating programming style\ldots{} or so it seems.

But linear type systems, at first, may seem insufficient to do that job fully. In a linear type system, we can only promise what we do to a value we receive. We cannot say much about what has been done to it previously. And thus, we cannot ensure it has been used exactly once \emph{overall}.

For instance, given a function of type \mintinline{haskellc}/t ⊸ u/, if we apply it to an argument of type \mintinline{haskellc}/t/, we know we won't duplicate this value of type \mintinline{haskellc}/t/ (as long as we use the result of type \mintinline{haskellc}/u/ exactly once). But we can't say whether the \mintinline{haskellc}/t/ has been duplicated or not beforehand: that's a major issue.

Stating facts about \emph{the past} of a value is the land of \emph{uniqueness typing} or unique type systems. In a unique type system, the main property is not how many times a function uses a value to produce its output, but rather, whether a value has been aliased or not. Uniqueness (or non-uniqueness) is a property on values, while linearity is a property on functions\footnote{At least in the presentations above that follows from Girard's Linear Logic. There exists approaches of Linear Type systems that are more focused on the programming side, for which linearity is a property of values and their types, not functions.}. Having uniqueness or non-uniqueness property on values allows for easier control of resources that should always stay unique during their lifetime. The most common example is efficient implementation of functional arrays. If an array is handled uniquely and isn't shared, then we can implement array updates as in-place memory mutations~\cite{sergey_linearity_uniqueness_2022}. By being able to state uniqueness of values, statically through the type system, we can allow predictable compiler optimizations.

The thing is, uniqueness typing doesn't have a theoretical ground as rich as linear types. Unlike linearity, uniqueness properties have first been studied on the computational side and only later studied on the logical side, with~\cite{harrington_uniqueness_2006}. There are also two downsides of uniqueness typing for what we want to do later. First, there isn't an easy way with unique type system to force the use of something (and we definitely want a way to force a destination to be used and not just be dropped). We can state or prove that it isn't aliased, but that's all. Secondly, and more importantly perhaps, higher-order functions (capturing lambda abstractions) are way more finicky in unique type systems, as detailed in Section 5 of~\cite{harrington_uniqueness_2006} or mentioned briefly in~\cite{sergey_linearity_uniqueness_2022} (that's one reason why they prefer a linear system as a basis with a uniqueness topping instead of a uniqueness system with a linearity topping).

Actually,~\citet{sergey_linearity_uniqueness_2022} argue that modern programming languages would benefit from integrating both linear and uniqueness typing, as linearity and uniqueness appear to be half-dual, half-complementary notions that together are enough for a very precise control of resource usage in a language.

In \cref{chap:dps-haskell} however, our main target language will be Haskell, which doesn't have uniqueness typing, only linear types, as detailed in \cref{sec:intro-linearity}. Fortunately, we can emulate uniqueness control in a language equipped with linear types, using \emph{linear scopes} or the \emph{scope function trick}, as one might call it.

\paragraph{Linear scopes}

As we said before, in Linear Haskell, as in most linear type systems, functions can be made to use their arguments linearly or not. In other terms, linearity is a contract that links the input and output of a function. But we cannot say anything a priori about what happened to the value passed as a parameter before the function call, or what happens to the function result after. Linearity control can only be enforced on \emph{variables}.

Let say we want to design a file API without monads (that's actually one interesting use of linear/unique types). Assuming the following simplified API, one could write this small program:

\begin{unbreakable}
{\figtextsize
\begin{minted}{haskellc}
openFile ⩴ String ⊸ File
closeFile ⩴ File ⊸ ()

file ⩴ File  -- no way to force this value to be used exactly once
file = openFile "test.txt"

nonLinearUseButValid ⩴ () =
  closeFile file ; closeFile file  -- valid even if file is consumed twice
\end{minted}
}
\end{unbreakable}

With call-by-value or call-by-need semantics, the side-effects of \mintinline{haskellc}/openFile/ will be produced only once, while the side effects of \mintinline{haskellc}/closeFile/ will be produced twice, resulting in an error.

The solution to that is to forbid the consumer from creating and directly accessing a value of the resource type for which we want to enforce linearity/uniqueness. Instead, we force the consumer of a resource to pass a continuation representing what they want to do on the resource, so that we can check through its signature that it is indeed a linear continuation:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
withFile ⩴ String ⊸ (File ⊸ t) ⊸ t
closeFile ⩴ File ⊸ ()
nonLinearUseAndEffectivelyRejected ⩴ () =
  withFile "test.txt" (\file → closeFile °\mold{file}° ; closeFile °\mold{file}°) -- not linear
\end{minted}
}
\end{unbreakable}

The \mintinline{haskellc}/File/ type is in positive position in the signature of \mintinline{haskellc}/withFile/, so the function \mintinline{haskellc}/withFile/ should somehow know how to produce a \mintinline{haskellc}/File/, but this is opaque for the user. What matters is that a file can only be accessed by providing a linear continuation to \mintinline{haskellc}/withFile/.

Still, this is not enough; because \mintinline{haskellc}/\file → file/ is indeed a linear continuation, one could use \mintinline{haskellc}/withFile "test.txt" (\file → file)/ to leak a \mintinline{haskellc}/File/, and then use it in a non-linear fashion in the outside world. Hence we must forbid \mintinline{haskellc}/File/ from appearing anywhere in the return type of the continuation.

To do that, we ask the return type to be wrapped in the unrestricted modality \mintinline{haskellc}/Ur/ (the equivalent in the Linear Haskell realm to the $\ottstype{!}$ modality): because the value of type \mintinline{haskellc}/File/ comes from the left of a linear arrow, it cannot be promoted to an \mintinline{haskellc}/Ur/ without breaking linearity, so it cannot leave the scope and has to be used exactly once in this scope. That's why we speak of \emph{linear scopes}. Here's the updated example:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
withFile' ⩴ String ⊸ (File ⊸ Ur t) ⊸ Ur t
closeFile ⩴ File ⊸ ()
exampleOk ⩴ Ur () = withFile' "test.txt" (\file → closeFile file ; ¤Ur ())
exampleFail ⩴ Ur File =
  withFile' "test.txt" (\file → °\mold{¤Ur file}°) -- leaking file, effectively rejected
\end{minted}
}
\end{unbreakable}

Still, this solution is not perfect. We effectively prevent any linear resource from leaving the scope, even resources that have nothing to do with the file API and have been created in outer scopes! This issue is talked in length by~\citet{spiwack_linear_scopes_2023}. It manifests itself in the following example, which gets rejected:

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
readLine ⩴ File ⊸ (Ur String, File)
writeLine File ⊸ String ⊸ File

exampleFail' ⩴ Ur () =
  withFile' "test.txt" (\file1 →
    ...
    withFile' "test2.txt" (\file2 →
      let (Ur line, file1') = readLine file1
          file2' = writeLine file2 line
       in closeFile file2' ; °\mold{¤Ur file1'}°)
    ... -- other operations with file1' here
  )
\end{minted}
}
\end{unbreakable}

The problem here is that we cannot return \mintinline{haskellc}/file1'/ from the inner scope, because it's a linear resource that cannot be promoted to \mintinline{haskellc}/Ur/. However, it would be perfectly safe to do so; it's just that we don't have precise enough tools at type level to convince the compiler of this fact.

In that case, the most obvious workaround is to extend the scope of \mintinline{haskellc}/withFile' "test2.txt" (file2 → ...)/ as long as we need to operate on \mintinline{haskellc}/file1'/. Very unfortunate and unpractical indeed to have to prolongate the inner scope because we still need to operate on resources from the outer one!\footnote{This is what is referred to as \emph{sticky end of linear scopes} in~\cite{spiwack_linear_scopes_2023}}

One easy improvement we can do is to have a single linear scope, providing a linear token type, and let this token type be used to instantiate any resource for which we want to enforce linearity/uniqueness. That way, we can just have one big linear scope, and write most of the code in direct style, still without monads!

\begin{unbreakable}
{\figtextsize
\begin{minted}[linenos,escapeinside=°°]{haskellc}
type Token
dup ⩴ Token ⊸ (Token, Token)
drop ⩴ Token ⊸ ()
withToken ⩴ (Token ⊸ Ur t) ⊸ Ur t

-- A first linear API
openFile' ⩴ String ⊸ Token ⊸ File
closeFile ⩴ File ⊸ ()

-- A second linear API
openDB ⩴ String ⊸ String ⊸ Token ⊸ DB
closeDB ⩴ DB ⊸ ()

exampleOk1 ⩴ Ur () =
  withToken (\tok → let file = openFile' "test.txt" tok in closeFile file ; ¤Ur ())
exampleOk2 ⩴ Ur () =
  withToken (\tok → let (tok1, tok2) = dup tok
                        file = openFile' "test.txt" tok1
                        db = openDB "localhost" 3306 tok2
                     in closeFile file ; closeDB db ; ¤Ur ())
exampleRejected ⩴ Ur File =
  withToken (\tok →  ¤Ur (openFile' "test.txt" tok)) -- leaking file which linearly depends on tok
\end{minted}
}
\end{unbreakable}

What might be surprising here is that we are allowed to duplicate or drop a token (with \mintinline{haskellc}/dup/ or \mintinline{haskellc}/drop/), which we announced as a linear resource!

In fact, the goal of the \mintinline{haskellc}/Token/ type is to tie every resource to a linearity requirement. The important part is that the \mintinline{haskellc}/Token/ type should not have a way to be promoted to \mintinline{haskellc}/Ur/, otherwise tokens and other linear resources could leak and live outside of the linear scope of \mintinline{haskellc}/withToken/.

The linear \mintinline{haskellc}/dup/ function creates two tokens out of one, but still tie the resulting tokens to the input one through a linear arrow. Allowing to duplicate tokens doesn't allow the consumer to duplicate the resources we want to control (files, database handles, etc.). So we're safe on that front.

With this approach, dealing with multiple linear resource types is much more uniform. We can use any remaining token to instantiate one or several new linear resources. As we only have one big linear scope, with every linear resource in it, there is no issue with nested scopes that need to be extended. When a program is sliced into many sub-functions, these functions don't even have to deal or know about the linear scope; they just have to pass around and receive tokens, an can otherwise be written in direct style.

Although it's out of scope for this document,~\citet{spiwack_linearly_2022} goes further on the idea of linear tokens and scopes, with \emph{Linear constraints}, which are roughly a way to pass tokens around in an implicit fashion, to make programming with linear types even more convenient\footnote{At the time of writing this document, Linear Constraints are seriously considered to be added into GHC, the main Haskell compiler~\cite{spiwack_linear_prop_2023}.}.

In next chapters, we will reuse the \emph{linear scope with tokens} strategy to enforce linearity and uniqueness on linear resources, to allow safe effects in pure, direct-style APIs.

\section{Conclusion}\label{sec:lin_lambda_conclusion}

In this chapter we introduced the linear $\lambda$-calculus, from its roots in linear logic, to more practical and computation-centric presentations. In particular, we laid theoretical foundations for a modal linear $\lambda$-calculus, which will be reused in the rest of this document.

Furthermore, we explored Linear Haskell, as a concrete target for experimenting with linear types. In particular, we illustrated how linear types can help with managing unique resources, that must be used but also must not be duplicated, even though it requires a bit of care to avoid the inherent limitations of linear types (in that, they can't guarantee per se what happened to a value in the past).

We are now in comfortable ground and have all the tool needed to explore both theoretical and practical aspect of functional destination passing.
