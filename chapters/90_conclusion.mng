\chapter*{Related Work}\label{sec:related-work}\addcontentsline{toc}{chapter}{Related Work}\markboth{Related Work}{Related Work}

In the realm of safe controlled memory updates for functional languages, \emph{functional data structures with holes} and \emph{destination passing} are two very close themes, since \emph{destination passing} provides a flexible and principled way to manipulate such structures. However, despite their conceptual overlap, \emph{functional destination passing} and \emph{functional data structures with holes} remain slightly distinct in their nature.

\emph{Functional destination passing} encompasses techniques where updates to write-once memory cells are exposed through a safe or at least controlled interface. In fact, in previous work, this interface has not often been directly exposed to programmers; instead, in these cases, \emph{destination passing} is employed internally by compilers to optimize functional programs. This includes performance improvements for functional linked data structures (Work~\labelcref{ssec:tmc}) and functional arrays (Work~\labelcref{ssec:shaikhha-dps}), which are written in the usual purely functional style but benefit from behind-the-scenes mutations to reduce allocations and improve performance, with no explicit user control. Ultimately, destinations can also be used in the operational semantics of a theoretical lambda-calculus (Work~\labelcref{ssec:sax}).

In contrast, \emph{functional structures with holes} focus on allowing incomplete data structures, a.k.a. structures with holes, to exist as first-class values---that the user can manipulate explicitely---in a purely functional setting, and be completed or refined later. Completing these structures necessarily involves some form of memory update, but many systems (Work~\labelcref{ssec:minamide,ssec:lorenzen-searchtree}) do not include an explicit notion of first-class write pointer (\emph{destination}) to do the mutation. In contrast, this thesis explores how both \emph{functional structures with holes} and \emph{functional destination passing} can be combined effectively, leveraging the strengths of each to achieve more expressive manipulation patterns of functional data, always in a safe manner.

Both of these notions inherently involve mutation and uninitialized data and therefore require robust static guarantees to ensure safety. \emph{Linear} and \emph{uniqueness type systems} are thus commonly employed to enforce strict control over destinations or structures with holes, preventing unsafe updates and premature reads on incomplete structures. However, as demonstrated in this work, \emph{linearity} alone is insufficient to capture the full range of safety and expressiveness we want. Modal type systems (Work~\labelcref{ssec:oxidizing,ssec:granule}) are in that case particularly helpful to let us design a coherent and ergonomic type system, easily, for our specific needs. Complementary approaches such as \emph{permission-based} type systems (Work~\labelcref{ssec:mezzo}) and \emph{lifetime} management techniques \emph{à la} Rust (Work~\labelcref{ssec:rust-lifetimes}) also play an important role to enforce fine-grained control over resources, which may be leveraged to control our destinations or structures with holes.

\section{Tail modulo constructor}\label{ssec:tmc}

\cite{bour_tmc_2021} (and \cite{bour_tmc_2024}, expanding the former with a correctness proof) focus on the perennial problem that the map function on linked lists is not tail-recursive, hence consumes stack space. The map function can be made tail recursive, but at the cost of an extra reverse operation on the list. Their observation is that there's a systematic transformation of functions where the only recursive call is under a data constructor application (e.g. \emph{cons} in the map instance) to a destination-passing tail-recursive implementation.

Here, there's no destination in user land, only in the intermediate representation. However, there is a programmatic interface: the programmer annotates a function like
\begin{verbatim}
let[@tail_mod_cons] rec map =
\end{verbatim}
to ask the compiler to perform the translation. The compiler will then throw an error if it cannot, making the optimization behavior entirely predictable. This has been included in the OCaml compiler since version 4.14, and is the one example we know of destinations built in a production-grade functional compiler.

Both our \destcalculus{} and DPS Haskell API allow to write the result of a tail-modulo-constructor transformation (that is, the destination-passing tail-recursive program) as a well-typed program in userland, which is not possible in OCaml. However, we do not provide a way to do a similar automatic transformation itself.

On the flip-side, tail modulo constructor is too weak to handle our difference lists, as it misses first-class support for structure with holes; they can only exist in the background for the duration of a function being transformed. The tail-modulo-constructor transformation also cannot be applied on our breadth-first tree traversal example (see \cref{sec:bft,ssec:bf-tree-traversal}).

\section{Destination-passing style for efficient memory management}\label{ssec:shaikhha-dps}

~\citet{shaikhha_destination-passing_2017} present a destination-based intermediate language for a functional array programming language, with destination-specific optimizations, that boasts near-C performance.

This is another significant evidence to date of the benefits of destination-passing style for performance in functional languages, although their work is on array programming, while this PhD manuscript focuses on linked data structures. They can therefore benefit from optimizations that are perhaps less valuable for us, such as allocating one contiguous memory chunk for several arrays.

The other difference between their work and ours is that their language is solely an intermediate language: it would be unsound to program in it manually, as there are no particular safety guarantees. We, on the other hand, are proposing a theoretical system, but also a concrete Haskell API to make it sound for the programmer to use destinations directly.

We see these two aspects as complementing each other: good compiler optimizations are important to alleviate the burden from the programmer and allow high-level abstraction; having the possibility to use destinations in code affords the programmer more control, should they need it.

\section{Semi-axiomatic sequent calculus}\label{ssec:sax}

In~\cite{deyoung_sax_2020} constructors return to a destination rather than allocating memory. It is very unlike the other systems described in this chapter in that it's completely founded in the Curry-Howard isomorphism. Specifically it gives an interpretation of a sequent calculus which mixes Gentzen-style deduction rules and Hilbert-style axioms. As a consequence, they feature a \emph{par} connective that is completely symmetric; and, unlike our $[[⌊ T ⌋ ¹ν]]$ type, their dualization connective is involutive.

The cost of this elegance is that computations may try to pattern-match on a hole, in which case they must wait for the hole to be filled. So the semantics of holes is that of a future or a promise. In turns this requires the semantics of their calculus to be fully concurrent, while \destcalculus{} only needs a sequential execution model, with a very tame form of state manipulation (linear substitutions on the evaluation context).

\section{A functional representation of data structures with a hole}\label{ssec:minamide}\label{ssec:ampar-motivation}

The idea of using linear types as a foundation of a functional calculus in which incomplete data structures can exist and be composed as first class values dates back to~\cite{minamide_functional_1998}. Our systems, both theoretically and in the implementations, are strongly inspired by theirs. In~\cite{minamide_functional_1998}, a first-class structure with a hole is called a \emph{hole abstraction}. Hole abstractions are represented by a special kind of linear functions with bespoke restrictions. As with any function, we cannot pattern-match on their output (or pass it to another function) until they have been applied; but they also have the restriction that we cannot pattern-match on their argument---the \emph{hole variable}---as that one can only be used directly as an argument of data constructors, or of other hole abstractions. The type of hole abstractions $\ottstype{([[T]], [[S]]) hfun}$ (a structure of type $[[S]]$ missing a $[[T]]$ to be complete) is thus a weak form of linear function type $\ottstype{[[T]] \multimap [[S]]}$.

In~\cite{minamide_functional_1998}, it's only ever possible to represent structures with a single hole. But this is a rather superficial restriction. The author does not comment on this, but we believe that this restriction only exists for convenience of the exposition: the language is lowered to a language without function abstraction and where composition is performed by combinators and it's easier to write a combinator for single-argument-function composition.

However, we have seen in \cref{sec:linear-lens} that we can actually design combinators for structures with multiple holes, thanks to linear lenses (at the cost of some extra syntactic burden). So having multiple-hole data structures would not have changed their system in any profound way.

The more important difference is that their system is based on a type of linear functions, while our systems in \cref{chap:dest-calculus,chap:dps-haskell} exploit the expressivity of the linear logic's ``par'' type. In classical linear logic, linear implication $\ottstype{[[T]] \multimap [[S]]}$ is reinterpreted as $\ottstype{[[S]]\,\mathop{\parr}\,[[T]]^{\perp}}$. We, likewise, reinterpret $\ottstype{([[T]], [[S]]) hfun}$ as $[[S ⧔ ⌊ T ⌋ ¹ν]]$ or \mintinline{haskellc}/Ampar s (Dest t)/ ---a sort of weak ``par''.

A key consequence is that destinations---as first-class representations of holes---appear naturally in \destcalculus{} or DPS Haskell, while~\cite{minamide_functional_1998} does not have them. This means that using~\cite{minamide_functional_1998}, one can implement the examples with difference lists and queues from \cref{ssec:efficient-queue,ssec:dpshaskell-dlist}, but could not do our breadth-first traversal example from \cref{sec:bft,ssec:bf-tree-traversal}, since it requires being able to store destinations in a structure.

Nevertheless, we still retain the main restrictions that~\citet{minamide_functional_1998} places on hole abstractions. For instance, we cannot pattern-match on $[[S]]$ in (unapplied) $\ottstype{([[T]], [[S]]) hfun}$; so in \destcalculus{}, we cannot act directly on the left-hand side $[[S]]$ of $[[S ⧔ T]]$, only on the right-hand side $[[T]]$ (the equivalent restrictions are also in place in DPS Haskell). Similarly, hole variables can only be used as arguments of constructors or hole abstractions; it's reflected in \destcalculus{} by the fact that the only way to act on destinations is via fill operations.

The ability to manipulate and, in particular, to store destinations does come at a cost: \destcalculus{} requires an additional notion of ages to ensure that destinations are used soundly across scopes. In the absence of such a mechanism, as in DPS Haskell, the use of destinations for linear data must be carefully constrained to preserve soundness. Despite these additional requirements, our systems---both theoretical and practical---are strictly more general than that of \citet{minamide_functional_1998}. Their system can be naturally embedded within \destcalculus{} (see \cref{sec:transl-minamide}) or expressed in DPS Haskell (see \cref{sec:single-hole}).

\section{Tail recursion modulo context and The functional essence of imperative binary search Trees}\label{ssec:lorenzen-searchtree}

The recent line of work from \citet{lorenzen_searchtree_2024,leijen_tail_2025} develops in two main axes. The first explores a generalization of Work~\labelcref{ssec:tmc}, by considering the \emph{tail-recursion modulo context} transformation, in which the \emph{context} can be instantiated in various forms. For example, when the context is a one-hole data constructor context---equivalent to the hole abstractions from Work~\labelcref{ssec:minamide}---they recover the \emph{tail-recursion modulo constructor} transformation. But the framework extends further: they are able to handle monoid contexts, semiring contexts, exponential contexts, and more. Hence, it allows the transformation of a broader class of typically non-tail-recursive functions into efficient tail-recursive equivalents, via a formalized and systematic method, surpassing the scope of the transformation described in Work~\labelcref{ssec:tmc}.

The second axis, which is closer to our own, focuses on the semantics and representation of first-class one-hole data constructor contexts, continuing the lineage of Minamide’s work (\labelcref{ssec:minamide}). In particular, they investigate how such one-hole data constructor contexts (i.e. data structure with one hole) behave under various memory models, particularly in settings where the constructor contexts are not managed linearly! This closely echoes the copy-on-write strategy that also emerged in our work on \destcalculus{} (see the end of \cref{ssec:sem} and beginning of \cref{sec:implem-destcalculus}), when ampars do not have to be managed linearly. However, their approach goes further: they define formal operational semantics over an explicit memory store, incorporating reference counting. They also propose optimizations to reduce copying overhead, by detecting when a structure with a hole is uniquely referenced and can thus be safely mutated in place---reminiscent of techniques in \cite{lorenzen_fp_2023}.

Furthermore, they implement this \emph{hybrid} strategy---as they call it---in the Koka programming language, and demonstrate performance gains brought by the \emph{tail-recursion modulo context} transformation on various benchmarks. On the \emph{map} example (see \cref{ssec:benchmark-map}), their approach achieves performance on par with, or even superior to, our DPS Haskell implementation.

On the other hand, their system remains at the same level of expressiveness as Minamide’s: destinations are purely internal and not exposed to the user. As they acknowledge, their model cannot represent structures with multiple holes, and thus, like Minamide’s, it cannot support breadth-first tree traversal.

\section{Oxidizing OCaml}\label{ssec:oxidizing}

\citet{lorenzen_oxidizing_2024} present an extension of the OCaml type system to support modes. Their modes are split along three different ``axes'', among which affinity and locality are comparable to our multiplicities and ages from \cref{chap:dest-calculus}.
Like our multiplicities, there are two modes for affinity \verb|once| and \verb|many|, though in~\cite{lorenzen_oxidizing_2024}, \verb|once| supports weakening, whereas \destcalculus{}'s $[[¹]]$ multiplicity is properly linear (proper linearity matters for destination lest we can discard them without filling the corresponding hole, and end up reading uninitialized memory).

Locality tracks scope. There are two locality modes, \verb|local| (does not escape the function) and \verb|global| (can escape the current function). The authors present their locality mode as a drastic simplification of Rust's lifetime system (see Work~\labelcref{ssec:rust-lifetimes}), which nevertheless fits their need.

However, this system is too limited to track the scope of destinations as precisely as \destcalculus{} needs. The key issue is that if destinations from nested scopes are assigned the same mode (which has to happen when mapping an infinity of ages to just two), we lose the ability to safely distinguish their lifetimes---making it possible to reproduce the counterexamples from \cref{sec:scope-escape-dests}. This suggests that implementing \destcalculus{} in its entirety within modal OCaml is likely infeasible.

That said, if a mechanism could be introduced to prevent discarding resources of mode \verb|once|, then much---if not all---of the destination-passing APIs described in \cref{chap:dps-haskell,chap:dps-haskell} could still be realized in their version of OCaml, as these APIs rely only on linearity, not full age tracking.

\citet{lorenzen_oxidizing_2024} is not the first attempt to introduce a control system over resources in OCaml. \cite{scherer_fabulous_2017} explored integrating an ML-like statically typed language with another featuring linear types and state, in which references and in-place reuse is covered quite explicitly.

\section{Quantitative program reasoning with graded modal types}\label{ssec:granule}

\citet{orchard_granule_2019} present a functional framework with generalized support for \emph{graded modal types}, and the language \emph{Granule} as an instance of the said framework. Graded modal types refer to a type system with graded modalities---that is, modalities indexed with modes, like $\ottstype{!}_{[[m]]}$---in which the mode, here called \emph{coeffect}, belongs to a semiring, as in~\cite{ghica_bounded_2014,bernardy_modality_2020}.

Their framework can be instantiated with different semirings, and they showcase a practical instance that supports fine-grained linearity (where usage counts come from $\ottsmode{\mathbb{N}} \cup \{\ottsmode{\omega}\}$, rather than the coarser $\{\ottsmode{1}, \ottsmode{\omega}\}$), along with a tagging system for public/private data to enable sensitive dataflow analysis. They furthermore include \emph{mode ranges}, and a suitable ordering/subtyping relation on modes (similar to ours), that gives more flexibility in the way modes can be used and combined.

With this, Granule appears to be a promising target for functional destination-passing. While the instantiation they present lacks a notion of ages or explicit scope control, making a full embedding of \destcalculus{} likely infeasible---it seems on the other hand capable of supporting (at type level at least) much, if not all, of the destination-passing APIs we developed for Haskell in \cref{chap:dps-haskell,chap:ext-linear-nonlinear}.

Their general framework, however, could be instantiated with the semiring we developed in \cref{ssec:age-control}, and be used as a basis for the type system of \destcalculus{}. But we would still need to extend the set of runtime values to account for structure with holes and destinations, and likewise, introduce a mechanism for state update---such as the linear substitutions on the evaluation context we employed in \cref{ssec:sem}.

\section{Programming with permissions in Mezzo}\label{ssec:mezzo}

\citet{protzenko_mezzo_2013} introduced the Mezzo programming language, which features user-facing mutable data structures (unlike ours, where mutability is hidden from the user). Mezzo’s soundness is ensured by a powerful capability system, which governs, in particular, how mutable structures or references can be accessed and shared. In this respect, Mezzo bears a strong resemblance to the Rust programming language.

Of particular relevance to our work is Mezzo’s ability to safely ``freeze'' mutable data structures into immutable ones once construction is complete. This principle is applied, for instance, in Mezzo's standard library for lists, where an efficient \emph{map} function on immutable lists is implemented using destination-passing and mutable lists internally. The capability system guarantees the safe and correct use of destinations in this low-level implementation. An earlier example of destination-passing used as a performance optimization in a mutable setting can also be found in~\cite{larus_restructuring_1989}.

\section{Rust lifetimes}\label{ssec:rust-lifetimes}

The Rust programming language uses a system of lifetimes (see e.g.~\cite{pearce_lifetime_2021}) that plays a similar role as our system of ages in \destcalculus{} (\cref{chap:dest-calculus}).

Rust lifetimes are symbolic. Borrows and moves generate constraints (inequalities of the form $\alpha\leqslant\beta$) on the symbolic lifetimes. For example, that the lifetime of a reference is larger than the lifetime of any structure the reference is stored in. Without enforcing these constraints, Rust would face the same kinds of issues described in \cref{sec:scope-escape-dests}: a borrow could outlive the data it refers to, just as destinations in our system could outlive the structures they point to if scope escape were not properly prevented.
In Rust, the borrow checker must check that the constraints are solvable, while in \destcalculus{}, ages are set explicitly, with no analysis needed.

Another difference between the two systems is that \destcalculus{}'s ages (and modes in general) are relative. An explicit modality $\ottstype{!}_{[[¹↑^ka]]}$ must be used when a part has an age different than its parent, and means that the part is $[[ka]]$ scope older than the parent. On the other hand, Rust's lifetimes are absolute, and the lifetime of a part is tracked independently of the lifetime of its parent.

% \makeatletter
% \emptypage@emptypage
% \makeatother
\clearpage
\hbox{}\thispagestyle{empty}
\clearpage

\cleardoublepage\phantomsection\chapter*{Conclusion}\label{chap:global-conclusion}\addcontentsline{toc}{chapter}{Conclusion}\markboth{Conclusion}{Conclusion}

\phantomsection\section*{Contributions}\addcontentsline{toc}{section}{Contributions}\markboth{Conclusion}{Contributions}

At the end of this exotic but exciting journey, we can say with confidence: safe, functional destination passing is not an oxymoron---it exists!

Destination passing is, at its core, the act of passing a write pointer so that a function can write its result directly into a memory cell, rather than returning it. This approach offers performance gains by avoiding intermediate allocations, and expressiveness gains by enabling novel ways of constructing data---the former primarily in imperative contexts, and the latter in functional ones. Yet, at first glance, destination passing appears quite impure: tightly coupled with memory mutation, and seemingly at odds with functional purity.

Our early experiments in Haskell, actually pre-dating the formal development of \destcalculus{}, gave us the insight that linear types can tame the impure nature of destinations. Linearity can ensure that a destination is used exactly once, guaranteeing that once a value has been written to a memory cell, it will not be overwritten. This makes destination passing compatible with immutability, a cornerstone of functional programming.

However, \emph{scope escape} was discovered not long after, and with it, we realized that destinations for linear data would pose a significant challenge for type safety, even with a strict linear discipline being enforced on these destinations. Scope escape occurs when a linear resource is stored in a linear container that outlives the scope in which the resource was introduced. Linear interfaces typically rely on the assumption that each resource type comes with a specific set of destructor-like functions, and that one of these must be called for each resource. If the resource type is opaque, linearity forces us to consume the resource in the scope where it becomes available, and then the only way to do so is by calling one of its designated destructors. But if we can fake the consumption of the resource by storing it linearly outside its scope---performing scope escape---instead of calling a destructor, then we break this expectation. In our case, scope escape happens when a destination \mintinline{haskellc}/Dest m t/---the linear resource---is filled into a destination \mintinline{haskellc}/Dest Ⴈ (Dest m t)/ from a parent scope---representing the linear container. But scope escape is not an issue specific to destinations: it poses a threat for any kind of resource whose API relies on linearity for safety. Conversely, scope escape can be introduced by any writable form of storage given a linear interface, whether it be destinations, OCaml-style \mintinline{haskellc}/ref/s, etc.

In early DPS Haskell, we avoided scope escape by disallowing destinations for linear data entirely. Even then, we could still reap quite a few benefits of destination passing: implementing tail-recursive \emph{map}, efficient difference lists, and other patterns involving first-class structures with holes.

Yet this restriction limited expressiveness. As shown in the breadth-first traversal example (\cref{chap:dps-haskell}), some patterns could not be expressed without destinations for linear data. It became clear that a more complete solution was needed---one that reconciled destinations for linear data with memory safety.

To make progress, we stepped away from implementation concerns and designed a theoretical calculus, \destcalculus{}, so centered around destination passing that conventional functional data constructors are not even necessary. The key achievement was a mechanized proof of soundness for \destcalculus{}. To permit this, our calculus depends on several notable features:\nopagebreak[4]
\begin{itemize}
    \item A linear type system, akin to the one from Linear Haskell, ensuring that destinations are neither duplicated nor discarded---letting us know statically when a structure is fully filled (except if scope escape happens);
    \item An age system, that allows precise tracking of scopes in which resources are introduced. The age system is the main technical device used to prevent the scope escape issue;
    \item  A modal type system combining linearity and age control into a same mode semiring, inspired by recent work in the literature;
    \item A novel typing discipline for structures with holes, in which both the holes in a structure and their associated destinations are represented as named bindings in the typing context. When a hole binding and a destination binding share the same name, they cancel each other out. This mechanism allows us to enforce statically that a structure with holes has exactly as many unused destinations as it has holes---no more, no less;
    \item Operational semantics based on the explicit manipulation of an evaluation context. In particular, linear substitutions on the evaluation context are used to model a lightweight form of state update. Compared to store-based semantics, our approach is simpler, while still faithfully capturing the behavior of destination-induced memory mutations;
\end{itemize}

Armed with this theory, we could return to our original motivation: practical destination passing in Haskell. Haskell's support for linear types made it a natural fit\footnote{Haskell was also chosen due to the industrial context of this PhD, with funding from a company with industrial functional programming in its DNA, and due to my supervisor Arnaud Spiwack’s involvement in Linear Haskell~\cite{bernardy_linear_2018,spiwack_linearly_2022,spiwack_linear_prop_2023}.}, but its lack of age tracking meant we had to explore new trade-offs. In particular, we had to balance the simplicity of the destination-passing API with the expressiveness we desired. In \destcalculus{}, we could have both---but only with the help of a tailored type system featuring ages.

In \cref{chap:dps-haskell,chap:ext-linear-nonlinear}, we iterated through increasingly powerful destination-passing APIs in Haskell, each narrowing the gap with \destcalculus{}. By the end of \cref{chap:ext-linear-nonlinear}, we recovered a system expressive enough to implement all motivating examples from \destcalculus{}, with no adaptations required further than mere translation of a syntax to another.

The last important contribution of our work is practical as well. In the course of developing DPS Haskell, we implemented the API within \emph{Compact regions}---a separate memory space in the Haskell heap that is only loosely overseen by the garbage collector. Our initial motivation was to ensure that memory mutations caused by destination-filling primitives would not interfere with the invariants expected by the garbage collector and the runtime system. But this design choice turned out to bring additional benefits we had not anticipated. \emph{Compact regions} are often used to store long-lived data structures more efficiently by avoiding repeated garbage collection passes over them. Our DPS Haskell API offers, in fact, a more direct and efficient way to allocate such structures in compact regions safely. The benchmarks in \cref{sec:benchmark} support this claim: destination passing---implemented inside compact regions---proves to be a valuable tool in specific scenarios, either enabling more direct, imperative-like programming when desired (without compromising memory safety), or allowing more efficient compact region use, e.g. in the parser example we developed in \cref{ssec:parser-sexpr}.

Of all the approaches we explored in our journey, the one from \cref{chap:dps-haskell} is likely the most viable for practical adoption, hitting a good balance between simplicity and usability. The vision behind \destcalculus{}---where every data structure is built through destination passing---diverges radically from conventional functional programming styles. This divergence is justified by the need to rigorously explore the theoretical foundations and safety considerations of destination passing, but it means \destcalculus{} addresses concerns that are rather niche within the broader landscape of everyday programming. Meanwhile, the API developed in \cref{chap:ext-linear-nonlinear} is a significant effort to expand what is achievable relying solely on linearity as a safety mechanism; but its current form is too complex and cluttered to be made into an industrial-grade library. It showcases the extent of what we were able to achieve, yet there's likely room for simplification and refinement.

With these reflections in mind, we hope we have presented the reader with a clear and enjoyable overview of functional destination passing, through a journey rooted in type theory, language design, and practical implementation concerns.

\clearpage
\phantomsection\section*{Future perspectives}\addcontentsline{toc}{section}{Future perspectives}\markboth{Conclusion}{Future perspectives}

There are still many interesting threads that can be pursued to improve this work. The most immediate is probably to further refine the existing DPS Haskell implementation. This involves, in particular, fine-tuning its performance, which remains one of the main selling points of the approach. This is even more important given its dual role as both a destination-passing interface and a better compact region allocation mechanism, as described above.

With a longer time frame, we can envision other improvements on both the practical and theoretical sides, as we detail below.

\paragraph{Destination passing support for data structures with unpacked fields}

A first direction for improvement would be to extend destination-passing support to data structures with unpacked fields---that is, fields that embed child elements directly instead of pointing to them.

Until now, we have restricted our attention to purely linked structures, and for good reason: they allow efficient composition (i.e., merging) of structures with holes.

Throughout both our theoretical work and implementations, our goal has been to perform destination filling as simple, single memory write. Also, since Minamide's foundational work, composability has been a key feature of structures with holes, crucial for building constructs like ampar-based difference lists.

When composing two ampars using \mintinline{haskellc}/extendComp/ (or filling a parent ampar's destination with a child ampar using \mintinline{haskellc}/fillComp/), if the involved fields are indirected, then composition boils down to writing the address of the child into the hole of the parent. See below (we annotate memory locations with \mintinline{text}/@addr =/ syntax, and denote holes by $\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$):\nopagebreak[4]%
{\figtextsize%
\begin{minted}[linenos,escapeinside=°°]{haskellc}
extendComp ⩴ ∀ s t u. Ampar t u ⊸ Ampar s (Dest Ⴈ t) ⊸ Ampar s u

parentAmpar :: Ampar [t] (Dest Ⴈ [t]) = newAmpar tok1 &extend @'(:) @1 1
-- in memory:  ¤Ampar 0x05ba (¤Dest °$\ottshname{0x05ca}$°)
-- @0x05ba =   0xcb1e : °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°
-- @0xcb1e =   I# 1#

childAmpar :: Ampar [t] (Dest Ⴈ [t]) = newAmpar tok2 &extend @'(:) @1 2
-- in memory:  ¤Ampar 0x0840 (¤Dest °$\ottshname{0x0850}$°)
-- @0x0840 =   0xbc4e : °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°
-- @0xbc4e =   I# 2#

resAmpar :: Ampar [t] (Dest Ⴈ [t]) = parentAmpar &extendComp childAmpar
-- in memory:  ¤Ampar () (¤Dest °$\ottshname{0x0850}$°)
-- @0x05ba =   0xcb1e : 0x0840
-- @0xcb1e =   I# 1#
-- @0x0840 =   0xbc4e : °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°
-- @0xbc4e =   I# 2#
\end{minted}
}


To perform the composition, only the second field of the \emph{cons} constructor at \mintinline{text}/0x05ba/ needs to be updated. The resulting ampar inherits the destination from \mintinline{haskellc}/childAmpar/, which still correctly points to the hole at \mintinline{text}/0x0850/.

\pagebreak

However, things become more complicated when unpacked fields are involved:\nopagebreak[4]%
{\figtextsize%
\begin{minted}[linenos,escapeinside=°°]{haskellc}
extendComp ⩴ ∀ s t u. Ampar t u ⊸ Ampar s (Dest Ⴈ t) ⊸ Ampar s u

-- GHC does not support UNPACK for polymorphic fields so we cannot do:
-- data Pair a b = Pair {-# UNPACK #-} !a {-# UNPACK #-} !b
data IntPair = ¤IntPair {-# UNPACK #-} !Int {-# UNPACK #-} !Int
data NestedPair = ¤NestedPair {-# UNPACK #-} !Int {-# UNPACK #-} !IntPair

parentAmpar :: Ampar NestedPair (Dest Ⴈ IntPair)
  = newAmpar tok1 &extend @'NestedPair @1 1
-- in memory:  ¤Ampar 0x05ba (¤Dest °$\ottshname{0x05ca}$°)
-- @0x05ba =   ¤NestedPair 1# °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°

childAmpar :: Ampar IntPair (Dest Ⴈ Int) = newAmpar tok2 &extend @'IntPair @1 2
-- in memory:  ¤Ampar 0x0840 (¤Dest °$\ottshname{0x0850}$°)
-- @0x0840 =   ¤IntPair 2# °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°

resAmpar :: Ampar NestedPair (Dest Ⴈ Int) = parentAmpar &extendComp childAmpar
-- in memory:  ¤Ampar 0x05ba (¤Dest °$\ottshname{0x0850}$°)
-- @0x05ba =   ¤NestedPair 1# 2# °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°
-- @0x0840 =   ¤IntPair 2# °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°
\end{minted}
}


Now, to perform composition, we must copy the fields of the child structure into the hole of the parent, as the parent field is annotated as unpacked. For instance, here the payload \mintinline[escapeinside=°°]{haskellc}/¤2# °$\ottshname{\fbox{$\ottshname{\phantom{a}}$}}$°/ is copied into the hole at \mintinline{text}/0x05ca/. But doing so invalidates the destination inherited from the child: it still points to \mintinline{text}/0x0850/, while the remaining hole in \mintinline{haskellc}/resAmpar/ now lives at \mintinline[escapeinside=°°]{text}/0x5d2/!

Addressing this would require tracking and updating destination pointers dynamically whenever composition happens throughout an unpacked field. But such a mechanism would impose significant overhead and undermine the simplicity and efficiency of our current model.

A more realistic path forward---one we are likely to adopt in \cite{bagrel_primitives_2024}---is to distinguish destinations that point to unpacked fields from those that point to indirected ones.\footnote{Whether a field is unpacked or indirected can be determined using \mintinline{haskellc}/GHC.Generics/.} We would then restrict the use of \mintinline{haskellc}/fillComp/ and \mintinline{haskellc}/extendComp/ to cases where the destination targets an indirected field. This approach preserves much of the expressiveness of destination-passing and the composition of incomplete structures, while still supporting unpacked fields. The new restrictions would apply only to those unpacked fields (that were not supported before) and would not compromise the rest of the system, allowing destinations to remain simple pointers, with no need for complex update mechanisms.

\paragraph{DPS Haskell, outside of compact regions}

A second direction for future work is to implement destination-passing support directly within Haskell’s standard garbage-collected heap, rather than relying on the protective boundary of compact regions.

The main challenge lies in the close interaction between memory safety and the internals of GHC’s garbage collector, particularly regarding when and where immutability assumptions are relied upon. GHC uses a copying collector with a variety of performance-critical optimizations, and violating its expectations about immutability could lead to subtle and potentially catastrophic bugs. For instance, if structures with holes are moved by the GC, then any live destination referring to them must be updated accordingly to avoid dangling pointers.

Another challenge is that GHC’s runtime system is complex and actively evolving, with key implementation details often spread across various documents, so acquiring a complete and up-to-date understanding of this system would likely require input from experienced contributors to the compiler and runtime.

Still, adapting the DPS Haskell implementation to run directly on the standard GC heap could significantly lower the barrier to adoption. Programmers would be able to use destination-passing selectively, without the overhead and constraints associated with compact regions. It would likely bring substantial performance improvements as well, by avoiding the need to copy data into compact regions. For example, in our \emph{map} function using destinations, every value in the input list must currently be copied into the region, even though destination-passing is only needed for the spine. This direction remains uncertain---the runtime implications are subtle and the engineering effort nontrivial---but if achievable, the payoff could be great.

\paragraph{Proof of safety for the practical DPS Haskell APIs}

By the end of this PhD, we achieved our two main goals: demonstrating that functional destination passing can be made safe, and providing evidence that practical implementations of destination passing in an industrial-grade functional language are feasible and can deliver performance benefits.

The last natural step is to reconcile these two strands of work, by using the theoretical framework developed in \destcalculus{} (\cref{chap:dest-calculus}) to prove that the practical implementations introduced in \cref{chap:dps-haskell,chap:ext-linear-nonlinear} are indeed safe.

Such a proof would involve translating each operator from the DPS Haskell APIs into their counterparts in \destcalculus{}, and showing that for any well-typed DPS Haskell program, there exists a suitable age assignment that makes the translated \destcalculus{} program typecheck. We recall here that despite Haskell being notoriously call-by-need, our DPS implementation rely on compact regions in which any data structure is forced to normal form, which matches the call-by-value semantics of \destcalculus{}. So there should not be a mismatch on that front.

The main obstacle though is that despite their common roots, there are subtle but key differences between \destcalculus{} and the various DPS Haskell APIs. For example, in the version from \cref{chap:dps-haskell}, the absence of destinations for linear data has wide-reaching implications: it allows destinations of heterogeneous ages to be safely stored in usual Haskell data structures like standard library lists. This flexibility would be difficult to justify in \destcalculus{}, which lacks any notion of non-destination-based data structures. Similarly, in the API from \cref{chap:ext-linear-nonlinear}, we rely both on a non-capturing \mintinline{haskellc}/updWith/ operator (unlike the original $\ottkw{upd}{\ottkw{\ltimes}}$), and on the \mintinline{haskellc}/extend/ functions, which would effectively translate to a controlled yet capturing version of $\ottkw{upd}{\ottkw{\ltimes}}$.

The key, then, may not be to reuse \destcalculus{} exactly as it stands, but to design a slightly adapted formal language tailored to the specific API we aim to verify. Much of the existing formalization---especially the infrastructure for managing typing contexts, hole renaming, and substitution---could likely be reused with minimal changes. While the original proofs (\cref{sec:formal-proof}, \cite{bagrel_proofs_2025}) were not written with reusability in mind (as we expected \destcalculus{} to be general enough in its current form), only a relatively small portion depends directly on the typing rules of individual operators. Adapting these rules to match the ones of the DPS Haskell API from \cref{chap:dps-haskell} for example would be time-consuming, but probably tractable. In return, this path offers a promising opportunity to generalize our formal development and extend its impact.
