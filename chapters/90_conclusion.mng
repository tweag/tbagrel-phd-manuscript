\chapter*{Related Work}\label{sec:related-work}\addcontentsline{toc}{chapter}{Related Work}\markboth{Related Work}{Related Work}

\section{Destination-passing style for efficient memory management}\label{ssec:shaikhha-dps}

~\citet{shaikhha_destination-passing_2017} present a destination-based intermediate language for a functional array programming language, with destination-specific optimizations, that boasts near-C performance.

This is the most comprehensive evidence to date of the benefits of destination-passing style for performance in functional languages, although their work is on array programming, while this article focuses on linked data structures. They can therefore benefit from optimizations that are perhaps less valuable for us, such as allocating one contiguous memory chunk for several arrays.

The main difference between their work and ours is that their language is solely an intermediate language: it would be unsound to program in it manually. We, on the other hand, are proposing a type system to make it sound for the programmer to program directly with destinations.

We see these two aspects as complementing each other: good compiler optimizations are important to alleviate the burden from the programmer and allow high-level abstraction; having the possibility to use destinations in code affords the programmer more control, should they need it.

\section{Programming with Permissions in Mezzo}\label{ssec:mezzo}

~\citet{protzenko_mezzo_2013} introduced the Mezzo programming language, in which mutable data structures can be freezed into immutable ones after having been completed. This principle is used to some extend in their list standard library module, to mimic a form of DPS programming. An earlier appearance of DPS programming as a mean to achieve better performance in a mutable language can also be seen in~\cite{larus_restructuring_1989}.

\section{Tail modulo constructor}\label{ssec:tmc}

Another example of destinations in a compiler's optimizer is~\cite{bour_tmc_2021}. It's meant to address the perennial problem that the map function on linked lists isn't tail-recursive, hence consumes stack space. The observation is that there's a systematic transformation of functions where the only recursive call is under a constructor to a destination-passing tail-recursive implementation.

Here again, there's no destination in user land, only in the intermediate representation. However, there is a programmatic interface: the programmer annotates a function like
\begin{verbatim}
let[@tail_mod_cons] rec map =
\end{verbatim}
to ask the compiler to perform the translation. The compiler will then throw an error if it can't. This way, contrary to the optimizations in~\cite{shaikhha_destination-passing_2017}, it is entirely predictable.

This has been available in OCaml since version 4.14. This is the one example we know of of destinations built in a production-grade compiler. Our \destcalculus{} makes it possible to express the result tail-modulo-constructor in a typed language. It can be used to write programs directly in that style,  or it could serve as a typed target language for an automatic transformation. On the flip-side, tail modulo constructor is too weak to handle our difference lists or breadth-first traversal examples.

\section{A functional representation of data structures with a hole}\label{ssec:minamide}\label{ssec:ampar-motivation}

The idea of using linear types as a foundation of a functional calculus in which incomplete data structures can exist and be composed as first class values dates back to~\cite{minamide_functional_1998}. Our system is strongly inspired by theirs. In~\cite{minamide_functional_1998}, a first-class structure with a hole is called a \emph{hole abstraction}. Hole abstractions are represented by a special kind of linear functions with bespoke restrictions. As with any function, we can't pattern-match on their output (or pass it to another function) until they have been applied; but they also have the restriction that we cannot pattern-match on their argument ---the \emph{hole variable}--- as that one can only be used directly as argument of data constructors, or of other hole abstractions. The type of hole abstractions, $\ottstype{([[T]], [[S]]) hfun}$ is thus a weak form of linear function type $\ottstype{[[T]] \multimap [[S]]}$.

In~\cite{minamide_functional_1998}, it's only ever possible to represent structures with a single hole. But this is a rather superficial restriction. The author doesn't comment on this, but we believe that this restriction only exists for convenience of the exposition: the language is lowered to a language without function abstraction and where composition is performed by combinators. While it's easy to write a combinator for single-argument-function composition, it's cumbersome to write combinators for functions with multiple arguments. But having multiple-hole data structures wouldn't have changed their system in any profound way.

The more important difference is that while their system is based on a type of linear functions, ours is based on the linear logic's ``par'' type. In classical linear logic, linear implication $\ottstype{[[T]] \multimap [[S]]}$ is reinterpreted as $\ottstype{[[S]]\mathop{\parr}[[T]]^{\perp}}$. We, likewise, reinterpret $\ottstype{([[T]], [[S]]) hfun}$ as $[[S ⧔ ⌊ T ⌋ ¹ν]]$ (a sort of weak ``par'').

A key consequence is that destinations ---as first-class representations of holes--- appear naturally in \destcalculus{}, while~\cite{minamide_functional_1998} doesn't have them. This means that using~\cite{minamide_functional_1998}, or the more recent but similarly expressive system from~\cite{lorenzen_searchtree_2024}, one can implement the examples with difference lists and queues from~\cref{ssec:efficient-queue}, but couldn't do our breadth-first traversal example from~\cref{sec:bft}, since it requires to be able to store destinations in a structure.

Nevertheless, we still retain the main restrictions that~\citet{minamide_functional_1998} places on hole abstractions. For instance, we can't pattern-match on $[[S]]$ in (unapplied) $\ottstype{([[T]], [[S]]) hfun}$; so in \destcalculus{}, we can't act directly on the left-hand side $[[S]]$ of $[[S ⧔ T]]$, only on the right-hand side $[[T]]$. Similarly, hole variables can only be used as arguments of constructors or hole abstractions; it's reflected in \destcalculus{} by the fact that the only way to act on destinations is via fill operations, with either hollow constructors or another ampar.

The ability to manipulate destinations, and in particular, store them, does come at a cost though: the system needs this additional notion of ages to ensure that destinations are used soundly. On the other hand, our system is strictly more general, in~\citet{minamide_functional_1998}'s system can be embedded in \destcalculus{}, and if one stays in this fragment, we're never confronted with ages.

\section{Semi-axiomatic sequent calculus}\label{ssec:sax}

In~\cite{deyoung_sax_2020} constructors return to a destination rather than allocating memory. It is very unlike the other systems described in this section in that it's completely founded in the Curry-Howard isomorphism. Specifically it gives an interpretation of a sequent calculus which mixes Gentzen-style deduction rules and Hilbert-style axioms. As a consequence, the \emph{par} connective is completely symmetric, and, unlike our $[[⌊ T ⌋ ¹ν]]$ type, their dualization connective is involutive.

The cost of this elegance is that computations may try to pattern-match on a hole, in which case they must wait for the hole to be filled. So the semantics of holes is that of a future or a promise. In turns this requires the semantics of their calculus to be fully concurrent, which is a very different point in the design space.

\section{Rust lifetimes}\label{ssec:rust-lifetimes}

Rust uses a system of lifetimes (see e.g.~\cite{pearce_lifetime_2021}) to ensure that borrows don't live longer than what they reference. It plays a similar role as our system of ages.

Rust lifetimes are symbolic. Borrows and moves generate constraints (inequalities of the form $\alpha\leqslant\beta$) on the symbolic lifetimes. For instance, that a the lifetime of a reference is larger than the lifetime of any structure the reference is stored in. Without such constraints, Rust would have similar problems to those of \cref{sec:scope-escape-dests}. The borrow checker then checks that the constraints are solvable. This contrasts with \destcalculus{} where ages are set explicitly, with no analysis needed.

Another difference between the two systems is that \destcalculus{}'s ages (and modes in general) are relative. An explicit modality $\ottstype{!}_{[[¹↑^ka]]}$ must be used when a part has an age different than its parent, and means that the part is $[[ka]]$ scope older than the parent. On the other hand, Rust's lifetimes are absolute, the lifetime of a part is tracked independently of the lifetime of its parent.

\section{Oxidizing OCaml}

~\citet{lorenzen_oxidizing_2024} present an extension of the OCaml type system to support modes. Their modes are split along three different ``axes'', among which affinity and locality are comparable to our multiplicities and ages.
Like our multiplicities, there are two modes for affinity \verb|once| and \verb|many|, though in~\cite{lorenzen_oxidizing_2024}, \verb|once| supports weakening, whereas \destcalculus{}'s $[[¹]]$ multiplicity is properly linear (proper linearity matters for destination lest we end up reading uninitialized memory).

Locality tracks scope. There are two locality modes, \verb|local| (doesn't escape the current scope) and \verb|global| (can escape the current scope). The authors present their locality mode as a drastic simplification of Rust's lifetime system, which nevertheless fits their need.

However, such a simplified system would be a bit too weak to track the scope of destinations. The observation is that if destinations from two nested scopes are given the same mode, then we can't safely do anything with them, as it would be enough to reproduce the counterexamples of \cref{sec:scope-escape-dests}. So in order to type the breadth-first traversal example of \cref{sec:bft}, where destinations are stored in a structure, we need at least $[[ν]]$ (for the current scope), $[[↑]]$ (for the previous scope exactly), plus at least one extra mode for the rest of the scopes (destinations of this generic age cannot be safely used). It turns out that such systems with finitely many ages are incredibly easy to get wrong, and it was in fact much simpler to design a system with infinitely many ages.

\cleardoublepage\phantomsection\chapter*{Conclusion and future work}\label{chap:global-conclusion}\addcontentsline{toc}{chapter}{Conclusion and future work}\markboth{Conclusion and future work}{Conclusion and future work}

Destination passing is not a novel technique in the history of programming languages. It has been employed very early ---when proper manual memory management was required even for the most basic programs--- as a way to avoid useless allocations of memory for intermediary results.

It then has been kinda forgotten during the golden era of object-oriented programming, for which automatic memory management schemes have arisen. But today, we revisit destination passing in another setting, that is, functional programming languages, with slightly different expectations.

Of course, we don't intend to go back to fully manual memory management for modern functional languages. Not having to care about memory management in most cases is a real advantage of modern programming languages. Instead, we want to be either let the compiler optimize programs automatically, at compile time, taking advantage of destination passing~\cite{bour_tmc_2021,leijen_trmc_2023}; or, as we shown in this document, be able to manually opt-in for destination passing in specific parts of a functional program, for which we need improved performance or expressivity.

Destination passing having an imperative nature at heart, it means it need to be tamed for use in a functional programming language, especially if we want to respect immutability and purity principles which are very common in such settings.

For that, we first looked into linear type systems (\cref{chap:preli}). From their roots in (linear) logic to their modern presentations and implementations, we've seen how linear types can be leveraged to ensure that a resource will only be used exactly once.

With linear types in hand, we embarked in a journey in which we tried to capture the essence of destination passing in a formal, functional model, denoted \destcalculus{}. (\cref{chap:dest-calculus}). The idea was that a destination ---a pointer to a yet unused memory cell--- must be used exactly once, and that after it has been used, we are allowed to read the memory cell it pointed to, and the value contained within is now guaranteed not to change (to abide by immutability). Consequently, the core principle is that a destination should always point to an existing hole (a yet unused memory cell), and that every remaining hole in a structure must be referenced by exactly one destination that hasn't been used yet. Destinations acts as both witnesses and a way of filling those holes, and when no destination remains, a structure (should) no longer have holes.

Early on this journey however, we discovered the dreaded \emph{scope escape} issue, which asked for more than a year of research to be spent on the topic before we could finally devise a solution. Scope escape (\cref{sec:scope-escape-dests}) is what can happen when a destination is used to store another destination. Even with a linear discipline imposed on destinations, it can become undistinguishable at type level whether a destination has been used ---which means the associated memory cell now contains a valid value--- or if the destination has been stored away for later use, which means we shouldn't read the content of the corresponding memory cell yet otherwise we risk a segmentation fault or a similar memory issue.

Because of scope escape being possible, we had to reinforce the type system of \destcalculus{}, and add a \emph{system of ages} in addition to the linear discipline, to make it memory safe. Fortunately, in \cref{sec:modal-lin}, we had developed a \emph{modal} presentation of a linear type system, based on an underlying semiring algebraic structure, which is well suited for extension. So we were able to combine the linear system and the new age system in the same semiring structure. This led to a type system for \destcalculus{} (\cref{sec:syntax-type-system}) which is very close to earlier work.

In \destcalculus{}, the novel objects, compared to an usual $\lambda$-calculus, are ampars ---which encapsulate data structures which may be yet incomplete, so which may contain holes--- and destinations, which are linear-controlled pointers to these holes. Holes (in structures) are not first-class objects of the language themselves, unlike destinations, but they still play a very important role in the typing and semantics of \destcalculus{}. Notably, we developed a new typing strategy for structure with holes in \cref{ssec:runtime-values}. Both the holes of an ampar, and destinations attached to the ampar, appear as named bindings inside the typing context, and a hole binding and destination binding of the same name then cancel each other out. Thus a well-balanced ampar types in an empty typing context; while a non-empty context indicates that the ampar has either a non-referenced hole, or an extra destination pointing to a hole that has been filled already! This is how we ensure, within the type system, that no destination can be reused or forgotten.

The operational semantics of \destcalculus{} (\cref{ssec:sem}) relies on a rather abstract memory model, where the state of the program is purely described by the evaluation context and the term under focus, and where memory writes resulting from destination-filling operations are conducted as global substitutions on named holes inside the evaluation context. This model for semantics appear to be simpler than store semantics techniques (as the typing rules for evaluation contexts can be directly derived from the term ones, while typing rules for a store are much more complex) while still capturing satisfyingly the essence of destination-induced memory mutations.

A main contribution of this work is that we were able to formalize \destcalculus in the Coq proof assistant\cref{sec:formal-proof} and then produce a mechanically-verified proof of the usual safety theorems for \destcalculus, namely \emph{progress} and \emph{preservation}.

With \destcalculus, we thus obtain a formal functional language with destination passing so rooted at its core that we have maximal flexibility to use destinations, and for instance don't even need normal data constructors to be part of the language (they can be recovered from destination filling operators). But this comes at the cost of needing a custom type system to make the system safe.

Thus, the second, and equally important part of the PhD ---especially given the fact that the PhD is funded by a company with industrial functional programming in its DNA---  was focused on bringing most of the ideas of \destcalculus, and destination passing in general, to a practical, industrial functional programming language. Haskell was a language of choice for this, given its existing support for linear types. All the more so since my industrial supervisor is one of the authors of Linear Haskell, and a major proponent for linear type support and improving their ergonomics inside GHC. The main challenge of implementation destination passing in Haskell, is that of course, we cannot use the custom type system based on linearity and ages of \destcalculus{} to ensure safety. We have to do with linearity alone.

Initial experiments for destination-passing style programming in Haskell actually predates the theoretical work on \destcalculus. Indeed, at that time, scope escape was still very much an issue, for which we had not yet found a valid solution (so the age system had not been invented either), so we decided, in the meantime, to start writing down an initial API for destination passing in Haskell (\cref{chap:dps-haskell}), in which scope escape cannot happen because we decided, as a safeguard, to forbid storing destinations inside other destinations. A major concern was finding a way to not make the Haskell memory model blow up by doing unsafe mutations. For that, we used \emph{compact regions}, that is, memory areas free from the surveillance of the garbage collector, in which we could temporarily hold unfinished data structures. This work ultimately ranged from designing a cohesive and ergonomic high-level API, down to the low-level implementation of compiler's primitives into which destination-filling operations are translated. At the end, we obtain encouraging results and benchmarks (\cref{sec:benchmark}) which show that destination passing is a tool of choice, in specific situations, to improve the performance of functional programs, but also to make some of them easier to express, in a more direct, imperative-like style, without sacrificing memory guarantees. Yet, there is still work to do to improve both the ergonomics and performance of the DPS implementation, if we hope for DPS to become a more widespread technique in the functional programming community.

One inconvenience with DPS Haskell, in the form of this first implementation, is that the restriction we imposed initially ---that destinations cannot store any linear resource to avoid scope escape--- impeds the flexibility and expressivity gains that we claimed true for \destcalculus. In particular, one of our preferred examples, the traversal of a tree in breadth-first fashion, cannot be implemented with just our shiny destination toolbox, and we have to revert to normal Haskell data structures at some point, instead of using only our new destination-based data structures.

In \cref{chap:ext-linear-nonlinear}, we try to reclaim much of this lost expressivity. But relaxing the safety constraint that destinations cannot be filled with linear resources means we must be particularly careful to avoid scope escape, and a range of other issues that can manifest themselves when we don't have ages (and age control) to help. Still, at the end of this journey, we get a way to write our dear breadth-first tree traversal with just destination-enabled data structures, in Haskell, with an API that we think is safe. 

Further work would consist in making sure the initial and upgraded DPS Haskell APIs, devised in \cref{chap:dps-haskell,chap:ext-linear-nonlinear} are safe. We claimed that \destcalculus{} can be used as a framework to reason about the safety and correctness of other destination-passing systems, and it would be time to see if it stands up to the test.

The second axis of improvement would consist in implementing DPS support for Haskell inside the garbage-collected heap (the "normal" memory area in which most Haskell objects live), outside of the protective shell of compact regions. The difficulty being that the safety of such a system is very reliant on the knowledge of the garbage collection system, and in particular, when precisely the assumptions of immutability are used.

I hope that at this point, the reader has a got a good and enjoyable overview of this exotic journey, trying to reconcile imperative techniques and functional principles, from high-level and formal considerations down to low-level implementation and the associated performance concerns. Finally, we can celebrate: safe, functional, destination passing is not an oxymoron; it exists! 
